{"id": "2511.20968", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2511.20968", "abs": "https://arxiv.org/abs/2511.20968", "authors": ["Andrew T. Karl"], "title": "SVEMnet: An R package for Self-Validated Elastic-Net Ensembles and Multi-Response Optimization in Small-Sample Mixture--Process Experiments", "comment": null, "summary": "SVEMnet is an R package for fitting Self-Validated Ensemble Models (SVEM) with elastic-net base learners and for performing multi-response optimization in small-sample mixture--process design-of-experiments (DOE) studies with numeric, categorical, and mixture factors. SVEMnet wraps elastic-net and relaxed elastic-net models for Gaussian and binomial responses from glmnet in a fractional random-weight (FRW) resampling scheme with anti-correlated train/validation weights; penalties are selected by validation-weighted AIC- and BIC-type criteria, and predictions are averaged across replicates to stabilize fits near the interpolation boundary. In addition to the core SVEM engine, the package provides deterministic high-order formula expansion, a permutation-based whole-model test heuristic, and a mixture-constrained random-search optimizer that combines Derringer--Suich desirability functions, bootstrap-based uncertainty summaries, and optional mean-level specification-limit probabilities to generate scored candidate tables and diverse exploitation and exploration medoids for sequential fit--score--run--refit workflows. A simulated lipid nanoparticle (LNP) formulation study illustrates these tools in a small-sample mixture--process DOE setting, and simulation experiments based on sparse quadratic response surfaces benchmark SVEMnet against repeated cross-validated elastic-net baselines."}
{"id": "2511.21497", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21497", "abs": "https://arxiv.org/abs/2511.21497", "authors": ["Andrew Golightly", "Sarah E. Heaps", "Chris Sherlock", "Laura E. Wadkin", "Darren J. Wilkinson"], "title": "Nested ensemble Kalman filter for static parameter inference in nonlinear state-space models", "comment": "29 pages", "summary": "The ensemble Kalman filter (EnKF) is a popular technique for performing inference in state-space models (SSMs), particularly when the dynamic process is high-dimensional. Unlike reweighting methods such as sequential Monte Carlo (SMC, i.e. particle filters), the EnKF leverages either the linear Gaussian structure of the SSM or an approximation thereof, to maintain diversity of the sampled latent states (the so-called ensemble members) via shifting-based updates. Joint parameter and state inference using an EnKF is typically achieved by augmenting the state vector with the static parameter. In this case, it is assumed that both parameters and states follow a linear Gaussian state-space model, which may be unreasonable in practice. In this paper, we combine the reweighting and shifting methods by replacing the particle filter used in the SMC^2 algorithm of Chopin et al., with the ensemble Kalman filter. Hence, parameter particles are weighted according to the estimated observed-data likelihood from the latest observation computed by the EnKF, and particle diversity is maintained via a resample-move step that targets the marginal parameter posterior under the EnKF. Extensions to the resulting algorithm are proposed, such as the use of a delayed acceptance kernel in the rejuvenation step and incorporation of nonlinear observation models. We illustrate the resulting methodology via several applications."}
{"id": "2511.21563", "categories": ["stat.CO", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.21563", "abs": "https://arxiv.org/abs/2511.21563", "authors": ["Sam Power", "Giorgos Vasdekis"], "title": "Some aspects of robustness in modern Markov Chain Monte Carlo", "comment": "52 pages, 29 Figures, Review article", "summary": "Markov Chain Monte Carlo (MCMC) is a flexible approach to approximate sampling from intractable probability distributions, with a rich theoretical foundation and comprising a wealth of exemplar algorithms. While the qualitative correctness of MCMC algorithms is often easy to ensure, their practical efficiency is contingent on the `target' distribution being reasonably well-behaved.\n  In this work, we concern ourself with the scenario in which this good behaviour is called into question, reviewing an emerging line of work on `robust' MCMC algorithms which can perform acceptably even in the face of certain pathologies.\n  We focus on two particular pathologies which, while simple, can already have dramatic effects on standard `local' algorithms. The first is roughness, whereby the target distribution varies so rapidly that the numerical stability of the algorithm is tenuous. The second is flatness, whereby the landscape of the target distribution is instead so barren and uninformative that one becomes lost in uninteresting parts of the state space. In each case, we formulate the pathology in concrete terms, review a range of proposed algorithmic remedies to the pathology, and outline promising directions for future research."}
{"id": "2511.21553", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.21553", "abs": "https://arxiv.org/abs/2511.21553", "authors": ["Yacine Mohamed Idir", "Thomas Romary"], "title": "Efficient bayesian spatially varying coefficients modeling for censored data using the vecchia approximation", "comment": null, "summary": "Spatially varying coefficients (SVC) models allow for marginal effects to be non-stationary over space and thus offer a higher degree of flexibility with respect to standard geostatistical models with external drift. At the same time, SVC models have the advantage that they are easily interpretable. They offer a flexible framework for understanding how the relationships between dependent and independent variables vary across space. The most common methods for modelling such data are the Geographically Weighted Regression (GWR) and Bayesian Gaussian Process (Bayes-GP). The Bayesian SVC model, which assumes that the coefficients follow Gaussian processes, provides a rigorous approach to account for spatial non-stationarity. However, the computational cost of Bayes-GP models can be prohibitively high when dealing with large datasets or/and when using a large number of covariates, due to the repeated inversion of dense covariance matrices required at each Markov chain Monte Carlo (MCMC) iteration. In this study, we propose an efficient Bayes-GP modeling framework leveraging the Vecchia approximation to reduce computational complexity while maintaining accuracy. The proposed method is applied to a challenging soil pollution data set in Toulouse, France, characterized by a high degree of censorship (two-thirds censored observations) and spatial clustering. Our results demonstrate the ability of the Vecchia-based Bayes-GP model to capture spatially varying effects and provide meaningful insights into spatial heterogeneity, even under the constraints of censored data."}
{"id": "2511.20711", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.20711", "abs": "https://arxiv.org/abs/2511.20711", "authors": ["José Camacho"], "title": "A Set of Rules for Model Validation", "comment": null, "summary": "The validation of a data-driven model is the process of assessing the model's ability to generalize to new, unseen data in the population of interest. This paper proposes a set of general rules for model validation. These rules are designed to help practitioners create reliable validation plans and report their results transparently. While no validation scheme is flawless, these rules can help practitioners ensure their strategy is sufficient for practical use, openly discuss any limitations of their validation strategy, and report clear, comparable performance metrics."}
{"id": "2511.20960", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20960", "abs": "https://arxiv.org/abs/2511.20960", "authors": ["Soumojit Das", "Nairanjana Dasgupta", "Prashanta Dutta"], "title": "Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification", "comment": null, "summary": "Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain. We develop a geometric framework for post-hoc calibration of neural network probability outputs, treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric. Our approach yields Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems (Proposition~1) while extending naturally to multi-class settings -- providing a principled generalization that existing methods lack. Complementing calibration, we define geometric reliability scores based on Fisher--Rao distance and construct neutral zones for principled deferral of uncertain predictions.\n  Theoretical contributions include: (i) consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and (ii) tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework (calibration followed by reliability-based deferral) captures 72.5\\% of errors while deferring 34.5\\% of samples. Notably, this operational gain is achievable with any well-calibrated probability output; the contribution of geometric calibration lies in its theoretical foundations rather than empirical superiority over simpler alternatives. This work bridges information geometry and statistical learning, offering formal guarantees relevant to applications requiring rigorous validation."}
{"id": "2511.20851", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20851", "abs": "https://arxiv.org/abs/2511.20851", "authors": ["Mousam Sinha", "Tirtha Sarathi Ghosh", "Ridam Pal"], "title": "When Features Beat Noise: A Feature Selection Technique Through Noise-Based Hypothesis Testing", "comment": null, "summary": "Feature selection has remained a daunting challenge in machine learning and artificial intelligence, where increasingly complex, high-dimensional datasets demand principled strategies for isolating the most informative predictors. Despite widespread adoption, many established techniques suffer from notable limitations; some incur substantial computational cost, while others offer no definite statistical driven stopping criteria or assesses the significance of their importance scores. A common heuristic approach introduces multiple random noise features and retains all predictors ranked above the strongest noise feature. Although intuitive, this strategy lacks theoretical justification and depends heavily on heuristics. This paper proposes a novel feature selection method that addresses these limitations. Our approach introduces multiple random noise features and evaluates each feature's importance against the maximum importance value among these noise features incorporating a non-parametric bootstrap-based hypothesis testing framework to establish a solid theoretical foundation. We establish the conceptual soundness of our approach through statistical derivations that articulate the principles guiding the design of our algorithm. To evaluate its reliability, we generated simulated datasets under controlled statistical settings and benchmarked performance against Boruta and Knockoff-based methods, observing consistently stronger recovery of meaningful signal. As a demonstration of practical utility, we applied the technique across diverse real-world datasets, where it surpassed feature selection techniques including Boruta, RFE, and Extra Trees. Hence, the method emerges as a robust algorithm for principled feature selection, enabling the distillation of informative predictors that support reliable inference, enhanced predictive performance, and efficient computation."}
{"id": "2511.20712", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2511.20712", "abs": "https://arxiv.org/abs/2511.20712", "authors": ["Ashutosh Dumka", "Raghupathi Kandiboina", "Skylar Knickerbocker", "Neal Hawkins", "Jonathan Wood", "Anuj Sharma"], "title": "Integrating Spatial and Temporal Effects in Seat-Belt Compliance Assessment with Telematics Data", "comment": null, "summary": "Seat belt use remains one of the most effective measures for reducing vehicle occupant fatalities and injuries. Yet, seat-belt compliance across different locales demands far more granular data than traditional, roadside surveys can provide. These surveys are spatially sparse, temporally intermittent, and costly to administer, often providing coarse-grained snapshots insufficient for capturing dynamic behavioral patterns or localized disparities. Telematics data emerges as a transformative alternative, offering continuous, high-resolution driver event records, such as seat belt latch status, across vast geographic areas. This granular and scalable data enables the application of advanced spatiotemporal models that more accurately reflect the complex interactions driving seatbelt use. This study utilizes telematics data to generate county-level seat belt compliance metrics for Iowa in 2022, employing a suite of beta-regression models that incorporate spatial and temporal random effects. The study findings demonstrate that models including both spatial and temporal components outperform those with spatial or temporal effects alone, underscoring the importance of jointly accounting for geographic clustering and temporal dynamics. Among explanatory variables, vehicle miles traveled (VMT) and per capita income emerge as significant predictors of compliance rates. The significant spatial and temporal effects highlight that telematics-based granular data substantially enhances model fit and inference quality. The results demonstrate that integrating granular telematics data within sophisticated spatiotemporal frameworks significantly improves inference, providing policymakers with precise insights for targeted interventions and advancing traffic safety research."}
{"id": "2511.20833", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20833", "abs": "https://arxiv.org/abs/2511.20833", "authors": ["Ruyi Liu", "Joshua L. Warren", "Yuki Ohnishi", "Donna Spiegelman", "Liangyuan Hu", "Fan Li"], "title": "Calibrated Bayes analysis of cluster-randomized trials", "comment": null, "summary": "In cluster-randomized trials (CRTs), entire clusters of individuals are randomized to treatment, and outcomes within a cluster are typically correlated. While frequentist approaches are standard practice for CRT analysis, Bayesian methods have emerged as a strong alternative. Previous work has investigated the use of Bayesian hierarchical models for continuous, binary, and count outcomes in CRTs, but these approaches focus on model-based treatment effect coefficients as the target estimands, which may have ambiguous interpretation under model misspecification and informative cluster size. In this article, we introduce a calibrated Bayesian procedure for estimand-aligned analysis of CRTs even in the presence of potentially misspecified models. We propose estimators targeting both the cluster-average treatment effect (cluster-ATE) and individual-average treatment effect (individual-ATE), particularly in scenarios with informative cluster sizes. We additionally explore strategies for summarizing the posterior samples that can achieve the frequentist coverage guarantee even under working model misspecification. We provide simulation evidence to demonstrate the model-robustness property of the proposed Bayesian estimators in CRTs, and further investigate the impact of covariate adjustment as well as the use of more flexible Bayesian nonparametric working models."}
{"id": "2511.21376", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.21376", "abs": "https://arxiv.org/abs/2511.21376", "authors": ["Lukas Pin", "Stef Baas", "Gianmarco Caruso", "David S. Robertson", "Sofía S. Villar"], "title": "Informed Burn-In Decisions in RAR: Harmonizing Adaptivity and Inferential Precision Based on Study Setting", "comment": "18 pages, 3 figures, 4 tables", "summary": "Response-Adaptive Randomization (RAR) is recognized for its potential to deliver improvements in patient benefit. However, the utility of RAR is contingent on regularization methods to mitigate early instability and preserve statistical integrity. A standard regularization approach is the ''burn-in'' period, an initial phase of equal randomization before treatment allocation adapts based on accrued data. The length of this burn-in is a critical design parameter, yet its selection remains unsystematic and improvised, as no established guideline exists. A poorly chosen length poses significant risks: one that is too short leads to high estimation bias and type-I error rate inflation, while one that is too long impedes the intended patient and power benefits of using adaptation. The challenge of selecting the burn-in generalizes to a fundamental question: what is the statistically appropriate timing for the first adaptation? We introduce the first systematic framework for determining burn-in length. This framework synthesizes core factors - total sample size, problem difficulty, and two novel metrics (reactivity and expected final allocation error) - into a single, principled formula. Simulation studies, grounded in real-world designs, demonstrate that lengths derived from our formula successfully stabilize the trial. The formula identifies a ''sweet spot'' that mitigates type-I error rate inflation and mean-squared error, preserving the advantages of higher power and patient benefit. This framework moves researchers from conjecture toward a systematic, reliable approach."}
{"id": "2511.20888", "categories": ["stat.ML", "cs.CC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20888", "abs": "https://arxiv.org/abs/2511.20888", "authors": ["Arthur Jacot"], "title": "Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size with ResNets", "comment": null, "summary": "This paper argues that DNNs implement a computational Occam's razor -- finding the `simplest' algorithm that fits the data -- and that this could explain their incredible and wide-ranging success over more traditional statistical methods. We start with the discovery that the set of real-valued function $f$ that can be $ε$-approximated with a binary circuit of size at most $cε^{-γ}$ becomes convex in the `Harder than Monte Carlo' (HTMC) regime, when $γ>2$, allowing for the definition of a HTMC norm on functions. In parallel one can define a complexity measure on the parameters of a ResNets (a weighted $\\ell_1$ norm of the parameters), which induce a `ResNet norm' on functions. The HTMC and ResNet norms can then be related by an almost matching sandwich bound. Thus minimizing this ResNet norm is equivalent to finding a circuit that fits the data with an almost minimal number of nodes (within a power of 2 of being optimal). ResNets thus appear as an alternative model for computation of real functions, better adapted to the HTMC regime and its convexity."}
{"id": "2511.20869", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20869", "abs": "https://arxiv.org/abs/2511.20869", "authors": ["Eric Crislip", "Mohammad Khalil", "Teresa Portone", "Oksana Chkrebtii", "Kyle Neal"], "title": "Closure Term Estimation in Spatiotemporal Models of Dynamical Systems", "comment": null, "summary": "Closure modeling - the statistical modeling of missing dynamics in the natural sciences and engineering - is a growing and active area of research. Existing methods for closure modeling are often computationally prohibitive, lack uncertainty quantification, or require noise-free observations of the temporal derivatives over the system state. We propose a novel, computationally efficient approach for the modeling and estimation of closure terms over the spatiotemporal domain that provides uncertainty quantification and is effective even when the observations of the system state are sparse or contain moderate levels of noise. The efficacy of our approach is demonstrated in both one and two spatial dimensions through numerical experiments using the Fisher-KPP reaction-diffusion equation and the advection-diffusion equation as exemplars."}
{"id": "2511.21526", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.21526", "abs": "https://arxiv.org/abs/2511.21526", "authors": ["Alexandra Carpentier", "Christophe Giraud", "Nicolas Verzelen"], "title": "Phase Transition for Stochastic Block Model with more than $\\sqrt{n}$ Communities (II)", "comment": null, "summary": "A fundamental theoretical question in network analysis is to determine under which conditions community recovery is possible in polynomial time in the Stochastic Block Model (SBM). When the number $K$ of communities remains smaller than $\\sqrt{n}$ --where $n$ denotes the number of nodes--, non-trivial community recovery is possible in polynomial time above, and only above, the Kesten--Stigum (KS) threshold, originally postulated using arguments from statistical physics.\n  When $K \\geq \\sqrt{n}$, Chin, Mossel, Sohn, and Wein recently proved that, in the \\emph{sparse regime}, community recovery in polynomial time is achievable below the KS threshold by counting non-backtracking paths. This finding led them to postulate a new threshold for the many-communities regime $K \\geq \\sqrt{n}$. Subsequently, Carpentier, Giraud, and Verzelen established the failure of low-degree polynomials below this new threshold across all density regimes, and demonstrated successful recovery above the threshold in certain moderately sparse settings. While these results provide strong evidence that, in the many community setting, the computational barrier lies at the threshold proposed in~Chin et al., the question of achieving recovery above this threshold still remains open in most density regimes.\n  The present work is a follow-up to~Carpentier et al., in which we prove Conjecture~1.4 stated therein by: \\\\ 1- Constructing a family of motifs satisfying specific structural properties; and\\\\ 2- Proving that community recovery is possible above the proposed threshold by counting such motifs.\\\\ Our results complete the picture of the computational barrier for community recovery in the SBM with $K \\geq \\sqrt{n}$ communities. They also indicate that, in moderately sparse regimes, the optimal algorithms appear to be fundamentally different from spectral methods."}
{"id": "2511.20960", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20960", "abs": "https://arxiv.org/abs/2511.20960", "authors": ["Soumojit Das", "Nairanjana Dasgupta", "Prashanta Dutta"], "title": "Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification", "comment": null, "summary": "Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain. We develop a geometric framework for post-hoc calibration of neural network probability outputs, treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric. Our approach yields Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems (Proposition~1) while extending naturally to multi-class settings -- providing a principled generalization that existing methods lack. Complementing calibration, we define geometric reliability scores based on Fisher--Rao distance and construct neutral zones for principled deferral of uncertain predictions.\n  Theoretical contributions include: (i) consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and (ii) tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework (calibration followed by reliability-based deferral) captures 72.5\\% of errors while deferring 34.5\\% of samples. Notably, this operational gain is achievable with any well-calibrated probability output; the contribution of geometric calibration lies in its theoretical foundations rather than empirical superiority over simpler alternatives. This work bridges information geometry and statistical learning, offering formal guarantees relevant to applications requiring rigorous validation."}
{"id": "2511.20876", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20876", "abs": "https://arxiv.org/abs/2511.20876", "authors": ["Huiyun Tang", "Long Feng", "Yang Li", "Feifei Wang"], "title": "Data Privatization in Vertical Federated Learning with Client-wise Missing Problem", "comment": null, "summary": "Vertical Federated Learning (VFL) often suffers from client-wise missingness, where entire feature blocks from some clients are unobserved, and conventional approaches are vulnerable to privacy leakage. We propose a Gaussian copulabased framework for VFL data privatization under missingness constraints, which requires no prior specification of downstream analysis tasks and imposes no restriction on the number of analyses. To privately estimate copula parameters, we introduce a debiased randomized response mechanism for correlation matrix estimation from perturbed ranks, together with a nonparametric privatized marginal estimation that yields consistent CDFs even under MAR. The proposed methods comprise VCDS for MCAR data, EVCDS for MAR data, and IEVCDS, which iteratively refines copula parameters to mitigate MAR-induced bias. Notably, EVCDS and IEVCDS also apply under MCAR, and the framework accommodates mixed data types, including discrete variables. Theoretically, we introduce the notion of Vertical Distributed Attribute Differential Privacy (VDADP), tailored to the VFL setting, establish corresponding privacy and utility guarantees, and investigate the utility of privatized data for GLM coefficient estimation and variable selection. We further establish asymptotic properties including estimation and variable selection consistency for VFL-GLMs. Extensive simulations and a real-data application demonstrate the effectiveness of the proposed framework."}
{"id": "2511.21595", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.21595", "abs": "https://arxiv.org/abs/2511.21595", "authors": ["Mauro Bernardi", "Antonio Canale", "Marco Stefanucci"], "title": "On the Degrees of Freedom of some Lasso procedures", "comment": null, "summary": "The effective degrees of freedom of penalized regression models quantify the actual amount of information used to generate predictions, playing a pivotal role in model evaluation and selection. Although a closed-form estimator is available for the Lasso penalty, adaptive extensions of widely used penalized approaches, including the Adaptive Lasso and Adaptive Group Lasso, have remained without analogous theoretical characterization. This paper presents the first unbiased estimator of the effective degrees of freedom for these methods, along with their main theoretical properties, for both orthogonal and non-orthogonal designs, derived within Stein's unbiased risk estimation framework. The resulting expressions feature inflation terms influenced by the regularization parameter, coefficient signs, and least-squares estimates. These advances enable more accurate model selection criteria and unbiased prediction error estimates, illustrated through synthetic and real data. These contributions offer a rigorous theoretical foundation for understanding model complexity in adaptive regression, bridging a critical gap between theory and practice."}
{"id": "2511.21115", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21115", "abs": "https://arxiv.org/abs/2511.21115", "authors": ["Lechen Feng", "Haoran Li", "Lucky Li", "Xingqiu Zhao"], "title": "Nonconvex Penalized LAD Estimation in Partial Linear Models with DNNs: Asymptotic Analysis and Proximal Algorithms", "comment": null, "summary": "This paper investigates the partial linear model by Least Absolute Deviation (LAD) regression. We parameterize the nonparametric term using Deep Neural Networks (DNNs) and formulate a penalized LAD problem for estimation. Specifically, our model exhibits the following challenges. First, the regularization term can be nonconvex and nonsmooth, necessitating the introduction of infinite dimensional variational analysis and nonsmooth analysis into the asymptotic normality discussion. Second, our network must expand (in width, sparsity level and depth) as more samples are observed, thereby introducing additional difficulties for theoretical analysis. Third, the oracle of the proposed estimator is itself defined through a ultra high-dimensional, nonconvex, and discontinuous optimization problem, which already entails substantial computational and theoretical challenges. Under such the challenges, we establish the consistency, convergence rate, and asymptotic normality of the estimator. Furthermore, we analyze the oracle problem itself and its continuous relaxation. We study the convergence of a proximal subgradient method for both formulations, highlighting their structural differences lead to distinct computational subproblems along the iterations. In particular, the relaxed formulation admits significantly cheaper proximal updates, reflecting an inherent trade-off between statistical accuracy and computational tractability."}
{"id": "2511.20884", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20884", "abs": "https://arxiv.org/abs/2511.20884", "authors": ["Qingyang Sun", "Jerome P. Reiter"], "title": "Differentially Private Fisher Randomization Tests for Binary Outcomes", "comment": "39 pages, 7 figures", "summary": "Across many disciplines, causal inference often relies on randomized experiments with binary outcomes. In such experiments, the Fisher randomization test provides exact, assumption-free tests for causal effects. Sometimes the outcomes are sensitive and must be kept confidential, for example, when they comprise physical or mental health measurements. Releasing test statistics or p-values computed with the confidential outcomes can leak information about the individuals in the study. Those responsible for sharing the analysis results may wish to bound this information leakage, which they can do by ensuring the released outputs satisfy differential privacy. In this article, we develop and compare several differentially private versions of the Fisher randomization test for binary outcomes. Specifically, we consider direct perturbation approaches that inject calibrated noise into test statistics or p-values, as well as a mechanism-aware, Bayesian denoising framework that explicitly models the privacy mechanism. We further develop decision-making procedures under privacy constraints, including a Bayes risk-optimal rule and a frequentist-calibrated significance test. Through theoretical results, simulation studies, and an application to the ADAPTABLE clinical trial, we demonstrate that our methods can achieve valid and interpretable causal inference while ensuring the differential privacy guarantee."}
{"id": "2511.21223", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21223", "abs": "https://arxiv.org/abs/2511.21223", "authors": ["Jasraj Singh", "Shelvia Wongso", "Jeremie Houssineau", "Badr-Eddine Chérief-Abdellatif"], "title": "Maxitive Donsker-Varadhan Formulation for Possibilistic Variational Inference", "comment": null, "summary": "Variational inference (VI) is a cornerstone of modern Bayesian learning, enabling approximate inference in complex models that would otherwise be intractable. However, its formulation depends on expectations and divergences defined through high-dimensional integrals, often rendering analytical treatment impossible and necessitating heavy reliance on approximate learning and inference techniques. Possibility theory, an imprecise probability framework, allows to directly model epistemic uncertainty instead of leveraging subjective probabilities. While this framework provides robustness and interpretability under sparse or imprecise information, adapting VI to the possibilistic setting requires rethinking core concepts such as entropy and divergence, which presuppose additivity. In this work, we develop a principled formulation of possibilistic variational inference and apply it to a special class of exponential-family functions, highlighting parallels with their probabilistic counterparts and revealing the distinctive mathematical structures of possibility theory."}
{"id": "2511.20980", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20980", "abs": "https://arxiv.org/abs/2511.20980", "authors": ["Giorgos Bakoyannis", "Aristofanis Rontogiannis", "Ying Zhang", "Wanzhu Tu", "Ann Mwangi", "Constantin T. Yiannoutsos"], "title": "Robustness intervals for competing risks analysis with causes of failure missing not at random", "comment": null, "summary": "Analysis of competing risks data is often complicated by the incomplete or selectively missing information on the cause of failure. Standard approaches typically assume that the cause of failure is missing at random (MAR), an assumption that is generally untestable and frequently implausible in observational studies. We propose a novel sensitivity analysis framework for the proportional cause-specific hazards model that accommodates missing-not-at-random (MNAR) scenarios. A sensitivity parameter is used to quantify the association between missingness and the unobserved cause of failure. Regression coefficients are estimated as functions of this parameter, and a simultaneous confidence band is constructed via a wild bootstrap procedure. This allows identification of a range of MNAR scenarios for which effects remain statistically significant; we refer to this range as a robustness interval. The validity of the proposed approach is justified both theoretically, via empirical process theory, and empirically, through simulation studies. We apply the method to the analysis of data from an HIV cohort study in sub-Saharan Africa, where a substantial proportion of causes of failure are missing and the MAR assumption is implausible. The analysis shows that key findings regarding risk factors for care interruption and mortality are robust across a broad spectrum of MNAR scenarios, underscoring the method's utility in situations with MNAR causes of failure."}
{"id": "2511.21526", "categories": ["stat.ML", "cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.21526", "abs": "https://arxiv.org/abs/2511.21526", "authors": ["Alexandra Carpentier", "Christophe Giraud", "Nicolas Verzelen"], "title": "Phase Transition for Stochastic Block Model with more than $\\sqrt{n}$ Communities (II)", "comment": null, "summary": "A fundamental theoretical question in network analysis is to determine under which conditions community recovery is possible in polynomial time in the Stochastic Block Model (SBM). When the number $K$ of communities remains smaller than $\\sqrt{n}$ --where $n$ denotes the number of nodes--, non-trivial community recovery is possible in polynomial time above, and only above, the Kesten--Stigum (KS) threshold, originally postulated using arguments from statistical physics.\n  When $K \\geq \\sqrt{n}$, Chin, Mossel, Sohn, and Wein recently proved that, in the \\emph{sparse regime}, community recovery in polynomial time is achievable below the KS threshold by counting non-backtracking paths. This finding led them to postulate a new threshold for the many-communities regime $K \\geq \\sqrt{n}$. Subsequently, Carpentier, Giraud, and Verzelen established the failure of low-degree polynomials below this new threshold across all density regimes, and demonstrated successful recovery above the threshold in certain moderately sparse settings. While these results provide strong evidence that, in the many community setting, the computational barrier lies at the threshold proposed in~Chin et al., the question of achieving recovery above this threshold still remains open in most density regimes.\n  The present work is a follow-up to~Carpentier et al., in which we prove Conjecture~1.4 stated therein by: \\\\ 1- Constructing a family of motifs satisfying specific structural properties; and\\\\ 2- Proving that community recovery is possible above the proposed threshold by counting such motifs.\\\\ Our results complete the picture of the computational barrier for community recovery in the SBM with $K \\geq \\sqrt{n}$ communities. They also indicate that, in moderately sparse regimes, the optimal algorithms appear to be fundamentally different from spectral methods."}
{"id": "2511.20985", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20985", "abs": "https://arxiv.org/abs/2511.20985", "authors": ["Xiaoya Wang", "Richard J. Cook", "Yeying Zhu", "Tugba Akkaya-Hocagil", "R. Colin Carter", "Sandra W. Jacobson", "Joseph L. Jacobson", "Louise M. Ryan"], "title": "Two-stage Estimation for Causal Inference Involving a Semi-continuous Exposure", "comment": null, "summary": "Methods for causal inference are well developed for binary and continuous exposures, but in many settings, the exposure has a substantial mass at zero-such exposures are called semi-continuous. We propose a general causal framework for such semi-continuous exposures, together with a novel two-stage estimation strategy. A two-part propensity structure is introduced for the semi-continuous exposure, with one component for exposure status (exposed vs unexposed) and another for the exposure level among those exposed, and incorporates both into a marginal structural model that disentangles the effects of exposure status and dose. The two-stage procedure sequentially targets the causal dose-response among exposed individuals and the causal effect of exposure status at a reference dose, allowing flexibility in the choice of propensity score methods in the second stage. We establish consistency and asymptotic normality for the resulting estimators, and characterise their limiting values under misspecification of the propensity score models. Simulation studies evaluate finite sample performance and robustness, and an application to a study of prenatal alcohol exposure and child cognition demonstrates how the proposed methods can be used to address a range of scientific questions about both exposure status and exposure intensity."}
{"id": "2511.21675", "categories": ["stat.ML", "cs.LG", "cs.SI", "econ.EM"], "pdf": "https://arxiv.org/pdf/2511.21675", "abs": "https://arxiv.org/abs/2511.21675", "authors": ["Sadegh Shirani", "Mohsen Bayati"], "title": "On Evolution-Based Models for Experimentation Under Interference", "comment": null, "summary": "Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification."}
{"id": "2511.21001", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21001", "abs": "https://arxiv.org/abs/2511.21001", "authors": ["Y. Xu", "T. Wu", "A. Van Dyne", "E. Lee", "L. Eyler", "X. Tu"], "title": "Semiparametric Models for Practice Effects in Longitudinal Cognitive Trajectories: Application to an Aging Cohort Study", "comment": "14 pages, 3 figures", "summary": "Background: True cognitive longitudinal decline can be obscured by repeated testing, which is called practice effects (PEs). We developed a modeling framework that aligns participants by baseline and estimates visit-specific PEs independently of age-related change.\n  Method: Using real data ($N=175$), we estimated within-subject correlations via linear mixed-effects modeling and applied these parameters to simulate longitudinal trajectories for healthy controls (HC) and individuals with schizophrenia (SZ). Simulations incorporated aging, diagnostic differences, and cumulative PE indicators. Generalized estimating equations (GEEs) were fit with and without PEs to compare model performance.\n  Results: Models that ignored PEs inflated estimates of cognitive stability and attenuated HC--SZ group differences. Including visit-specific PEs improved recovery of true trajectories and more accurately distinguished aging effects from learning-related gains. Interaction models further identified that PEs may differ by diagnosis or by age at baseline.\n  Conclusion: Practice effects meaningfully bias longitudinal estimates if left unmodeled. The proposed alignment-based GEE framework provides a principled method to estimate PEs and improves accuracy in both simulated and real-world settings.\n  Keywords: practice effects; repeat testing; serial testing; longitudinal testing; mild cognitive impairment; cognitive change."}
{"id": "2511.20711", "categories": ["stat.ME", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.20711", "abs": "https://arxiv.org/abs/2511.20711", "authors": ["José Camacho"], "title": "A Set of Rules for Model Validation", "comment": null, "summary": "The validation of a data-driven model is the process of assessing the model's ability to generalize to new, unseen data in the population of interest. This paper proposes a set of general rules for model validation. These rules are designed to help practitioners create reliable validation plans and report their results transparently. While no validation scheme is flawless, these rules can help practitioners ensure their strategy is sufficient for practical use, openly discuss any limitations of their validation strategy, and report clear, comparable performance metrics."}
{"id": "2511.21060", "categories": ["stat.ME", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.21060", "abs": "https://arxiv.org/abs/2511.21060", "authors": ["Vladimir Berman"], "title": "Zipf Distributions from Two-Stage Symbolic Processes: Stability Under Stochastic Lexical Filtering", "comment": "16 pages", "summary": "Zipf's law in language lacks a definitive origin, debated across fields. This study explains Zipf-like behavior using geometric mechanisms without linguistic elements. The Full Combinatorial Word Model (FCWM) forms words from a finite alphabet, generating a geometric distribution of word lengths. Interacting exponential forces yield a power-law rank-frequency curve, determined by alphabet size and blank symbol probability. Simulations support predictions, matching English, Russian, and mixed-genre data. The symbolic model suggests Zipf-type laws arise from geometric constraints, not communicative efficiency."}
{"id": "2511.21060", "categories": ["stat.ME", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.21060", "abs": "https://arxiv.org/abs/2511.21060", "authors": ["Vladimir Berman"], "title": "Zipf Distributions from Two-Stage Symbolic Processes: Stability Under Stochastic Lexical Filtering", "comment": "16 pages", "summary": "Zipf's law in language lacks a definitive origin, debated across fields. This study explains Zipf-like behavior using geometric mechanisms without linguistic elements. The Full Combinatorial Word Model (FCWM) forms words from a finite alphabet, generating a geometric distribution of word lengths. Interacting exponential forces yield a power-law rank-frequency curve, determined by alphabet size and blank symbol probability. Simulations support predictions, matching English, Russian, and mixed-genre data. The symbolic model suggests Zipf-type laws arise from geometric constraints, not communicative efficiency."}
{"id": "2511.21126", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21126", "abs": "https://arxiv.org/abs/2511.21126", "authors": ["Xiaozhu Zhang", "Nir Keret", "Ali Shojaie", "Armeen Taeb"], "title": "Convex Mixed-Integer Programming for Causal Additive Models with Optimization and Statistical Guarantees", "comment": null, "summary": "We study the problem of learning a directed acyclic graph from data generated according to an additive, non-linear structural equation model with Gaussian noise. We express each non-linear function through a basis expansion, and derive a maximum likelihood estimator with a group l0-regularization that penalizes the number of edges in the graph. The resulting estimator is formulated through a convex mixed-integer program, enabling the use of branch-and-bound methods to obtain a solution that is guaranteed to be accurate up to a pre-specified optimality gap. Our formulation can naturally encode background knowledge, such as the presence or absence of edges and partial order constraints among the variables. We establish consistency guarantees for our estimator in terms of graph recovery, even when the number of variables grows with the sample size. Additionally, by connecting the optimality guarantees with our statistical error bounds, we derive an early stopping criterion that allows terminating the branch-and-bound procedure while preserving consistency. Compared with existing approaches that either assume equal error variances, restrict to linear structural equation models, or rely on heuristic procedures, our method enjoys both optimization and statistical guarantees. Extensive simulations and real-data analysis show that the proposed method achieves markedly better graph recovery performance."}
{"id": "2511.21563", "categories": ["stat.CO", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.21563", "abs": "https://arxiv.org/abs/2511.21563", "authors": ["Sam Power", "Giorgos Vasdekis"], "title": "Some aspects of robustness in modern Markov Chain Monte Carlo", "comment": "52 pages, 29 Figures, Review article", "summary": "Markov Chain Monte Carlo (MCMC) is a flexible approach to approximate sampling from intractable probability distributions, with a rich theoretical foundation and comprising a wealth of exemplar algorithms. While the qualitative correctness of MCMC algorithms is often easy to ensure, their practical efficiency is contingent on the `target' distribution being reasonably well-behaved.\n  In this work, we concern ourself with the scenario in which this good behaviour is called into question, reviewing an emerging line of work on `robust' MCMC algorithms which can perform acceptably even in the face of certain pathologies.\n  We focus on two particular pathologies which, while simple, can already have dramatic effects on standard `local' algorithms. The first is roughness, whereby the target distribution varies so rapidly that the numerical stability of the algorithm is tenuous. The second is flatness, whereby the landscape of the target distribution is instead so barren and uninformative that one becomes lost in uninteresting parts of the state space. In each case, we formulate the pathology in concrete terms, review a range of proposed algorithmic remedies to the pathology, and outline promising directions for future research."}
{"id": "2511.21266", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21266", "abs": "https://arxiv.org/abs/2511.21266", "authors": ["Lotta M. Meijerink", "Artuur M. Leeuwenberg", "Jungyeon Choi", "Bas B. L. Penning de Vries", "Johannes A. Langendijk", "Judith G. M. van Loon", "Remi A. Nout", "Karel G. M. Moons", "Ewoud Schuit"], "title": "Treatment effect estimation by comparing observed and predicted outcomes: theory and practical illustration in radiotherapy", "comment": "13 pages, 1 figure", "summary": "Prediction models developed before the introduction of a new treatment may be used to estimate treatment effects of newly introduced treatments. One approach, known as model-based clinical evaluation in radiotherapy, does this by comparing observed outcomes under a new treatment with predicted outcomes had these patients received the standard treatment. This article clarifies the relevant conditions needed for valid average treatment effect estimation using this approach, using the potential outcomes framework and a practical case study."}
{"id": "2511.21278", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21278", "abs": "https://arxiv.org/abs/2511.21278", "authors": ["Huiyun Tang", "Feifei Wang", "Long Feng", "Yang Li"], "title": "Enterprise Profit Prediction Using Multiple Data Sources with Missing Values through Vertical Federated Learning", "comment": null, "summary": "Small and medium-sized enterprises (SMEs) play a crucial role in driving economic growth. Monitoring their financial performance and discovering relevant covariates are essential for risk assessment, business planning, and policy formulation. This paper focuses on predicting profits for SMEs. Two major challenges are faced in this study: 1) SMEs data are stored across different institutions, and centralized analysis is restricted due to data security concerns; 2) data from various institutions contain different levels of missing values, resulting in a complex missingness issue. To tackle these issues, we introduce an innovative approach named Vertical Federated Expectation Maximization (VFEM), designed for federated learning under a missing data scenario. We embed a new EM algorithm into VFEM to address complex missing patterns when full dataset access is unfeasible. Furthermore, we establish the linear convergence rate for the VFEM and establish a statistical inference framework, enabling covariates to influence assessment and enhancing model interpretability. Extensive simulation studies are conducted to validate its finite sample performance. Finally, we thoroughly investigate a real-life profit prediction problem for SMEs using VFEM. Our findings demonstrate that VFEM provides a promising solution for addressing data isolation and missing values, ultimately improving the understanding of SMEs' financial performance."}
{"id": "2511.21282", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21282", "abs": "https://arxiv.org/abs/2511.21282", "authors": ["Xinran Li"], "title": "Learning Across Experiments and Time: Tackling Heterogeneity in A/B Testing", "comment": null, "summary": "A/B testing plays a central role in data-driven product development, guiding launch decisions for new features and designs. However, treatment effect estimates are often noisy due to short horizons, early stopping, and slowly accumulating long-tail metrics, making early conclusions unreliable. A natural remedy is to pool information across related experiments, but naive pooling potentially fails: within experiments, treatment effects may evolve over time, so mixing early and late outcomes without accounting for nonstationarity induces bias; across experiments, heterogeneity in product, user population, or season dilutes the signal with unrelated noise. These issues highlight the need for pooling strategies that adapt to both temporal evolution and cross-experiment variability. To address these challenges, we propose a local empirical Bayes framework that adapts to both temporal and cross-experiment heterogeneity. Throughout an experiment's timeline, our method builds a tailored comparison set: time-aware within the experiment to respect nonstationarity, and context-aware across experiments to draw only from comparable counterparts. The estimator then borrows strength selectively from this set, producing stabilized treatment effect estimates that remain sensitive to both time dynamics and experimental context. Through theoretical analysis and empirical evaluation, we show that the proposed local pooling strategy consistently outperforms global pooling by reducing variance while avoiding bias. Our proposed framework enhances the reliability of A/B testing under practical constraints, thereby enabling more timely and informed decision-making."}
{"id": "2511.21376", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.21376", "abs": "https://arxiv.org/abs/2511.21376", "authors": ["Lukas Pin", "Stef Baas", "Gianmarco Caruso", "David S. Robertson", "Sofía S. Villar"], "title": "Informed Burn-In Decisions in RAR: Harmonizing Adaptivity and Inferential Precision Based on Study Setting", "comment": "18 pages, 3 figures, 4 tables", "summary": "Response-Adaptive Randomization (RAR) is recognized for its potential to deliver improvements in patient benefit. However, the utility of RAR is contingent on regularization methods to mitigate early instability and preserve statistical integrity. A standard regularization approach is the ''burn-in'' period, an initial phase of equal randomization before treatment allocation adapts based on accrued data. The length of this burn-in is a critical design parameter, yet its selection remains unsystematic and improvised, as no established guideline exists. A poorly chosen length poses significant risks: one that is too short leads to high estimation bias and type-I error rate inflation, while one that is too long impedes the intended patient and power benefits of using adaptation. The challenge of selecting the burn-in generalizes to a fundamental question: what is the statistically appropriate timing for the first adaptation? We introduce the first systematic framework for determining burn-in length. This framework synthesizes core factors - total sample size, problem difficulty, and two novel metrics (reactivity and expected final allocation error) - into a single, principled formula. Simulation studies, grounded in real-world designs, demonstrate that lengths derived from our formula successfully stabilize the trial. The formula identifies a ''sweet spot'' that mitigates type-I error rate inflation and mean-squared error, preserving the advantages of higher power and patient benefit. This framework moves researchers from conjecture toward a systematic, reliable approach."}
{"id": "2511.21534", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21534", "abs": "https://arxiv.org/abs/2511.21534", "authors": ["Matvey Ortyashov", "AmirEmad Ghassami"], "title": "A Sensitivity Analysis Framework for Causal Inference Under Interference", "comment": null, "summary": "In many applications of causal inference, the treatment received by one unit may influence the outcome of another, a phenomenon referred to as interference. Although there are several frameworks for conducting causal inference in the presence of interference, practitioners often lack the data necessary to adjust for its effects. In this paper, we propose a weighting-based sensitivity analysis framework that can be used to assess the systematic bias arising from ignoring interference. Unlike most of the existing literature, we allow for the presence of unmeasured confounding, and show that the combination of interference and unmeasured confounding is a notable challenge to causal inference. We also study a third factor contributing to systematic bias: lack of transportability. Our framework enables practitioners to assess the impact of these three issues simultaneously through several easily interpretable sensitivity parameters that can reflect a wide range of intuitions about the data."}
{"id": "2511.21553", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2511.21553", "abs": "https://arxiv.org/abs/2511.21553", "authors": ["Yacine Mohamed Idir", "Thomas Romary"], "title": "Efficient bayesian spatially varying coefficients modeling for censored data using the vecchia approximation", "comment": null, "summary": "Spatially varying coefficients (SVC) models allow for marginal effects to be non-stationary over space and thus offer a higher degree of flexibility with respect to standard geostatistical models with external drift. At the same time, SVC models have the advantage that they are easily interpretable. They offer a flexible framework for understanding how the relationships between dependent and independent variables vary across space. The most common methods for modelling such data are the Geographically Weighted Regression (GWR) and Bayesian Gaussian Process (Bayes-GP). The Bayesian SVC model, which assumes that the coefficients follow Gaussian processes, provides a rigorous approach to account for spatial non-stationarity. However, the computational cost of Bayes-GP models can be prohibitively high when dealing with large datasets or/and when using a large number of covariates, due to the repeated inversion of dense covariance matrices required at each Markov chain Monte Carlo (MCMC) iteration. In this study, we propose an efficient Bayes-GP modeling framework leveraging the Vecchia approximation to reduce computational complexity while maintaining accuracy. The proposed method is applied to a challenging soil pollution data set in Toulouse, France, characterized by a high degree of censorship (two-thirds censored observations) and spatial clustering. Our results demonstrate the ability of the Vecchia-based Bayes-GP model to capture spatially varying effects and provide meaningful insights into spatial heterogeneity, even under the constraints of censored data."}
{"id": "2511.21562", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21562", "abs": "https://arxiv.org/abs/2511.21562", "authors": ["Sang Kyu Lee", "Tongwu Zhang", "Hyokyoung G. Hong", "Haolei Weng"], "title": "StaRQR-K: False Discovery Rate Controlled Regional Quantile Regression", "comment": null, "summary": "Quantifying how genomic features influence different parts of an outcome distribution requires statistical tools that go beyond mean regression, especially in ultrahigh-dimensional settings. Motivated by the study of LINE-1 activity in cancer, we propose StaRQR-K, a stabilized regional quantile regression framework with model-X knockoffs for false discovery rate control. StaRQR-K identifies CpG sites whose methylation levels are associated with specific quantile regions of an outcome, allowing detection of heterogeneous and tail-sensitive effects. The method combines an efficient regional quantile sure independence screening procedure with a winsorizing-based model-X knockoff filter, providing false discovery rate (FDR) control for regional quantile regression. Simulation studies show that StaRQR-K achieves valid FDR control and substantially higher power than existing approaches. In an application to The Cancer Genome Atlas head and neck cancer cohort, StaRQR-K reveals quantile-region-specific associations between CpG methylation and LINE-1 activity that improve out-of-sample prediction and highlight genomic regions with known functional relevance."}
{"id": "2511.21595", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.21595", "abs": "https://arxiv.org/abs/2511.21595", "authors": ["Mauro Bernardi", "Antonio Canale", "Marco Stefanucci"], "title": "On the Degrees of Freedom of some Lasso procedures", "comment": null, "summary": "The effective degrees of freedom of penalized regression models quantify the actual amount of information used to generate predictions, playing a pivotal role in model evaluation and selection. Although a closed-form estimator is available for the Lasso penalty, adaptive extensions of widely used penalized approaches, including the Adaptive Lasso and Adaptive Group Lasso, have remained without analogous theoretical characterization. This paper presents the first unbiased estimator of the effective degrees of freedom for these methods, along with their main theoretical properties, for both orthogonal and non-orthogonal designs, derived within Stein's unbiased risk estimation framework. The resulting expressions feature inflation terms influenced by the regularization parameter, coefficient signs, and least-squares estimates. These advances enable more accurate model selection criteria and unbiased prediction error estimates, illustrated through synthetic and real data. These contributions offer a rigorous theoretical foundation for understanding model complexity in adaptive regression, bridging a critical gap between theory and practice."}
{"id": "2511.20960", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.20960", "abs": "https://arxiv.org/abs/2511.20960", "authors": ["Soumojit Das", "Nairanjana Dasgupta", "Prashanta Dutta"], "title": "Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification", "comment": null, "summary": "Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain. We develop a geometric framework for post-hoc calibration of neural network probability outputs, treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric. Our approach yields Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems (Proposition~1) while extending naturally to multi-class settings -- providing a principled generalization that existing methods lack. Complementing calibration, we define geometric reliability scores based on Fisher--Rao distance and construct neutral zones for principled deferral of uncertain predictions.\n  Theoretical contributions include: (i) consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and (ii) tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework (calibration followed by reliability-based deferral) captures 72.5\\% of errors while deferring 34.5\\% of samples. Notably, this operational gain is achievable with any well-calibrated probability output; the contribution of geometric calibration lies in its theoretical foundations rather than empirical superiority over simpler alternatives. This work bridges information geometry and statistical learning, offering formal guarantees relevant to applications requiring rigorous validation."}
{"id": "2511.21497", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.21497", "abs": "https://arxiv.org/abs/2511.21497", "authors": ["Andrew Golightly", "Sarah E. Heaps", "Chris Sherlock", "Laura E. Wadkin", "Darren J. Wilkinson"], "title": "Nested ensemble Kalman filter for static parameter inference in nonlinear state-space models", "comment": "29 pages", "summary": "The ensemble Kalman filter (EnKF) is a popular technique for performing inference in state-space models (SSMs), particularly when the dynamic process is high-dimensional. Unlike reweighting methods such as sequential Monte Carlo (SMC, i.e. particle filters), the EnKF leverages either the linear Gaussian structure of the SSM or an approximation thereof, to maintain diversity of the sampled latent states (the so-called ensemble members) via shifting-based updates. Joint parameter and state inference using an EnKF is typically achieved by augmenting the state vector with the static parameter. In this case, it is assumed that both parameters and states follow a linear Gaussian state-space model, which may be unreasonable in practice. In this paper, we combine the reweighting and shifting methods by replacing the particle filter used in the SMC^2 algorithm of Chopin et al., with the ensemble Kalman filter. Hence, parameter particles are weighted according to the estimated observed-data likelihood from the latest observation computed by the EnKF, and particle diversity is maintained via a resample-move step that targets the marginal parameter posterior under the EnKF. Extensions to the resulting algorithm are proposed, such as the use of a delayed acceptance kernel in the rejuvenation step and incorporation of nonlinear observation models. We illustrate the resulting methodology via several applications."}
