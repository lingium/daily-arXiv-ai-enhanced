<div id=toc></div>

# Table of Contents

- [stat.AP](#stat.AP) [Total: 7]
- [stat.ME](#stat.ME) [Total: 21]
- [stat.ML](#stat.ML) [Total: 13]
- [stat.CO](#stat.CO) [Total: 2]


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [1] [Order-Constrained Spectral Causality in Multivariate Time Series](https://arxiv.org/abs/2601.01216)
*Alejandro Rodriguez Dominguez*

Main category: stat.AP

TL;DR: 提出基于序约束谱非不变性的多元时间序列因果分析算子理论框架，通过源分量时间形变的二阶依赖算子敏感性定义方向性影响，在非线性、集体依赖场景下超越传统Granger因果分析


<details>
  <summary>Details</summary>
Motivation: 传统Granger因果分析主要关注线性、成对预测关系，难以捕捉复杂系统中的非线性、集体方向性依赖。需要一种更一般、可扩展的因果分析框架来处理多元时间序列中的系统级因果监测

Method: 基于序约束谱非不变性的算子理论框架：定义方向性影响为指定源分量在允许的序保持时间形变下二阶依赖算子的敏感性；使用正交不变谱泛函进行总结；通过基于移位的随机化方法建立非光滑supremum-infimum统计量的存在性、一致性和有效推断

Result: 在线性高斯假设下与线性Granger因果一致，但能捕捉非线性、集体方向性依赖；模拟显示正确尺寸和强大功效；金融应用显示方向性组织具有阶段性、压力依赖性，因果传播增强但保持多通道，主导因果枢纽快速重新分配，稳健传输通道稀疏且时域异质

Conclusion: 该框架为复杂系统提供了可扩展、可解释的因果分析工具，补充了相关性、因子和成对Granger式分析，特别适用于系统级因果监测和非线性依赖场景

Abstract: We introduce an operator-theoretic framework for causal analysis in multivariate time series based on order-constrained spectral non-invariance. Directional influence is defined as sensitivity of second-order dependence operators to admissible, order-preserving temporal deformations of a designated source component, yielding an intrinsically multivariate causal notion summarized through orthogonally invariant spectral functionals. Under linear Gaussian assumptions, the criterion coincides with linear Granger causality, while beyond this regime it captures collective and nonlinear directional dependence not reflected in pairwise predictability. We establish existence, uniform consistency, and valid inference for the resulting non-smooth supremum--infimum statistics using shift-based randomization that exploits order-induced group invariance, yielding finite-sample exactness under exact invariance and asymptotic validity under weak dependence without parametric assumptions. Simulations demonstrate correct size and strong power against distributed and bulk-dominated alternatives, including nonlinear dependence missed by linear Granger tests with appropriate feature embeddings. An empirical application to a high-dimensional panel of daily financial return series spanning major asset classes illustrates system-level causal monitoring in practice. Directional organization is episodic and stress-dependent, causal propagation strengthens while remaining multi-channel, dominant causal hubs reallocate rapidly, and statistically robust transmission channels are sparse and horizon-heterogeneous even when aggregate lead--lag asymmetry is weak. The framework provides a scalable and interpretable complement to correlation-, factor-, and pairwise Granger-style analyses for complex systems.

</details>


### [2] [Wildfire Evacuation Analysis Using Facebook Data: Evidence from Palisades and Eaton Fires](https://arxiv.org/abs/2601.01052)
*Shangkun Jiang,Ruggiero Lovreglio,Thomas J. Cova,Sangung Park,Susu Xu,Xilei Zhao*

Main category: stat.AP

TL;DR: 利用Facebook数据开发野火疏散分析框架，提出DEDI指标识别高风险低疏散社区，揭示疏散行为的时空异质性


<details>
  <summary>Details</summary>
Motivation: 野火频发对城乡交界社区构成严重威胁，理解疏散行为对应急规划至关重要。现有研究缺乏高分辨率数据来全面分析疏散行为模式。

Method: 使用2025年Palisades和Eaton火灾期间的高分辨率Facebook数据，开发综合框架计算疏散相关指标（遵从率、出发时间、延迟、OD流、旅行距离、目的地类型），并提出新的DEDI指标识别高风险低疏散区域。

Result: 发现时空异质性：靠近火源的居民疏散更早，夜间或延迟的疏散令导致遵从率低、延迟长；东西Altadena模式对比明显；DEDI识别社区社会脆弱性和火灾风险更高；大多数疏散目的地为居民区，长距离疏散集中在酒店和公共设施。

Conclusion: Facebook数据在数据驱动的野火疏散规划中具有巨大潜力，DEDI指标能帮助识别高风险低疏散社区，为针对性应急规划提供依据。

Abstract: The growing frequency and intensity of wildfires pose serious threats to communities in wildland-urban interface regions. Understanding evacuation behavior is critical for effective emergency planning. This study analyzes evacuation during the 2025 Palisades and Eaton Fires using high-resolution Facebook data. We propose a comprehensive framework to derive wildfire evacuation-related metrics, including compliance rate, departure timing, delay, origin-destination flows, travel distance, and destination types. A new metric, Damage-Evacuation Disparity Index (DEDI), identifies areas with severe structural damage but low evacuation compliance. Results reveal spatiotemporal heterogeneity: residents closer to the fire evacuated earlier, whereas late or nighttime orders led to lower compliance and longer delays. Contrasting patterns between East and West Altadena further illustrate this disparity. DEDI-identified communities exhibited higher social vulnerability and fire risk. Most evacuations concluded in residential areas, while longer trips concentrated in hotels and public facilities. These findings showcase the Facebook data's potential for data-driven wildfire evacuation planning.

</details>


### [3] [Discussion Network Formation and Evolution in an Online Professional Development Class: Evidence from a MOOC for K-12 Educators](https://arxiv.org/abs/2601.01117)
*Shuhan Ai*

Main category: stat.AP

TL;DR: 该研究使用网络分析方法，探讨了面向教育工作者的MOOC中同伴讨论网络的形成与演化机制，发现互惠性、传递闭合效应、小组同质性等因素显著影响网络结构。


<details>
  <summary>Details</summary>
Motivation: 随着面向教育工作者的MOOC（MOOC-Eds）激增，理解教育工作者在在线专业发展环境中如何互动和形成同伴网络变得日益重要。本研究旨在揭示在线专业发展背景下同伴支持学习的驱动机制。

Method: 研究分析了2013年春季面向美国和国际教育工作者的MOOC-Ed课程数据。采用横截面和时间指数随机图模型（ERGMs和TERGMs），分析两个网络子样本：最大连通分量（N=363）和具有三次或以上互动的活跃参与者（N=227）。

Result: 结果显示：1）强烈的互惠效应（参与者互动的可能性高6-9倍）；2）显著的传递闭合效应（与共享共同讨论伙伴的同伴形成联系的可能性高2倍以上）；3）分配讨论小组同质性是联系形成的最强预测因素；4）区域同质性和连接意愿也显著影响网络结构；5）讨论活动在课程中期达到高峰后急剧下降；6）网络结构从广泛分布的参与演变为紧密连接核心的集中互动。

Conclusion: 研究揭示了在线专业发展背景下同伴支持学习的驱动机制，为促进教育工作者在MOOC学习环境中的持续参与提供了设计启示。网络结构从广泛参与向紧密核心的演化表明需要设计策略来维持更广泛的参与度。

Abstract: Understanding how educators interact and form peer networks in online professional development contexts has become increasingly important as MOOCs for educators (MOOC-Eds) proliferate. This study examines peer discussion network formation and evolution in 'The Digital Learning Transition in K-12 Schools', a MOOC-Ed offered to U.S. and international educators in Spring 2013. Using cross-sectional and temporal exponential random graph models (ERGMs and TERGMs), the study analyzes two network subsamples: the largest connected component (N = 363) and active participants with three or more interactions (N = 227). Results reveal strong reciprocity and transitive closure effects across both networks, with participants six to nine times more likely to reciprocate interactions and over twice as likely to form ties with peers sharing common discussion partners. Assigned discussion group homophily emerged as the strongest predictor of tie formation, while regional homophily and willingness to connect also significantly influenced network structure. Temporal analysis showed discussion activity peaked mid-course before declining sharply, with network structure evolving from broadly distributed participation to concentrated interaction among a tightly connected core. These findings illuminate the mechanisms driving peer-supported learning in online professional development contexts and suggest design implications for fostering sustained educator engagement in MOOC-based learning environments.

</details>


### [4] [Model-Assisted Causal Inference for the Treatment Effect on Recurrent Events in the Presence of Terminal Events](https://arxiv.org/abs/2601.01245)
*Yiyuan Huang,Ling Zhou,Min Zhang,Peter X. K. Song*

Main category: stat.AP

TL;DR: 提出PR-MSMaT方法，在因果推断框架下解决终末事件存在时复发事件治疗效果评估问题，克服现有方法在时间变化复发率下的局限性


<details>
  <summary>Details</summary>
Motivation: 评估机械循环支持设备对终末期心衰患者复发事件风险的治疗效果，现有方法在存在终末事件时存在生存时间偏倚问题，且当复发事件率随时间变化时WA检验会出现I类错误膨胀

Method: 提出比例率边际结构模型辅助检验(PR-MSMaT)，在复发事件和终末事件可分离治疗效应的因果推断框架下，处理时间变化的复发事件率

Result: 模拟研究表明PR-MSMaT能正确控制I类错误，同时保持与WA检验相当的检验效能；应用于机械循环支持注册数据比较不同设备的术后胃肠道出血风险

Conclusion: PR-MSMaT填补了时间变化复发率下治疗效果评估的方法学空白，为存在终末事件的复发事件分析提供了更稳健的统计方法

Abstract: This paper is motivated by evaluating the benefits of patients receiving mechanical circulatory support (MCS) devices in end-stage heart failure management inference, in which hypothesis testing for a treatment effect on the risk of recurrent events is challenged in the presence of terminal events. Existing methods based on cumulative frequency unreasonably disadvantage longer survivors as they tend to experience more recurrent events. The While-Alive-based (WA) test has provided a solution to address this survival-length-bias problem, and it performs well when the recurrent event rate holds constant over time. However, if such a constant-rate assumption is violated, the WA test can exhibit an inflated type I error and inaccurate estimation of treatment effects. To fill this methodological gap, we propose a Proportional Rate Marginal Structural Model-assisted Test (PR-MSMaT) in the causal inference framework of separable treatment effects for recurrent and terminal events. Using the simulation study, we demonstrate that our PR-MSMaT can properly control type I error while gaining power comparable to the WA test under time-varying recurrent event rates. We employ PR-MSMaT to compare different MCS devices with the postoperative risk of gastrointestinal bleeding among patients enrolled in the Interagency Registry of Mechanically Assisted Circulatory Support program.

</details>


### [5] [Errors-in-variables regression for dependent data with estimated error covariance matrix: To prewhiten or not?](https://arxiv.org/abs/2601.01351)
*Jingkun Qiu,Hanyue Chen,Song Xi Chen*

Main category: stat.AP

TL;DR: 比较含误差变量回归模型中预白化与未预白化估计器的效率与计算成本，发现预白化不一定提高效率，但需要更大样本量保证渐近正态性，计算成本更高。


<details>
  <summary>Details</summary>
Motivation: 在高维误差协方差矩阵下，含误差变量回归模型的统计推断中，预白化处理在气候变化研究的最优指纹方法中被用于提高加权最小二乘估计效率，但预白化是否真正提高估计效率尚不明确。

Method: 比较预白化与未预白化估计器在估计效率和计算成本方面的表现，分析预白化操作对误差协方差矩阵估计所需样本量的要求。

Result: 预白化操作不一定能提高未预白化估计器的估计效率，但需要更大的集合样本量来保证渐近正态性，因此需要更多的计算资源。

Conclusion: 在含误差变量回归模型中，预白化处理虽然在某些情况下被采用，但实际应用中需权衡其有限的效率提升与显著增加的计算成本，特别是在高维误差协方差矩阵情况下。

Abstract: We consider statistical inference for errors-in-variables regression models with dependent observations under the high dimensionality of the error covariance matrix. It is tempting to prewhiten the model and data that had led to efficient weighted least squares estimation in the presence of the measurement errors, as being practised in the optimal fingerprinting approach in climate change studies. However, it is unclear to what extent the prewhitened estimator can improve the estimation efficiency of the unprewhitened estimator for errors-in-variables regression. We compare the prewhitening and unprewhitening estimators in terms of their estimation efficiency and computational cost. It shows that while the prewhitening operation does not necessarily improve the estimation efficiency of its unprewhitening counterpart, it demands more on the ensemble size needed in the error-covariance matrix estimation to ensure the asymptotic normality, and hence it would requires much more computationally resource.

</details>


### [6] [Cyclists Cardiac Conundrum](https://arxiv.org/abs/2601.02011)
*Andrew Nugent,Yi Ting Loo,Jack Buckingham*

Main category: stat.AP

TL;DR: 研究开发了检测心率数据中"间隙性"尖峰的方法，发现在高心率（>160bpm）下的尖峰与心律失常症状相关，支持了高强度运动时心率跳跃可能预示心律失常的假设。


<details>
  <summary>Details</summary>
Motivation: 耐力运动员心律失常风险增加，但运动时通常无法使用心电图监测。先前研究发现心率数据中的"间隙性"特征可能与心律失常相关，需要开发更精确的检测方法来验证这一假设。

Method: 开发多种心率尖峰检测方法，首先在模拟数据上比较性能，发现移动平均平滑后对残差设置恒定阈值的方法最有效。然后将该方法应用于168名运动员的真实心率数据，分析尖峰率与心律失常调查问卷结果的相关性。

Result: 整体尖峰率与调查结果无显著相关性，但仅考虑心率高于160bpm时的尖峰时，发现了显著相关性。这支持了只有在高心率下的跳跃才与心律失常相关的假设。

Conclusion: 高心率下的心率跳跃可能是心律失常的特征指标，表明需要进一步研究更好的心率数据特征表征方法，为运动员心律失常风险评估提供非侵入性监测工具。

Abstract: Arrhythmia is an abnormality of the heart's rhythm, caused by problems in the conductive system and resulting in irregular heartbeats. There is increasing evidence that undertaking frequent endurance sports training elevates one's risk of arrhythmia. Arrhythmia is diagnosed using an electrocardiogram (ECG) but this is not typically available to athletes while exercising. Previous research by Crickles investigates the usefulness of commonly available heart rate data in detecting signs of arrhythmia. It is hypothesised that a feature termed 'gappiness', defined by jumps in the heart rate while the athlete is under exertion, may be a characteristic of arrhythmia. A correlation was found between the proportion of 'gappy' activities and survey responses about heart rhythm problems. We develop on this measure by exploring various methods to detect spikes in heart rate data, allowing us to describe the extent of irregularity in an activity via the rate of spikes. We first compare the performance of these methods on simulated data, where we find that smoothing using a moving average and setting a constant threshold on the residuals is most effective. This method was then implemented on real data provided by Crickles from 168 athletes, where no significant correlation was found between the spike rates and survey responses. However, when considering only those spikes that occur above a heart rate of 160 beats per minute (bpm) a significant correlation was found. This supports the hypothesis that jumps at only high heart rates are informative of arrhythmia and indicates the need for further research into better measures to characterise features of heart rate data.

</details>


### [7] [Initial data analysis of the national German transplantation registry with a focus on kidney transplantation](https://arxiv.org/abs/2601.02226)
*Lukas Klein,Gunter Grieser,Carl-Ludwig Fischer-Fröhlich,Axel Rahmel,Henrik Stahl,Andreas Wienke,Antje Jahn-Eimermacher*

Main category: stat.AP

TL;DR: 对德国移植登记数据（TxReg）进行初步数据分析，重点关注2006-2016年成人首次单肾移植数据，评估数据质量、缺失模式、一致性和事件时间数据可用性，为未来研究提供预处理建议。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解德国移植登记数据（TxReg）的数据特征，为未来的数据分析提供基础，识别数据质量问题和挑战，确保后续研究的可靠性和有效性。

Method: 对2006-2016年间成人首次单肾移植的14,954名受者和9,964名供者数据进行初步数据分析，涵盖25个数据表。采用决策树方法识别缺失数据模式，进行流入流出分析评估变量插补潜力，识别多源变量并分析事件时间数据。

Result: 缺失数据比例差异很大，有些表几乎完整，有些缺失超过50%。识别出168个多源变量，存在数据不一致但也可用于缺失值插补。流入流出分析显示某些变量适合插补，某些不适合。事件时间分析结果强烈依赖于变量选择。

Conclusion: TxReg数据用于研究存在挑战，需要仔细的数据预处理。研究结果为未来数据分析提供了具体的预处理建议，特别是关于缺失数据处理、多源变量利用和事件时间变量选择方面。

Abstract: This study presents an Initial Data Analysis (IDA) of the German Transplantation Registry (TxReg) data for a better data understanding and to inform future data analyses. The IDA is focusing on data on first-time kidney-only transplantations in adult recipients from deceased donors between 2006 and 2016 and refers to data from 14,954 recipients and 9,964 donors across 25 tables. Investigated aspects include missing data patterns and structure, data consistency, and availability of event time data. Results show that missing data proportions vary widely, with some tables nearly complete while others have over 50% missing values. Missing data patterns are identified using a decision tree approach. An influx and outflux analysis demonstrates that some variables have high potential for imputing missing data, while others were less suitable for imputation. We identified 168 multi-sourced variables that are reported by multiple data providers in parallel leading to discrepancies for some variables but also providing opportunities for missing data imputation. Our findings on event time data demonstrate the importance of carefully selecting the variables used for event time analyses as results will strongly depend on this selection. In summary, our findings highlight the challenges when utilizing the TxReg data for research and provide recommendations for data preprocessing and analysis in future analyses.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [8] [Deep Deterministic Nonlinear ICA via Total Correlation Minimization with Matrix-Based Entropy Functional](https://arxiv.org/abs/2601.00904)
*Qiang Li,Shujian Yu,Liang Ma,Chen Ma,Jingyu Liu,Tulay Adali,Vince D. Calhoun*

Main category: stat.ME

TL;DR: 提出DDICA框架，使用深度神经网络和矩阵熵函数直接优化独立性准则，实现非线性盲源分离，在多种应用中表现出高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统ICA方法依赖线性混合假设，难以捕捉复杂非线性关系且在噪声环境中鲁棒性不足，需要新的非线性盲源分离方法。

Method: 提出深度确定性非线性独立成分分析（DDICA），基于深度神经网络框架，利用矩阵熵函数直接优化独立性准则，通过随机梯度下降训练，无需变分近似或对抗方案。

Result: 在模拟信号混合、高光谱图像解混、初级视觉感受野建模和静息态fMRI数据分析等应用中，DDICA能有效分离独立成分，具有高精度和噪声鲁棒性。

Conclusion: DDICA为各种信号处理任务中的盲源分离提供了鲁棒且通用的解决方案，特别适用于非线性混合和噪声环境。

Abstract: Blind source separation, particularly through independent component analysis (ICA), is widely utilized across various signal processing domains for disentangling underlying components from observed mixed signals, owing to its fully data-driven nature that minimizes reliance on prior assumptions. However, conventional ICA methods rely on an assumption of linear mixing, limiting their ability to capture complex nonlinear relationships and to maintain robustness in noisy environments. In this work, we present deep deterministic nonlinear independent component analysis (DDICA), a novel deep neural network-based framework designed to address these limitations. DDICA leverages a matrix-based entropy function to directly optimize the independence criterion via stochastic gradient descent, bypassing the need for variational approximations or adversarial schemes. This results in a streamlined training process and improved resilience to noise. We validated the effectiveness and generalizability of DDICA across a range of applications, including simulated signal mixtures, hyperspectral image unmixing, modeling of primary visual receptive fields, and resting-state functional magnetic resonance imaging (fMRI) data analysis. Experimental results demonstrate that DDICA effectively separates independent components with high accuracy across a range of applications. These findings suggest that DDICA offers a robust and versatile solution for blind source separation in diverse signal processing tasks.

</details>


### [9] [Beyond P-Values: Importing Quantitative Finance's Risk and Regret Metrics for AI in Learning Health Systems](https://arxiv.org/abs/2601.01116)
*Richik Chakraborty*

Main category: stat.ME

TL;DR: 传统医学证据框架基于随机对照试验和静态统计方法，但AI临床系统具有持续学习、自适应和非平稳环境的特点，需要新的证据评估框架来应对校准漂移、罕见严重故障和累积次优决策等风险。


<details>
  <summary>Details</summary>
Motivation: AI在临床环境中的广泛应用挑战了传统医学证据框架的基本假设。传统统计方法（如随机对照试验、频率假设检验、静态置信区间）是为固定干预措施在稳定条件下评估而设计的，而AI驱动的临床系统具有持续学习、自适应、在非平稳环境中运行的特点，其临床风险主要来自校准漂移、罕见严重故障和累积次优决策，而非平均错误率。

Method: 借鉴定量金融和在线决策理论中的风险理论概念，提出将自适应AI系统的医学证据重新框架化为时间索引的校准稳定性、有界下行风险和受控累积遗憾。该方法不取代随机试验或因果推断，而是通过解决部署后出现的风险和不确定性维度来补充它们。

Result: 提出了一个原则性的数学语言来评估持续学习下的AI驱动临床系统，为临床实践、研究设计和监管监督提供了新的评估框架。

Conclusion: 传统统计显著性概念不足以描述学习健康系统中的证据和安全性。需要新的框架来评估AI临床系统的动态风险特征，重点关注时间索引的校准稳定性、有界下行风险和受控累积遗憾，以应对部署后出现的独特挑战。

Abstract: The increasing deployment of artificial intelligence (AI) in clinical settings challenges foundational assumptions underlying traditional frameworks of medical evidence. Classical statistical approaches, centered on randomized controlled trials, frequentist hypothesis testing, and static confidence intervals, were designed for fixed interventions evaluated under stable conditions. In contrast, AI-driven clinical systems learn continuously, adapt their behavior over time, and operate in non-stationary environments shaped by evolving populations, practices, and feedback effects. In such systems, clinical harm arises less from average error rates than from calibration drift, rare but severe failures, and the accumulation of suboptimal decisions over time.
  In this perspective, we argue that prevailing notions of statistical significance are insufficient for characterizing evidence and safety in learning health systems. Drawing on risk-theoretic concepts from quantitative finance and online decision theory, we propose reframing medical evidence for adaptive AI systems in terms of time-indexed calibration stability, bounded downside risk, and controlled cumulative regret. We emphasize that this approach does not replace randomized trials or causal inference, but complements them by addressing dimensions of risk and uncertainty that emerge only after deployment. This framework provides a principled mathematical language for evaluating AI-driven clinical systems under continual learning and offers implications for clinical practice, research design, and regulatory oversight.

</details>


### [10] [Matrix Decomposition-Based Approach to Estimate the STARTS Model](https://arxiv.org/abs/2601.01163)
*Satoshi Usami*

Main category: stat.ME

TL;DR: 提出一种新的STARTS模型估计方法，通过两阶段估计程序结合矩阵分解因子分析和传统SEM，显著降低不当解风险。


<details>
  <summary>Details</summary>
Motivation: STARTS模型在应用中经常出现不当解问题，现有估计方法（如最大似然法、条件ML、最小二乘法）存在局限性，需要开发更稳健的估计方法。

Method: 采用两阶段估计程序：第一阶段基于特征值分解的矩阵分解因子分析（MDFA），第二阶段结合传统SEM估计原理，将STARTS模型重新表述为因子分析框架。

Result: 新方法比常用的最大似然法、条件ML和最小二乘法显著降低不当解风险，同时结果与ML相似；与贝叶斯估计相比，无需指定先验分布，在小时间点数情况下能有效减少偏差。

Conclusion: 该方法为STARTS模型和其他SEM提供了更稳健的估计方案，通过敏感性分析能更有效地界定数据的合理结论范围。

Abstract: We propose a new estimation method for the Stable Trait, Auto Regressive Trait, and State (STARTS) model, which is well known for its frequent occurrence of improper solutions. The proposed approach is implemented through a two-stage estimation procedure that combines matrix decomposition factor analysis (MDFA) based on eigenvalue decomposition with conventional SEM estimation principles. By reformulating the STARTS model within a factor-analytic framework, this study presents a novel way of applying MDFA in the context of structural equation modeling (SEM). Through a simulation study and an empirical application to ToKyo Teen Cohort data, the proposed method was shown to entail a substantially lower risk of improper solutions than commonly used maximum likelihood, conditional ML, and (unweighted) least squares estimators, while tending to yield solutions similar to those obtained by ML. Compared with Bayesian estimation, the proposed method does not require the specification of appropriate (weakly informative) prior distributions and may effectively mitigate bias issues that arise when the number of time points is small. Applying the proposed method, as well as conducting sensitivity analyses informed by it, will enable researchers to more effectively delineate the range of plausible conclusions from data in estimating the STARTS model and other SEMs.

</details>


### [11] [A Modified Bayesian Criterion for Model Selection in Mixed and Hierarchical Frameworks](https://arxiv.org/abs/2601.01190)
*Diogenes de Jesus Ramirez,Anderson Melchor Hernandez,Isabel Cristina Ramirez,Luis Raúl Pericchi*

Main category: stat.ME

TL;DR: 提出了一种针对混合模型和层次结构的改进贝叶斯信息准则，通过引入似然函数海森矩阵的行列式来考虑似然曲面的曲率，从而更精细地惩罚模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统模型选择准则（如BIC、AIC）在混合模型和层次结构等复杂模型选择中可能不够准确，特别是在小样本或存在噪声变量的情况下。需要一种能够考虑似然曲面几何信息的更精细的惩罚方法。

Method: 提出改进的BIC准则，将似然函数海森矩阵的行列式纳入经典贝叶斯因子中，通过曲率信息更精细地惩罚模型复杂度。该方法适用于混合模型和层次结构，并通过理论推导和模拟研究验证。

Result: 在包括线性和线性混合模型的广泛模拟研究中，该准则一致优于传统方法（BIC、AIC及其变体），特别是在小样本条件或存在噪声变量的情况下，表现出更稳健和准确的模型选择能力。

Conclusion: 将似然曲面的曲率信息整合到模型选择准则中，能够在复杂数据环境下实现更稳健和准确的模型判别，为混合模型和层次结构的模型选择提供了更有效的工具。

Abstract: In this work, we propose a modified Bayesian Information Criterion (BIC) specifically designed for mixture models and hierarchical structures. This criterion incorporates the determinant of the Hessian matrix of the log-likelihood function, thereby refining the classical Bayes Factor by accounting for the curvature of the likelihood surface. Such geometric information introduces a more nuanced penalization for model complexity. The proposed approach improves model selection, particularly under small-sample conditions or in the presence of noise variables. Through theoretical derivations and extensive simulation studies-including both linear and linear mixed models-we show that our criterion consistently outperforms traditional methods such as BIC, Akaike Information Criterion (AIC), and related variants. The results suggest that integrating curvature-based information from the likelihood landscape leads to more robust and accurate model discrimination in complex data environments.

</details>


### [12] [A Novel Multiple Imputation Approach For Parameter Estimation in Observation-Driven Time Series Models With Missing Data](https://arxiv.org/abs/2601.01259)
*Guilherme Pumi,Taiane Schaedler Prass,Douglas Krauthein Verdum*

Main category: stat.ME

TL;DR: 提出一种针对观测驱动模型(ODM)的新型多重插补方法，专门用于处理时间序列中的缺失数据，能够保持数据的统计特性和依赖结构。


<details>
  <summary>Details</summary>
Motivation: 时间序列中缺失数据处理复杂，通用插补方法会扭曲数据的方差和依赖结构等关键统计特性，导致估计偏差和误导性推断。在明确依赖序列依赖性的模型中，这些问题更加严重。

Method: 利用观测驱动模型中系统分量的迭代特性，通过缺失数据传播依赖结构，最小化对估计的影响。该方法适用于连续、离散和混合类型数据，同时保持关键分布和依赖特性。

Result: 通过GARMA模型的蒙特卡洛模拟评估，即使在高达70%缺失数据的情况下也能有效工作。在巴西南部储能比例的实际应用中证明了其实用性。

Conclusion: 提出的多重插补方法专门针对观测驱动模型设计，能够有效处理时间序列缺失数据，保持数据的统计特性，优于传统插补技术。

Abstract: Handling missing data in time series is a complex problem due to the presence of temporal dependence. General-purpose imputation methods, while widely used, often distort key statistical properties of the data, such as variance and dependence structure, leading to biased estimation and misleading inference. These issues become more pronounced in models that explicitly rely on capturing serial dependence, as standard imputation techniques fail to preserve the underlying dynamics. This paper proposes a novel multiple imputation method specifically designed for parameter estimation in observation-driven models (ODM). The approach takes advantage of the iterative nature of the systematic component in ODM to propagate the dependence structure through missing data, minimizing its impact on estimation. Unlike traditional imputation techniques, the proposed method accommodates continuous, discrete, and mixed-type data while preserving key distributional and dependence properties. We evaluate its performance through Monte Carlo simulations in the context of GARMA models, considering time series with up to 70\% missing data. An application to the proportion of stocked energy stored in South Brazil further demonstrates its practical utility.

</details>


### [13] [Adaptive Kernel Regression for Constrained Route Alignment: Theory and Iterative Data Sharpening](https://arxiv.org/abs/2601.01344)
*Shiyin Du,Yiting Chen,Wenzhi Yang,Qiong Li,Xiaoping Shi*

Main category: stat.ME

TL;DR: 本文提出了一种自适应Nadaraya-Watson核回归估计器，用于解决路线对齐设计中的固定航点约束问题，通过权重调优参数分离全局平滑和局部约束满足，并开发迭代数据锐化算法提高精度。


<details>
  <summary>Details</summary>
Motivation: 现有路线对齐设计方法主要依赖几何优化或控制理论样条框架，缺乏能够平衡全局平滑性和精确点约束的系统统计建模方法。固定航点约束问题需要路径精确通过特定坐标，而传统方法在处理这种约束时存在局限性。

Method: 提出自适应Nadaraya-Watson核回归估计器，引入航点特定权重调优参数，将全局平滑与局部约束满足解耦。开发迭代数据锐化算法，系统性地减少偏差同时保持核框架稳定性。建立理论基础，推导渐近偏差和方差，证明内部约束模型下的收敛性。

Result: 1D和2D轨迹规划的数值案例研究表明，该方法能有效平衡均方根误差和曲率平滑度。铁路和公路路线规划的实证应用验证了框架的实际效用。方法在复杂约束对齐设计问题上提供了稳定、理论可靠且计算高效的解决方案。

Conclusion: 本文为固定航点约束问题提供了统计建模新方法，通过自适应核回归框架平衡平滑性和约束满足，避免了传统方法的"锯齿"伪影，具有理论保证和实际应用价值。

Abstract: Route alignment design in surveying and transportation engineering frequently involves fixed waypoint constraints, where a path must precisely traverse specific coordinates. While existing literature primarily relies on geometric optimization or control-theoretic spline frameworks, there is a lack of systematic statistical modeling approaches that balance global smoothness with exact point adherence. This paper proposes an Adaptive Nadaraya-Watson (ANW) kernel regression estimator designed to address the fixed waypoint problem. By incorporating waypoint-specific weight tuning parameters, the ANW estimator decouples global smoothing from local constraint satisfaction, avoiding the "jagged" artifacts common in naive local bandwidth-shrinking strategies. To further enhance estimation accuracy, we develop an iterative data sharpening algorithm that systematically reduces bias while maintaining the stability of the kernel framework. We establish the theoretical foundation for the ANW estimator by deriving its asymptotic bias and variance and proving its convergence properties under the internal constraint model. Numerical case studies in 1D and 2D trajectory planning demonstrate that the method effectively balances root mean square error (RMSE) and curvature smoothness. Finally, we validate the practical utility of the framework through empirical applications to railway and highway route planning. In sum, this work provides a stable, theoretically grounded, and computationally efficient solution for complex, constrained alignment design problems.

</details>


### [14] [Wasserstein Distributionally Robust Rare-Event Simulation](https://arxiv.org/abs/2601.01642)
*Dohyun Ahn,Huiyi Chen,Lewen Zheng*

Main category: stat.ME

TL;DR: 提出DRIS方法，通过分布鲁棒重要性采样估计Wasserstein模糊集下的最坏情况罕见事件概率，实现计算高效且具有消失相对误差


<details>
  <summary>Details</summary>
Motivation: 传统罕见事件模拟技术需要精确分布假设，在分布不确定性存在时效果受限。需要开发能处理分布模型风险的罕见事件概率估计框架。

Method: 提出分布鲁棒重要性采样(DRIS)：基于Wasserstein模糊集定义最坏情况罕见事件概率，利用对偶表征开发计算高效方法，大幅降低对偶分量估计的方差。

Result: DRIS方法实现简单、采样成本低，最重要的是达到了消失相对误差——罕见事件模拟中最强但难以证明的效率保证。数值研究显示DRIS优于现有基准方法。

Conclusion: DRIS为处理分布不确定性的罕见事件概率估计提供了有效框架，具有计算高效性和理论保证，显著优于传统方法。

Abstract: Standard rare-event simulation techniques require exact distributional specifications, which limits their effectiveness in the presence of distributional uncertainty. To address this, we develop a novel framework for estimating rare-event probabilities subject to such distributional model risk. Specifically, we focus on computing worst-case rare-event probabilities, defined as a distributionally robust bound against a Wasserstein ambiguity set centered at a specific nominal distribution. By exploiting a dual characterization of this bound, we propose Distributionally Robust Importance Sampling (DRIS), a computationally tractable methodology designed to substantially reduce the variance associated with estimating the dual components. The proposed method is simple to implement and requires low sampling costs. Most importantly, it achieves vanishing relative error, the strongest efficiency guarantee that is notoriously difficult to establish in rare-event simulation. Our numerical studies confirm the superior performance of DRIS against existing benchmarks.

</details>


### [15] [Unsupervised dense random survival forests identify interpretable patient profiles with heterogeneous treatment benefit](https://arxiv.org/abs/2601.01380)
*Xingyu Li,Qing Liu,Tony Jiang,Hong Amy Xia,Peng Wei,Brian P. Hobbs*

Main category: stat.ME

TL;DR: 提出一种基于超密集随机生存森林的无监督机器学习方法，用于识别癌症治疗反应异质性的患者亚组，提高精准肿瘤学的治疗效果。


<details>
  <summary>Details</summary>
Motivation: 精准肿瘤学旨在为合适的患者提供最佳癌症治疗，最大化治疗效果。然而，基于随机临床试验识别可能从实验性癌症治疗中获益更多的患者亚组存在显著的分析挑战。

Method: 引入一种基于超密集随机生存森林（最多10万棵树）的无监督机器学习方法，配备新的分裂规则，专门针对治疗效果异质性进行优化。

Result: 广泛模拟证实该方法能够检测异质性患者反应，区分有无异质性的数据集，同时保持严格的1%第一类错误率。在III期随机临床试验数据集上验证，显示基于基线特征的治疗反应存在显著患者异质性。

Conclusion: 该方法稳健、可解释，能有效识别响应亚组，为精准肿瘤学中患者分层和治疗效果优化提供了有力工具。

Abstract: Precision oncology aims to prescribe the optimal cancer treatment to the right patients, maximizing therapeutic benefits. However, identifying patient subgroups that may benefit more from experimental cancer treatments based on randomized clinical trials presents a significant analytical challenge. To address this, we introduce a novel unsupervised machine learning approach based on very dense random survival forests (up to 100,000 trees), equipped with a new splitting rule that explicitly targets treatment-effect heterogeneity. This method is robust, interpretable, and effectively identifies responsive subgroups. Extensive simulations confirm its ability to detect heterogeneous patient responses and distinguish between datasets with and without heterogeneity, while maintaining a stringent Type I error rate of 1%. We further validate its performance using Phase III randomized clinical trial datasets, demonstrating significant patient heterogeneity in treatment response based on baseline characteristics.

</details>


### [16] [Personalizing black-box models for nonparametric regression with minimax optimality](https://arxiv.org/abs/2601.01432)
*Sai Li,Linjun Zhang*

Main category: stat.ME

TL;DR: 该论文研究了在样本稀缺情况下如何有效利用预训练模型进行个性化回归，提出了理论框架和算法，证明了方法的统计最优性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模预训练模型（如深度神经网络和大语言模型）的普及，如何将这些模型有效整合到下游任务中，特别是在样本稀缺的情况下实现个性化适配，成为了一个重要问题。

Method: 提出了小样本个性化理论框架，开发了能够将黑盒预训练模型整合到回归过程中的算法，建立了非参数回归中的个性化问题框架。

Result: 建立了小样本个性化问题的最小最大最优率，证明了所提方法能够达到这个最优率，阐明了在样本稀缺情况下利用预训练模型的统计优势，并提供了当预训练模型不具信息性时的鲁棒性保证。

Conclusion: 该研究为在数据稀缺情况下有效利用预训练模型提供了理论支持和实用方法，通过模拟实验和加州住房数据集的应用验证了方法的有效性。

Abstract: Recent advances in large-scale models, including deep neural networks and large language models, have substantially improved performance across a wide range of learning tasks. The widespread availability of such pre-trained models creates new opportunities for data-efficient statistical learning, provided they can be effectively integrated into downstream tasks. Motivated by this setting, we study few-shot personalization, where a pre-trained black-box model is adapted to a target domain using a limited number of samples. We develop a theoretical framework for few-shot personalization in nonparametric regression and propose algorithms that can incorporate a black-box pre-trained model into the regression procedure. We establish the minimax optimal rate for the personalization problem and show that the proposed method attains this rate. Our results clarify the statistical benefits of leveraging pre-trained models under sample scarcity and provide robustness guarantees when the pre-trained model is not informative. We illustrate the finite-sample performance of the methods through simulations and an application to the California housing dataset with several pre-trained models.

</details>


### [17] [Reduced-Rank Autoregressive Model for High-Dimensional Multivariate Network Time Series](https://arxiv.org/abs/2601.01510)
*Qi Lyu,Xiaoyu Zhang,Guodong Li,Di Wang*

Main category: stat.ME

TL;DR: 提出RRNAR模型，通过低秩变量子空间和可分离双线性结构捕捉多元网络时间序列中的跨变量溢出效应，避免维度灾难


<details>
  <summary>Details</summary>
Motivation: 现有网络自回归模型通常将节点视为标量过程，忽略了跨变量溢出效应，无法捕捉多元网络时间序列中的复杂交互

Method: 提出RRNAR模型，结合已知网络拓扑和学习低秩变量子空间的可分离双线性转移结构，使用ScaledGD算法进行估计

Result: 建立了非渐近误差界，发现网络诱导的维度祝福现象：稀疏网络中网络参数估计精度随网络规模增长而提高；在交通和服务器监控网络中显著优于单变量和非结构化基准

Conclusion: RRNAR模型能有效捕捉多元网络时间序列中的跨变量传播机制，在稀疏网络中表现出维度祝福特性，为复杂系统建模提供了新工具

Abstract: Multivariate network time series are ubiquitous in modern systems, yet existing network autoregressive models typically treat nodes as scalar processes, ignoring cross-variable spillovers. To capture these complex interactions without the curse of dimensionality, we propose the Reduced-Rank Network Autoregressive (RRNAR) model. Our framework introduces a separable bilinear transition structure that couples the known network topology with a learnable low-rank variable subspace. We estimate the model using a novel Scaled Gradient Descent (ScaledGD) algorithm, explicitly designed to bridge the gap between rigid network scalars and flexible factor components. Theoretically, we establish non-asymptotic error bounds under a novel distance metric. A key finding is a network-induced blessing of dimensionality: for sparse networks, the estimation accuracy for network parameters improves as the network size grows. Applications to traffic and server monitoring networks demonstrate that RRNAR significantly outperforms univariate and unstructured benchmarks by identifying latent cross-channel propagation mechanisms.

</details>


### [18] [Cubic lower record-based transmuted family of distributions: Theory, Estimation, Applications](https://arxiv.org/abs/2601.01583)
*Caner Tanış*

Main category: stat.ME

TL;DR: 提出立方低记录转换分布族，其中特殊指数分布作为替代模型，通过九种参数估计方法比较，发现最小绝对距离估计优于最大似然估计，真实数据分析显示该模型拟合效果最佳


<details>
  <summary>Details</summary>
Motivation: 提供一个新的分布族——立方低记录转换分布，作为传统指数分布的替代模型，以更好地拟合实际数据

Method: 提出立方低记录转换分布族，研究其统计特性，使用九种参数估计方法（包括最大似然估计和最小绝对距离估计等），通过蒙特卡洛模拟比较方法性能，并用两个真实数据集验证模型拟合效果

Result: 蒙特卡洛模拟显示最小绝对距离估计是最大似然估计的有价值替代方法；两个真实数据分析表明，提出的立方低记录转换分布比竞争模型拟合效果更好

Conclusion: 立方低记录转换分布族是一个有效的统计模型，其特殊指数分布形式在实际应用中表现优异，最小绝对距离估计方法在参数估计中具有优势

Abstract: In this study, a family of distributions called cubic lower record-based transmuted is provided. A special case of this family is proposed as an alternative exponential distribution. Several statistical properties are explored. We utilize nine different methods to estimate the parameters of the suggested distribution. In order to compare the performances of these methods, we consider a comprehensive Monte-Carlo simulation study. As a result of simulation study, we conclude that minimum absolute distance estimator is a valuable alternative to maximum likelihood estimator. Then, we carried out two real-world data examples to evaluate the fits of introduced distribution as well as its potential competitor ones. The findings of real-world data analysis show that the best-fitting distribution for both datasets is our model.

</details>


### [19] [Predictive Assessment and Comparison of Bayesian Survival Models for Cancer Recurrence](https://arxiv.org/abs/2601.01662)
*Saku Suorsa,Aki Vehtari*

Main category: stat.ME

TL;DR: 本文针对癌症复发研究中常见的复杂数据特征（如未建模的删失事件时间和时变效应变量），提出了新的贝叶斯生存模型预测评估与比较方法


<details>
  <summary>Details</summary>
Motivation: 癌症复发研究中存在未建模的删失事件时间和时变效应变量等复杂特征，而现有的贝叶斯生存模型预测检查与比较方法无法充分处理这些特征，需要填补这一空白

Method: 提出了针对性的预测评估与比较建议，涵盖多种不同场景和模型，并提供了开源软件实现和代码以支持结果复现和实际应用

Result: 开发了专门针对癌症复发研究的贝叶斯生存模型预测评估框架，提供了实用的工具和方法来应对复杂数据特征的挑战

Conclusion: 通过引入新的预测评估建议和开源实现，填补了贝叶斯生存模型在癌症复发研究中的方法学空白，为处理复杂数据特征提供了实用解决方案

Abstract: Complex data features, such as unmodelled censored event times and variables with time-dependent effects, are common in cancer recurrence studies and pose challenges for Bayesian survival modelling. However, current methodologies for predictive model checking and comparison often fail to adequately address these features. This paper bridges that gap by introducing new, targeted recommendations for predictive assessment and comparison of Bayesian survival models for cancer recurrence. Our recommendations cover a variety of different scenarios and models. Accompanying code together with our implementations to open source software help in replicating the results and applying our recommendations in practice.

</details>


### [20] [Bayesian mortality forecasting with a Conway--Maxwell--Poisson specification](https://arxiv.org/abs/2601.01686)
*Jackie Siaw Tze Wong,Emiliano A. Valdez*

Main category: stat.ME

TL;DR: 使用Conway-Maxwell-Poisson分布进行随机死亡率建模，相比传统泊松和负二项分布能更好处理不同离散程度的数据，通过贝叶斯框架和MCMC方法实现参数估计，在英格兰和威尔士男性死亡率数据上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统死亡率建模通常使用泊松或负二项分布，但这些分布无法灵活处理不同离散程度（欠离散、等离散、过离散）的死亡计数数据。需要一种更灵活的分布来更好地捕捉死亡率数据的变异性特征。

Method: 采用Conway-Maxwell-Poisson分布对死亡计数进行建模，该分布能灵活处理不同离散程度的数据。建立贝叶斯框架，将离散水平作为未知参数，使用Gamma先验分布，通过马尔可夫链蒙特卡洛方法进行模型校准，并进行敏感性分析评估先验设定的稳健性。

Result: 在英格兰和威尔士男性死亡率数据的实证研究中，基于CMP的模型相比传统泊松和负二项模型，对现有数据和未来预测都提供了更好的拟合效果，特别是在数据呈现过离散特征时表现更优。

Conclusion: CMP分布为随机死亡率建模提供了一个灵活且有效的框架，能够更好地处理不同离散程度的死亡计数数据，贝叶斯方法确保了参数、过程和分布不确定性的稳健整合，模型在实证应用中表现出优越性能。

Abstract: This paper presents a novel approach to stochastic mortality modelling by using the Conway--Maxwell--Poisson (CMP) distribution to model death counts. Unlike standard Poisson or negative binomial distributions, the CMP is a more adaptable choice because it can account for different levels of variability in the data, a feature known as dispersion. Specifically, it can handle data that are underdispersed (less variable than expected), equidispersed (as variable as expected), and overdispersed (more variable than expected). We develop a Bayesian formulation that treats the dispersion level as an unknown parameter, using a Gamma prior to enable a robust and coherent integration of the parameter, process, and distributional uncertainty. The model is calibrated using Markov chain Monte Carlo (MCMC) methods, with model performance evaluated using standard statistical criteria such as residual analysis and scoring rules. An empirical study using England and Wales male mortality data shows that our CMP-based models provide a better fit for both existing data and future predictions compared to traditional Poisson and negative binomial models, particularly when the data exhibit overdispersion. Finally, we conduct a sensitivity analysis with respect to prior specification to assess robustness.

</details>


### [21] [Varying-Coefficient Mixture of Experts Model](https://arxiv.org/abs/2601.01699)
*Qicheng Zhao,Celia M. T. Greenwood,Qihuang Zhang*

Main category: stat.ME

TL;DR: 提出了VCMoE模型，允许混合专家模型中所有系数随索引变量变化，适用于纵向、空间等动态数据，建立了可识别性、一致性理论，开发了标签一致EM算法，并进行了基因表达数据分析。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家模型通常假设常数系数，无法处理纵向、空间等动态场景中协变量影响和潜在子群结构随时间/空间变化的问题。需要开发能捕捉系数动态变化的模型。

Method: 提出VCMoE模型，允许门控函数和专家模型中所有系数随索引变量变化。建立模型可识别性和一致性理论，开发标签一致EM算法进行估计，构建同时置信带和广义似然比检验进行推断。

Result: 模拟研究显示模型具有良好有限样本性能，偏差可接受，覆盖率满意。应用于小鼠胚胎单核基因表达数据，成功刻画了神经元两个潜在细胞亚群中基因表达关联的时间动态。

Conclusion: VCMoE模型能有效分析动态场景中的异质性数据，捕捉系数随时间/空间的变化，为纵向、空间等动态数据提供了灵活的建模框架。

Abstract: Mixture-of-Experts (MoE) is a flexible framework that combines multiple specialized submodels (``experts''), by assigning covariate-dependent weights (``gating functions'') to each expert, and have been commonly used for analyzing heterogeneous data. Existing statistical MoE formulations typically assume constant coefficients, for covariate effects within the expert or gating models, which can be inadequate for longitudinal, spatial, or other dynamic settings where covariate influences and latent subpopulation structure evolve across a known dimension. We propose a Varying-Coefficient Mixture of Experts (VCMoE) model that allows all coefficient effects in both the gating functions and expert models to vary along an indexing variable. We establish identifiability and consistency of the proposed model, and develop an estimation procedure, label-consistent EM algorithm, for both fully functional and hybrid specifications, along with the corresponding asymptotic distributions of the resulting estimators. For inference, simultaneous confidence bands are constructed using both asymptotic theory for the maximum discrepancy between the estimated functional coefficients and their true counterparts, and with bootstrap methods. In addition, a generalized likelihood ratio test is developed to examine whether a coefficient function is genuinely varying across the index variable. Simulation studies demonstrate good finite-sample performance, with acceptable bias and satisfactory coverage rates. We illustrate the proposed VCMoE model using a dataset of single nucleus gene expression in embryonic mice to characterize the temporal dynamics of the associations between the expression levels of genes Satb2 and Bcl11b across two latent cell subpopulations of neurons, yielding results that are consistent with prior findings.

</details>


### [22] [On regional treatment effect assessment using robust MAP priors](https://arxiv.org/abs/2601.01811)
*Xin Zhang,Hui Zhang,Satrajit Roychoudhury*

Main category: stat.ME

TL;DR: 本文提出了一种用于区域治疗效果评估的鲁棒MAP先验闭式近似方法，显著降低了计算负担，并证明MAP先验比幂先验更适合构建混合先验的信息成分。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯动态借用已成为评估区域治疗效果一致性的重要工具，但鲁棒元分析预测（MAP）先验在区域治疗效果评估中应用不足，主要由于其先验设定和后验计算的复杂性。

Method: 通过开发鲁棒MAP先验后验分布的闭式近似，利用其与幂先验的关系，大幅减少了识别所需操作特性先验参数的计算负担。

Result: 提出的方法显著降低了计算复杂度，使MAP先验能够高效应用于区域一致性评估，并且证明MAP先验比幂先验更适合构建混合先验的信息成分。

Conclusion: 鲁棒MAP先验是区域治疗效果评估中构建混合先验信息成分的有吸引力的选择，通过提出的闭式近似方法，能够实现高效透明的贝叶斯动态借用设计。

Abstract: Bayesian dynamic borrowing has become an increasingly important tool for evaluating the consistency of regional treatment effects which is a key requirement for local regulatory approval of a new drug. It helps increase the precision of regional treatment effect estimate when regional and global data are similar, while guarding against potential bias when they differ. In practice, the two-component mixture prior, of which one mixture component utilizes the power prior to incorporate external data, is widely used. It allows convenient prior specification, analytical posterior computation, and fast evaluation of operating characteristics. Though the robust meta-analytical-predictive (MAP) prior is broadly used with multiple external data sources, it remains underutilized for regional treatment effect assessment (typically only one external data source is available) due to its inherit complexity in prior specification and posterior computation. In this article, we illustrate the applicability of the robust MAP prior in the regional treatment effect assessment by developing a closed-form approximation for its posterior distribution while leveraging its relationship with the power prior. The proposed methodology substantially reduces the computational burden of identifying prior parameters for desired operating characteristics. Moreover, we have demonstrated that the MAP prior is an attractive choice to construct the informative component of the mixture prior compared to the power prior. The advantage can be explained through a Bayesian hypothesis testing perspective. Using a real-world example, we illustrate how our proposed method enables efficient and transparent development of a Bayesian dynamic borrowing design to show regional consistency.

</details>


### [23] [Causal Network Recovery in Perturb-seq Experiments Using Proxy and Instrumental Variables](https://arxiv.org/abs/2601.01830)
*Kwangmoon Park,Hongzhe Li*

Main category: stat.ME

TL;DR: 提出新方法从Perturb-seq数据中恢复因果基因网络，能处理未观测的混杂因素


<details>
  <summary>Details</summary>
Motivation: Perturb-seq技术能提供基因调控的因果洞察，但未观测的混杂因素（如实验设计限制导致的未观测基因）可能使因果推断产生偏差，现有方法缺乏明确处理这种混杂因素的能力

Method: 利用代理变量和工具变量策略，挖掘扰动中蕴含的丰富信息，实现无偏的有向无环图估计

Result: 模拟研究和K562细胞的CRISPR干扰实验分析表明，该方法优于忽略未测量混杂的基线方法，能更准确、更生物相关地恢复真实的基因因果DAG

Conclusion: 该方法为从Perturb-seq数据中稳健恢复因果基因网络提供了有效框架，特别适用于存在未观测混杂因素的情况

Abstract: Emerging single-cell technologies that integrate CRISPR-based genetic perturbations with single-cell RNA sequencing, such as Perturb-seq, have substantially advanced our understanding of gene regulation and causal influence of genes. While Perturb-seq data provide valuable causal insights into gene-gene interactions, statistical concerns remain regarding unobserved confounders that may bias inference. These latent factors may arise not only from intrinsic molecular features of regulatory elements encoded in Perturb-seq experiments, but also from unobserved genes arising from cost-constrained experimental designs. Although methods for analyzing largescale Perturb-seq data are rapidly maturing, approaches that explicitly account for such unobserved confounders in learning the causal gene networks are still lacking. Here, we propose a novel method to recover causal gene networks from Perturb-seq experiments with robustness to arbitrarily omitted confounders. Our framework leverages proxy and instrumental variable strategies to exploit the rich information embedded in perturbations, enabling unbiased estimation of the underlying directed acyclic graph (DAG) of gene expressions. Simulation studies and analyses of CRISPR interference experiments of K562 cells demonstrate that our method outperforms baseline approaches that ignore unmeasured confounding, yielding more accurate and biologically relevant recovery of the true gene causal DAGs.

</details>


### [24] [Spatio-temporal modeling and forecasting with Fourier neural operators](https://arxiv.org/abs/2601.01813)
*Pratik Nag,Andrew Zammit-Mangion,Sumeetpal Singh,Noel Cressie*

Main category: stat.ME

TL;DR: 该论文提出使用傅里叶神经算子构建统计动态时空模型进行预测，相比传统高斯过程等方法能更好地捕捉复杂时空依赖关系。


<details>
  <summary>Details</summary>
Motivation: 传统统计过程模型（如高斯过程）难以捕捉动态物理和生物现象中的环境异质性和复杂相互作用，需要更灵活的建模方法。

Method: 使用傅里叶神经算子构建统计动态时空模型，FNO能够高效近似线性或非线性偏微分方程的解算子，无需显式了解底层PDE。

Result: 通过非线性PDE模拟和真实数据（大西洋海表温度、欧洲降水）验证，FNO-DST模型相比现有最先进方法能提供更准确的预测和有效的量化不确定性。

Conclusion: FNO为基础的动态时空统计建模能够有效捕捉复杂现实世界的时空依赖关系，提供准确预测和可靠的不确定性量化。

Abstract: Spatio-temporal process models are often used for modeling dynamic physical and biological phenomena that evolve across space and time. These phenomena may exhibit environmental heterogeneity and complex interactions that are difficult to capture using traditional statistical process models such as Gaussian processes. This work proposes the use of Fourier neural operators (FNOs) for constructing statistical dynamical spatio-temporal models for forecasting. An FNO is a flexible mapping of functions that approximates the solution operator of possibly unknown linear or non-linear partial differential equations (PDEs) in a computationally efficient manner. It does so using samples of inputs and their respective outputs, and hence explicit knowledge of the underlying PDE is not required. Through simulations from a nonlinear PDE with known solution, we compare FNO forecasts to those from state-of-the-art statistical spatio-temporal-forecasting methods. Further, using sea surface temperature data over the Atlantic Ocean and precipitation data across Europe, we demonstrate the ability of FNO-based dynamic spatio-temporal (DST) statistical modeling to capture complex real-world spatio-temporal dependencies. Using collections of testing instances, we show that the FNO-DST forecasts are accurate with valid uncertainty quantification.

</details>


### [25] [A neighbour selection approach for identifying differential networks in conditional functional graphical models](https://arxiv.org/abs/2601.02292)
*Alessia Mapelli,Laura Carini,Francesca Ieva,Sara Sommariva*

Main category: stat.ME

TL;DR: 提出一种基于函数对函数回归的邻域选择方法，用于估计条件高斯函数图模型，能够处理协变量对功能连接的影响，提供可解释的系数并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 从EEG数据估计大脑功能连接对医学研究和诊断很重要。这些连接可能随个体差异和协变量（如年龄、精神状态、疾病严重程度）而变化，现有方法难以直接估计协变量特定的连接模式并提供可解释的系数。

Method: 提出基于函数对函数回归的邻域选择方法，用于表征条件高斯函数图模型。提供全自动、数据驱动的程序来推断观测函数变量间的条件依赖结构。成对交互可直接识别并允许随协变量变化，支持连续和离散协变量，产生可直接解释的系数。

Result: 通过广泛的模拟研究和实验EEG数据应用评估方法。结果显示相比现有方法有明显优势，包括更高的估计精度和显著降低的计算成本，特别是在高维设置中。

Conclusion: 该方法能够有效估计协变量特定的功能连接模式，提供可解释的系数来区分协变量引起的交互强度增加或减少，在计算效率和估计精度方面优于现有方法。

Abstract: Estimation of brain functional connectivity from EEG data is of great importance both for medical research and diagnosis. It involves quantifying the conditional dependencies among the activity of different brain areas from the time-varying electric field recorded by sensors placed outside the scalp. These dependencies may vary within and across individuals and be influenced by covariates such as age, mental status, or disease severity. Motivated by this problem, we propose a novel neighbour selection approach based on functional-on-functional regression for the characterization of conditional Gaussian functional graphical models. We provide a fully automated, data-driven procedure for inferring conditional dependence structures among observed functional variables. In particular, pairwise interactions are directly identified and allowed to vary as a function of covariates, enabling covariate-specific modulation of connectivity patterns. Our proposed method accommodates an arbitrary number of continuous and discrete covariates. Moreover, unlike existing methods for direct estimation of differential graphical models, the proposed approach yields directly interpretable coefficients, allowing discrimination between covariate-induced increases and decreases in interaction strength. The methodology is evaluated through extensive simulation studies and an application to experimental EEG data. The results demonstrate clear advantages over existing approaches, including higher estimation accuracy and substantially reduced computational cost, especially in high-dimensional settings.

</details>


### [26] [Causal inference for censored data with continuous marks](https://arxiv.org/abs/2601.01854)
*Lianqiang Qu,Long Lv,Liuquan Sun*

Main category: stat.ME

TL;DR: 提出一个针对带有连续标记的删失数据的因果推断框架，将经典竞争风险模型中的失败原因扩展为连续标记，并定义标记特异性处理效应


<details>
  <summary>Details</summary>
Motivation: 在删失数据中，当失效时间由连续变量（标记）标记时，由于标记的连续性质，每个特定标记的观测数据稀疏，使得因果关系的识别和估计变得困难

Method: 在潜在结果框架下定义新的标记特异性处理效应，提出局部平滑因果估计量，并建立其渐近性质

Result: 通过模拟研究和抗体介导预防试验的真实数据集评估了该方法

Conclusion: 该框架为带有连续标记的删失数据提供了有效的因果推断方法，解决了标记数据稀疏带来的挑战

Abstract: This paper presents a framework for causal inference in the presence of censored data, where the failure time is marked by a continuous variable known as a mark. The mark can be viewed as an extension of the failure cause in the classical competing risks model where the cause of failure is replaced by a continuous mark only observed at uncensored failure times. Due to the continuous nature of the marks, observations at each specific mark are sparse, making the identification and estimation of causality a challenging task. To address this issue, we define a new mark-specific treatment effect within the potential outcomes framework and characterize its identifying conditions. We then propose a local smoothing causal estimand and establish its asymptotic properties. We evaluate our method using simulation studies as well as a real dataset from the Antibody Mediated Prevention trials.

</details>


### [27] [Simulation of warping processes with applications to temperature data](https://arxiv.org/abs/2601.02154)
*Nolwenn Le Méhauté,Jean-François Coeurjolly,Marie-Hélène Descary*

Main category: stat.ME

TL;DR: 提出三种连续扭曲过程的模拟方法，包括基于随机化经验累积分布函数的新算法，用于生成具有指定期望和可控方差的扭曲过程


<details>
  <summary>Details</summary>
Motivation: 曲线配准在函数数据分析中通过扭曲函数分离振幅和相位变化至关重要，准确模拟扭曲过程对于开发正确处理函数数据相位变异的统计方法必不可少

Method: 研究比较三种方法：两种现有方法和基于随机化经验累积分布函数的新算法，为每种方法提供操作描述并建立模拟过程前两矩的理论结果

Result: 数值研究验证了理论发现并突出了三种方法各自的优点，最后应用于蒙特利尔温度分布分析，基于从温度分位数函数估计的扭曲过程进行模拟实现

Conclusion: 提出了模拟连续扭曲过程的有效方法，特别是基于随机化经验累积分布函数的新算法，为函数数据分析中正确处理相位变异提供了重要工具

Abstract: Curve registration plays a major role in functional data analysis by separating amplitude and phase variation through warping functions and the accurate simulation of warping processes is essential for developing statistical methods that properly account for phase variability in functional data. In this paper, we focus on the simulation of continuous warping processes with a prescribed expectation and a controllable variance. We study and compare three procedures, including two existing methods and a new algorithm based on randomized empirical cumulative distribution functions. For each approach, we provide an operational description and establish theoretical results for the first two moments of the simulated processes. A numerical study illustrates the theoretical findings and highlights the respective merits of the three methods. Finally, we present an application to the analysis of temperature distributions in Montreal based on simulated realizations from a warping process estimated from temperature quantile functions.

</details>


### [28] [Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction](https://arxiv.org/abs/2601.02322)
*Shuozhi Zuo,Yixin Wang*

Main category: stat.ME

TL;DR: 论文提出环境自适应协变量选择(EACS)算法，通过分析目标环境中协变量的分布特征来动态选择预测变量，在分布偏移下优于静态因果、不变性和ERM方法。


<details>
  <summary>Details</summary>
Motivation: 传统OOD预测方法（如因果/不变性学习）在实践中常不如经验风险最小化(ERM)，作者发现这是因为真实原因可能未被完全观测，此时非因果的虚假协变量可作为未观测原因的代理变量，但代理关系在特定分布偏移下会失效。

Method: 提出环境自适应协变量选择(EACS)算法：1) 从目标OOD环境的未标记数据中提取协变量分布的"特征签名"；2) 根据这些签名判断哪些代理协变量在当前环境中仍然可靠；3) 将环境级协变量摘要映射到环境特定的协变量集；4) 可融入先验因果知识作为约束。

Result: 在模拟和应用数据集上的实验表明，EACS在各种分布偏移下始终优于静态因果预测器、不变性预测器和ERM基准方法。

Conclusion: 最优预测协变量集不是通用的，而是取决于遇到的特定分布偏移类型。通过分析目标环境中协变量的可观测分布特征，可以自适应地选择可靠的预测变量，显著提升OOD预测性能。

Abstract: Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise when only a subset of the true causes of the outcome is observed. In these settings, non-causal spurious covariates can serve as informative proxies for unobserved causes and substantially improve prediction, except under distribution shifts that break these proxy relationships. Consequently, the optimal set of predictive covariates is neither universal nor necessarily exhibits invariant relationships with the outcome across all environments, but instead depends on the specific type of shift encountered. Crucially, we observe that different covariate shifts induce distinct, observable signatures in the covariate distribution itself. Moreover, these signatures can be extracted from unlabeled data in the target OOD environment and used to assess when proxy covariates remain reliable and when they fail. Building on this observation, we propose an environment-adaptive covariate selection (EACS) algorithm that maps environment-level covariate summaries to environment-specific covariate sets, while allowing the incorporation of prior causal knowledge as constraints. Across simulations and applied datasets, EACS consistently outperforms static causal, invariant, and ERM-based predictors under diverse distribution shifts.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [29] [Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights](https://arxiv.org/abs/2601.01029)
*Zeyu Bian,Max Biggs,Ruijiang Gao,Zhengling Qi*

Main category: stat.ML

TL;DR: 提出使用观测数据审计AI决策消费者剩余影响的实用框架，通过随机化定价和累积倾向权重避免需求函数显式估计，引入双重稳健估计器ACPW，支持灵活机器学习方法


<details>
  <summary>Details</summary>
Motivation: 传统方法通过估计需求函数计算消费者剩余存在实践困难：参数模型易误设，非参数方法数据需求大收敛慢。需要开发更实用的AI决策审计框架

Method: 利用算法定价的随机性（探索-利用权衡），通过累积倾向权重（CPW）重新加权购买结果重建积分，避免需求函数显式估计。提出双重稳健的增强累积倾向权重（ACPW）估计器，只需需求模型或历史定价策略分布之一正确设定

Result: 方法支持灵活机器学习估计消费者剩余，即使机器学习估计收敛较慢也能获得快速收敛率。扩展框架到不平等感知的剩余度量，量化利润-公平权衡。通过数值研究验证方法有效性

Conclusion: 开发了实用的AI决策消费者剩余审计框架，避免传统方法的实践困难，支持灵活机器学习方法，可扩展到公平性考量，为监管者和企业提供量化工具

Abstract: This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies.

</details>


### [30] [Fibonacci-Driven Recursive Ensembles: Algorithms, Convergence, and Learning Dynamics](https://arxiv.org/abs/2601.01055)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 该论文提出了基于斐波那契型更新流的递归集成学习算法与动力学基础，相比经典boosting的一阶加法更新，研究二阶递归架构，每个预测器依赖前两个预测器，形成有记忆的学习动态。


<details>
  <summary>Details</summary>
Motivation: 经典boosting使用一阶加法更新，缺乏记忆机制。论文旨在开发具有记忆能力的递归集成学习系统，通过斐波那契型更新流让集成能够整合历史结构同时适应新残差信息，统一递归集成、结构化加权和动力学系统视角。

Method: 提出递归权重更新算法族，涵盖斐波那契、三波那契及高阶递归；建立连续时间极限得到控制集成演化的微分方程系统；使用Rademacher复杂度和算法稳定性分析建立理论保证。

Result: 建立了全局收敛条件、谱稳定性准则和非渐近泛化界；实验表明递归流在核岭回归、样条平滑器和随机傅里叶特征模型中一致改善逼近和泛化能力，超越静态加权方法。

Conclusion: 该理论统一了递归集成、结构化加权和动力学系统视角，完成了从斐波那契加权、几何加权理论到完全动态递归集成学习系统的三部曲，为具有记忆机制的集成学习提供了理论基础。

Abstract: This paper develops the algorithmic and dynamical foundations of recursive ensemble learning driven by Fibonacci-type update flows. In contrast with classical boosting  Freund and Schapire (1997); Friedman (2001), where the ensemble evolves through first-order additive updates, we study second-order recursive architectures in which each predictor depends on its two immediate predecessors. These Fibonacci flows induce a learning dynamic with memory, allowing ensembles to integrate past structure while adapting to new residual information. We introduce a general family of recursive weight-update algorithms encompassing Fibonacci, tribonacci, and higher-order recursions, together with continuous-time limits that yield systems of differential equations governing ensemble evolution. We establish global convergence conditions, spectral stability criteria, and non-asymptotic generalization bounds under Rademacher Bartlett and Mendelson (2002) and algorithmic stability analyses. The resulting theory unifies recursive ensembles, structured weighting, and dynamical systems viewpoints in statistical learning. Experiments with kernel ridge regression Rasmussen and Williams (2006), spline smoothers Wahba (1990), and random Fourier feature models Rahimi and Recht (2007) demonstrate that recursive flows consistently improve approximation and generalization beyond static weighting. These results complete the trilogy begun in Papers I and II: from Fibonacci weighting, through geometric weighting theory, to fully dynamical recursive ensemble learning systems.

</details>


### [31] [Neural Networks on Symmetric Spaces of Noncompact Type](https://arxiv.org/abs/2601.01097)
*Xuan Son Nguyen,Shuo Yang,Aymeric Histace*

Main category: stat.ML

TL;DR: 提出了一种在非紧型对称空间上构建神经网络的新方法，基于统一的点-超平面距离公式，并应用于图像分类、EEG信号分类、图像生成和自然语言推理等任务。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究展示了神经网络在双曲空间和SPD流形上的良好性能，但这些空间都属于非紧型对称空间家族。目前缺乏一个统一的方法在这些空间上构建神经网络，特别是需要统一的点-超平面距离公式来设计网络层。

Method: 提出了一种统一的方法来推导非紧型对称空间上的点-超平面距离公式。首先建立了统一的距离公式框架，然后推导了在高秩非紧型对称空间上使用G不变黎曼度量的闭式距离表达式。基于这个距离公式，设计了全连接层和注意力机制。

Result: 方法在多个具有挑战性的基准测试中得到验证，包括图像分类、EEG信号分类、图像生成和自然语言推理任务。统一的距离公式能够恢复现有特定设置下的点-超平面距离公式。

Conclusion: 提出了一种在非紧型对称空间上构建神经网络的统一方法，通过推导点-超平面距离的闭式表达式，为设计全连接层和注意力机制提供了理论基础，并在多个实际任务中验证了有效性。

Abstract: Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference.

</details>


### [32] [Conformal Blindness: A Note on $A$-Cryptic change-points](https://arxiv.org/abs/2601.01147)
*Johan Hallberg Szabadváry*

Main category: stat.ML

TL;DR: 本文揭示了"保形盲区"现象：即使数据交换性发生显著破坏，p值序列仍可保持均匀分布，导致保形测试鞅无法检测到分布变化。


<details>
  <summary>Details</summary>
Motivation: 保形测试鞅(CTMs)是保形预测框架中用于检验数据交换性假设的标准方法，它通过监测p值序列对均匀分布的偏离来工作。虽然交换性意味着均匀的p值，但反之不成立。这就引出一个问题：是否存在交换性的显著破坏，但p值仍保持均匀，从而使CTMs失效？本文旨在回答这个问题。

Method: 通过显式构造，针对理论上的理想"预言机"一致性度量（由真实条件密度给出），证明了"A-隐秘变化点"的可能性。使用二元高斯分布，识别出一条边际均值变化但不改变一致性得分分布的直线，从而产生完全均匀的p值。通过模拟验证了这一现象。

Result: 模拟证实，即使是巨大的分布偏移也可能对CTM完全隐秘，突显了保形测试鞅的根本局限性。这强调了与潜在偏移对齐的一致性度量的关键作用。

Conclusion: 本文首次证明了"保形盲区"现象的存在，即交换性显著破坏时p值仍可保持均匀分布，导致CTMs失效。这揭示了保形测试鞅的根本局限性，并强调了选择与潜在分布变化对齐的一致性度量的重要性。

Abstract: Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \emph{conformal blindness}.
  Through explicit construction, for the theoretically ideal ``oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values.
  Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts.

</details>


### [33] [Evidence Slopes and Effective Dimension in Singular Linear Models](https://arxiv.org/abs/2601.01238)
*Kalyaan Rao*

Main category: stat.ML

TL;DR: 该论文研究了贝叶斯模型选择中Laplace近似和BIC在奇异模型中的失效问题，提出了基于实对数典范阈值(RLCT)的修正方法，在线性高斯秩模型和字典模型中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯模型选择方法（如Laplace近似和BIC）假设有效模型维度等于参数数量，但在过参数化或秩不足的奇异模型中这一假设不成立，导致模型选择出现偏差。

Method: 在线性高斯秩模型和线性子空间（字典）模型中，利用可解析处理的边际似然和RLCT，理论分析和实证研究Laplace/BIC的误差特性，并提出基于RLCT的修正方法。

Result: Laplace/BIC的误差随(d/2 - λ)log n线性增长，其中d是环境参数维度，λ是RLCT。RLCT感知的修正恢复了正确的证据斜率，并且对表示相同数据子空间的过完备重参数化具有不变性。

Conclusion: 该研究为奇异模型中Laplace失效提供了具体的有限样本特征描述，并证明证据斜率可作为简单线性设置中有效维度的实用估计器，为奇异学习理论的实际应用提供了基础。

Abstract: Bayesian model selection commonly relies on Laplace approximation or the Bayesian Information Criterion (BIC), which assume that the effective model dimension equals the number of parameters. Singular learning theory replaces this assumption with the real log canonical threshold (RLCT), an effective dimension that can be strictly smaller in overparameterized or rank-deficient models.
  We study linear-Gaussian rank models and linear subspace (dictionary) models in which the exact marginal likelihood is available in closed form and the RLCT is analytically tractable. In this setting, we show theoretically and empirically that the error of Laplace/BIC grows linearly with (d/2 minus lambda) times log n, where d is the ambient parameter dimension and lambda is the RLCT. An RLCT-aware correction recovers the correct evidence slope and is invariant to overcomplete reparameterizations that represent the same data subspace.
  Our results provide a concrete finite-sample characterization of Laplace failure in singular models and demonstrate that evidence slopes can be used as a practical estimator of effective dimension in simple linear settings.

</details>


### [34] [Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations](https://arxiv.org/abs/2601.01442)
*Dongrong Li,Tianwei Yu,Xiaodan Fan*

Main category: stat.ML

TL;DR: 提出一种用于处理缺失观测的隐马尔可夫模型的折叠吉布斯采样器，通过边缘化缺失观测和潜在状态来提高采样效率


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中经常存在缺失观测，这使得隐马尔可夫模型的应用变得复杂。现有的EM算法和吉布斯采样器存在非凸性、高计算复杂度和混合速度慢等问题

Method: 提出一种折叠吉布斯采样器，通过同时边缘化缺失观测和对应的潜在状态，直接从隐马尔可夫模型的后验分布中采样

Result: 该方法在估计精度上与现有方法相当，但能产生更大的有效样本量，当缺失条目较多时计算复杂度显著降低，在时间复杂度和采样效率方面优于现有算法

Conclusion: 提出的采样算法在计算和理论上都更快，特别在存在大量缺失条目时具有优势，经验评估表明其在时间复杂度和采样效率方面一致优于现有算法

Abstract: The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS).

</details>


### [35] [Modeling Information Blackouts in Missing Not-At-Random Time Series Data](https://arxiv.org/abs/2601.01480)
*Aman Sunesh,Allan Ma,Siddarth Nilol*

Main category: stat.ML

TL;DR: 提出一个联合建模交通动态和传感器缺失的潜状态空间框架，通过MNAR建模处理传感器黑屏问题，相比传统方法显著提升插补精度


<details>
  <summary>Details</summary>
Motivation: 大规模交通预测中传感器网络经常出现黑屏（连续缺失数据），传统方法假设数据随机缺失，但黑屏事件可能与未观测的交通状况相关，需要MNAR建模

Method: 提出潜状态空间框架：1) 线性动态系统建模交通动态；2) 伯努利观测通道建模传感器丢失，其概率取决于潜交通状态。使用扩展卡尔曼滤波进行推理，近似EM算法学习参数

Result: 在Seattle数据上，潜动态建模显著优于基线方法，插补RMSE从7.02/5.02降至4.23；MNAR建模提供额外改进至4.20。合成实验中MNAR优势随缺失对潜状态依赖增强而增加

Conclusion: 时间动态主导性能，MNAR建模提供原则性改进，在缺失确实包含信息时最有价值。框架有效处理交通数据中的黑屏问题

Abstract: Large-scale traffic forecasting relies on fixed sensor networks that often exhibit blackouts: contiguous intervals of missing measurements caused by detector or communication failures. These outages are typically handled under a Missing At Random (MAR) assumption, even though blackout events may correlate with unobserved traffic conditions (e.g., congestion or anomalous flow), motivating a Missing Not At Random (MNAR) treatment. We propose a latent state-space framework that jointly models (i) traffic dynamics via a linear dynamical system and (ii) sensor dropout via a Bernoulli observation channel whose probability depends on the latent traffic state. Inference uses an Extended Kalman Filter with Rauch-Tung-Striebel smoothing, and parameters are learned via an approximate EM procedure with a dedicated update for detector-specific missingness parameters. On the Seattle inductive loop detector data, introducing latent dynamics yields large gains over naive baselines, reducing blackout imputation RMSE from 7.02 (LOCF) and 5.02 (linear interpolation + seasonal naive) to 4.23 (MAR LDS), corresponding to about a 64% reduction in MSE relative to LOCF. Explicit MNAR modeling provides a consistent but smaller additional improvement on real data (imputation RMSE 4.20; 0.8% RMSE reduction relative to MAR), with similar modest gains for short-horizon post-blackout forecasts (evaluated at 1, 3, and 6 steps). In controlled synthetic experiments, the MNAR advantage increases as the true missingness dependence on latent state strengthens. Overall, temporal dynamics dominate performance, while MNAR modeling offers a principled refinement that becomes most valuable when missingness is genuinely informative.

</details>


### [36] [Variance-Reduced Diffusion Sampling via Conditional Score Expectation Identity](https://arxiv.org/abs/2601.01594)
*Alois Duston,Tan Bui-Thanh*

Main category: stat.ML

TL;DR: 本文提出了条件得分期望(CSE)恒等式，基于此开发了自归一化重要性采样(SNIS)的得分估计器，并与Tweedie估计器进行最优混合，降低了方差并提高了采样质量。


<details>
  <summary>Details</summary>
Motivation: 针对仿射扩散过程的边际得分估计问题，需要更有效的得分估计方法来提高采样效率和样本质量。

Method: 1) 提出并证明CSE恒等式；2) 基于CSE开发SNIS得分估计器；3) 分析CSE与Tweedie估计器的关系；4) 推导方差最小化的混合得分估计器；5) 扩展到贝叶斯逆问题。

Result: 最优混合估计器相比基线方法降低了方差，在固定计算预算下提高了样本质量，在图像重建和PDE逆问题中表现出更好的重建质量和样本多样性。

Conclusion: CSE恒等式为得分估计提供了新视角，基于此的最优混合估计器在采样效率和样本质量方面均有显著改进，可扩展到高维逆问题。

Abstract: We introduce and prove a \textbf{Conditional Score Expectation (CSE)} identity: an exact relation for the marginal score of affine diffusion processes that links scores across time via a conditional expectation under the forward dynamics. Motivated by this identity, we propose a CSE-based statistical estimator for the score using a Self-Normalized Importance Sampling (SNIS) procedure with prior samples and forward noise. We analyze its relationship to the standard Tweedie estimator, proving anti-correlation for Gaussian targets and establishing the same behavior for general targets in the small time-step regime. Exploiting this structure, we derive a variance-minimizing blended score estimator given by a state--time dependent convex combination of the CSE and Tweedie estimators. Numerical experiments show that this optimal-blending estimator reduces variance and improves sample quality for a fixed computational budget compared to either baseline. We further extend the framework to Bayesian inverse problems via likelihood-informed SNIS weights, and demonstrate improved reconstruction quality and sample diversity on high-dimensional image reconstruction tasks and PDE-governed inverse problems.

</details>


### [37] [Deep Linear Discriminant Analysis Revisited](https://arxiv.org/abs/2601.01619)
*Maxat Tezekbayev,Rustem Takhanov,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 传统深度LDA训练存在病态解或参数估计不一致问题，作者提出DNLL损失函数，在保持判别性能的同时恢复生成模型的概率解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度线性判别分析（LDA）训练存在两个问题：最大似然训练会导致类别均值漂移、协方差崩溃，学习到的表示几乎无判别性；而交叉熵训练虽然准确率高，但使分类头与底层生成模型解耦，导致参数估计高度不一致。需要一种方法既能保持判别性能，又能维持生成模型的结构一致性。

Method: 提出判别性负对数似然（DNLL）损失函数，在标准LDA对数似然基础上增加一个惩罚项，该惩罚项明确抑制多个类别同时可能出现的区域。DNLL可以解释为标准LDA NLL加上一个正则化项，防止模型产生病态解。

Result: 使用DNLL训练的深度LDA产生清晰、良好分离的潜在空间，在合成数据和标准图像基准测试中匹配softmax分类器的测试准确率，并且产生显著更好的校准预测概率，为深度判别模型恢复了连贯的概率解释。

Conclusion: DNLL损失函数成功调和了生成结构与判别性能之间的矛盾，使深度LDA既能保持高准确率，又能维持生成模型的概率一致性，为深度判别模型提供了更好的概率解释和校准性能。

Abstract: We show that for unconstrained Deep Linear Discriminant Analysis (LDA) classifiers, maximum-likelihood training admits pathological solutions in which class means drift together, covariances collapse, and the learned representation becomes almost non-discriminative. Conversely, cross-entropy training yields excellent accuracy but decouples the head from the underlying generative model, leading to highly inconsistent parameter estimates. To reconcile generative structure with discriminative performance, we introduce the \emph{Discriminative Negative Log-Likelihood} (DNLL) loss, which augments the LDA log-likelihood with a simple penalty on the mixture density. DNLL can be interpreted as standard LDA NLL plus a term that explicitly discourages regions where several classes are simultaneously likely. Deep LDA trained with DNLL produces clean, well-separated latent spaces, matches the test accuracy of softmax classifiers on synthetic data and standard image benchmarks, and yields substantially better calibrated predictive probabilities, restoring a coherent probabilistic interpretation to deep discriminant models.

</details>


### [38] [Simplex Deep Linear Discriminant Analysis](https://arxiv.org/abs/2601.01679)
*Maxat Tezekbayev,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 本文从似然角度重新审视深度线性判别分析，发现无约束的深度LDA在端到端MLE训练中会忽略判别性，导致类别重叠或坍缩。作者提出约束的深度LDA，将类别均值固定在正则单纯形顶点，限制协方差为球形，使MLE训练稳定且产生良好分离的类别簇。


<details>
  <summary>Details</summary>
Motivation: 传统LDA是简单的线性判别模型，但当将LDA头连接到神经编码器时，如何通过最大似然估计训练深度分类器成为一个问题。作者发现端到端MLE训练无约束深度LDA模型会忽略判别性，导致类别重叠或坍缩，分类性能下降。

Method: 提出约束的深度LDA公式：1) 将类别均值固定在潜在空间正则单纯形的顶点；2) 限制共享协方差为球形；3) 只学习先验和单个方差参数以及编码器参数。在这些几何约束下，MLE训练变得稳定。

Result: 在图像数据集（Fashion-MNIST、CIFAR-10、CIFAR-100）上，约束深度LDA模型达到与softmax基线相当的准确率，同时提供了简单、可解释的潜在几何结构，在二维投影中清晰可见。

Conclusion: 通过几何约束的深度LDA能够稳定地进行MLE训练，产生良好分离的类别簇，在保持竞争力的分类性能的同时，提供了比softmax分类器更可解释的潜在空间结构。

Abstract: We revisit Deep Linear Discriminant Analysis (Deep LDA) from a likelihood-based perspective. While classical LDA is a simple Gaussian model with linear decision boundaries, attaching an LDA head to a neural encoder raises the question of how to train the resulting deep classifier by maximum likelihood estimation (MLE). We first show that end-to-end MLE training of an unconstrained Deep LDA model ignores discrimination: when both the LDA parameters and the encoder parameters are learned jointly, the likelihood admits a degenerate solution in which some of the class clusters may heavily overlap or even collapse, and classification performance deteriorates. Batchwise moment re-estimation of the LDA parameters does not remove this failure mode. We then propose a constrained Deep LDA formulation that fixes the class means to the vertices of a regular simplex in the latent space and restricts the shared covariance to be spherical, leaving only the priors and a single variance parameter to be learned along with the encoder. Under these geometric constraints, MLE becomes stable and yields well-separated class clusters in the latent space. On images (Fashion-MNIST, CIFAR-10, CIFAR-100), the resulting Deep LDA models achieve accuracy competitive with softmax baselines while offering a simple, interpretable latent geometry that is clearly visible in two-dimensional projections.

</details>


### [39] [Sparse Convex Biclustering](https://arxiv.org/abs/2601.01757)
*Jiakun Jiang,Dewei Xiang,Chenliang Gu,Wei Liu,Binhuan Wang*

Main category: stat.ML

TL;DR: 提出SpaCoBi方法，通过惩罚噪声和凸优化框架，提高高维大规模数据集双聚类的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有双聚类方法在处理现代大规模高维数据集时面临挑战：高维特征噪声累积、非凸优化公式限制、计算复杂度高，导致准确性和稳定性随数据集增大而下降。

Method: 提出稀疏凸双聚类(SpaCoBi)，在双聚类过程中惩罚噪声，采用凸优化框架，引入基于稳定性的调优准则，平衡聚类保真度和稀疏性。

Result: 通过模拟实验和小鼠嗅球数据应用，SpaCoBi在准确性方面显著优于现有最先进方法。

Conclusion: SpaCoBi为高维大规模数据集的双聚类提供了一个鲁棒高效的解决方案。

Abstract: Biclustering is an essential unsupervised machine learning technique for simultaneously clustering rows and columns of a data matrix, with widespread applications in genomics, transcriptomics, and other high-dimensional omics data. Despite its importance, existing biclustering methods struggle to meet the demands of modern large-scale datasets. The challenges stem from the accumulation of noise in high-dimensional features, the limitations of non-convex optimization formulations, and the computational complexity of identifying meaningful biclusters. These issues often result in reduced accuracy and stability as the size of the dataset increases. To overcome these challenges, we propose Sparse Convex Biclustering (SpaCoBi), a novel method that penalizes noise during the biclustering process to improve both accuracy and robustness. By adopting a convex optimization framework and introducing a stability-based tuning criterion, SpaCoBi achieves an optimal balance between cluster fidelity and sparsity. Comprehensive numerical studies, including simulations and an application to mouse olfactory bulb data, demonstrate that SpaCoBi significantly outperforms state-of-the-art methods in accuracy. These results highlight SpaCoBi as a robust and efficient solution for biclustering in high-dimensional and large-scale datasets.

</details>


### [40] [A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk](https://arxiv.org/abs/2601.01970)
*Ayomide Afolabi,Ebere Ogburu,Symon Kimitei*

Main category: stat.ML

TL;DR: 该研究评估了三种模型（响应、风险、响应-风险）中多种分类器在信用卡邮件营销和违约预测中的性能，发现Extra Trees在响应模型中召回率最高（79.1%），Random Forest在风险模型中特异性最高（84.1%），在响应-风险多分类模型中准确率最高（83.2%）


<details>
  <summary>Details</summary>
Motivation: 解决信用卡业务中的两个关键问题：1）识别对邮件营销活动可能响应的客户（响应模型），2）识别违约风险低的客户（风险模型），以及3）同时解决这两个问题的综合模型（响应-风险模型）

Method: 使用多种分类器（包括Extra Trees和Random Forest等）在三个不同模型中进行评估：响应模型（二分类）、风险模型（二分类）、响应-风险模型（多分类），并优化各种性能指标来解决具体的信用风险和邮件响应业务问题

Result: 1）响应模型中：Extra Trees分类器获得最高召回率79.1%，能有效识别潜在响应者；2）风险模型中：Random Forest分类器获得最高特异性84.1%，能有效识别低违约风险客户；3）响应-风险多分类模型中：Random Forest分类器获得最高准确率83.2%，能同时有效识别潜在响应者和低风险用户

Conclusion: 不同分类器在不同业务场景下表现各异：Extra Trees适合识别邮件营销响应者，Random Forest在风险识别和综合分类任务中表现优异，研究为信用卡业务的精准营销和风险管理提供了有效的机器学习解决方案

Abstract: This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.

</details>


### [41] [From Mice to Trains: Amortized Bayesian Inference on Graph Data](https://arxiv.org/abs/2601.02241)
*Svenja Jedhoff,Elizaveta Semenova,Aura Raulo,Anne Meyer,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 将摊销贝叶斯推理（ABI）应用于图数据，使用置换不变的图编码器和神经后验估计器进行节点、边和图级参数的推理


<details>
  <summary>Details</summary>
Motivation: 图数据在多个领域广泛存在，但图参数的后验估计面临置换不变性、可扩展性和捕捉长距离依赖等挑战，需要新的推理方法

Method: 采用两模块流水线：总结网络将属性图映射为固定长度表示，推理网络近似参数后验分布；评估多种神经架构作为总结网络

Result: 在受控合成设置和两个真实世界领域（生物学和物流）中评估性能，包括恢复能力和校准度

Conclusion: 将ABI框架成功应用于图数据推理，能够处理图结构数据的独特挑战，为图参数的后验估计提供了有效解决方案

Abstract: Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [42] [Hamiltonian Monte Carlo for (Physics) Dummies](https://arxiv.org/abs/2601.01422)
*Arghya Mukherjee,Dootika Vats*

Main category: stat.CO

TL;DR: 这篇论文提供了哈密顿蒙特卡洛（HMC）的教学性概述，旨在弥合其理论基础与实际应用之间的差距，使应用研究人员更容易理解和使用HMC。


<details>
  <summary>Details</summary>
Motivation: 近年来基于抽样的推断方法受到广泛关注，HMC作为一种利用哈密顿动力学概念有效探索复杂目标分布的强大算法，其变体已在流行软件包中实现。然而，这些黑盒实现使得用户难以理解HMC的内部工作原理，特别是对于那些不熟悉底层物理原理的研究人员。作者希望通过教学性概述帮助应用研究人员更好地理解和使用HMC。

Method: 本文是一篇综述性文章，采用教学性概述的方法，系统性地介绍HMC的理论基础、算法原理和实际应用。文章重点解释HMC如何利用哈密顿动力学概念来高效探索复杂分布，并讨论其各种变体实现。

Result: 通过提供清晰的教学性概述，本文成功弥合了HMC理论基础与实际应用之间的差距，使应用研究人员能够更好地理解HMC的内部工作原理、优势、局限性及其在复杂模型贝叶斯推断中的作用。

Conclusion: HMC是一种强大的抽样算法，能够实现复杂模型的可扩展且精确的贝叶斯推断。通过教学性概述，本文使HMC对应用研究人员更加可访问，有助于推动该方法在统计学和机器学习领域的更广泛应用。

Abstract: Sampling-based inference has seen a surge of interest in recent years. Hamiltonian Monte Carlo (HMC) has emerged as a powerful algorithm that leverages concepts from Hamiltonian dynamics to efficiently explore complex target distributions. Variants of HMC are available in popular software packages, enabling off-the-shelf implementations that have greatly benefited the statistics and machine learning communities. At the same time, the availability of such black-box implementations has made it challenging for users to understand the inner workings of HMC, especially when they are unfamiliar with the underlying physical principles. We provide a pedagogical overview of HMC that aims to bridge the gap between its theoretical foundations and practical applicability. This review article seeks to make HMC more accessible to applied researchers by highlighting its advantages, limitations, and role in enabling scalable and exact Bayesian inference for complex models.

</details>


### [43] [grangersearch: An R Package for Exhaustive Granger Causality Testing with Tidyverse Integration](https://arxiv.org/abs/2601.01604)
*Nikolaos Korfiatis*

Main category: stat.CO

TL;DR: grangersearch是一个R包，用于在多个时间序列上执行穷举格兰杰因果检验搜索，提供简洁的探索性因果分析界面。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供一个简单易用的工具，在多个时间序列变量之间进行系统性的格兰杰因果分析，简化复杂的因果探索过程。

Method: 开发R包grangersearch，提供：(1)多变量间的穷举成对搜索，(2)自动滞后阶数优化与可视化，(3)兼容tidyverse的语法和管道操作符，(4)通过tidy()和glance()方法与broom生态系统集成，包装vars基础设施。

Result: 成功开发了grangersearch包，提供了完整的统计方法描述、工作示例演示，并为应用研究人员讨论了实际考虑因素。

Conclusion: grangersearch包为时间序列因果分析提供了一个强大而用户友好的工具，简化了探索性因果分析流程，特别适合应用研究人员使用。

Abstract: This paper introduces grangersearch, an R package for performing exhaustive Granger causality searches on multiple time series. The package provides: (1) exhaustive pairwise search across multiple variables, (2) automatic lag order optimization with visualization, (3) tidyverse-compatible syntax with pipe operators and non-standard evaluation, and (4) integration with the broom ecosystem through tidy() and glance() methods. The package wraps the vars infrastructure while providing a simple interface for exploratory causal analysis. We describe the statistical methodology, demonstrate the package through worked examples, and discuss practical considerations for applied researchers.

</details>
