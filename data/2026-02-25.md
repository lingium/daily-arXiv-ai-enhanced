<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 31]
- [stat.ML](#stat.ML) [Total: 14]
- [stat.AP](#stat.AP) [Total: 10]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Ostrom-Weighted Bootstrap: A Theoretically Optimal and Provably Complete Framework for Hierarchical Imputation in Multi-Agent Systems](https://arxiv.org/abs/2602.18442)
*Hirofumi Wakimoto*

Main category: stat.ME

TL;DR: OWB是一种用于多智能体投票数据的分层方差感知重采样方法，在已知方差时达到BLUE最优性，在未知方差时通过分层估计实现FGLS和EB收缩，并保证NaN-free数据补全。


<details>
  <summary>Details</summary>
Motivation: 现有重采样方法在处理多智能体投票数据时，未能同时考虑个体方差差异、实现统计最优性、提供贝叶斯解释、并保证数据补全的数值稳定性。

Method: 提出分层方差感知重采样方案OWB：Level 1在已知个体方差时构建BLUE估计；Level 2在分层正态模型中获得条件贝叶斯后验均值；实际应用中用分层池化经验估计替代未知方差，实现FGLS和EB收缩估计。

Result: OWB在已知方差时严格优于均匀加权；在分层模型中与条件贝叶斯后验均值一致；可行OWB具有FGLS和EB收缩双重解释；提供渐近有效的加权bootstrap置信区间；保证零NaN数据补全。

Conclusion: OWB是首个同时实现BLUE最优性、条件贝叶斯解释、EB收缩、FGLS渐近效率、一致加权bootstrap推断和零NaN补全的重采样方法，为多智能体投票数据分析提供了统一框架。

Abstract: We study the statistical properties of the \emph{Ostrom-Weighted Bootstrap} (OWB), a hierarchical, variance-aware resampling scheme for imputing missing values and estimating archetypes in multi-agent voting data. At Level~1, under mild linear model assumptions, the \emph{ideal} OWB estimator -- with known persona-level (agent-level) variances -- is shown to be the Gauss--Markov best linear unbiased estimator (BLUE) and to strictly dominate uniform weighting whenever persona variances differ. At Level~2, within a canonical hierarchical normal model, the ideal OWB coincides with the conditional Bayesian posterior mean of the archetype. We then analyze the \emph{feasible} OWB, which replaces unknown variances with hierarchically pooled empirical estimates, and show that it can be interpreted as both a feasible generalized least-squares (FGLS) and an empirical-Bayes shrinkage estimator with asymptotically valid weighted bootstrap confidence intervals under mild regularity conditions. Finally, we establish a Zero-NaN Guarantee: as long as each petal has at least one finite observation, the OWB imputation algorithm produces strictly NaN-free completed data using only explicit, non-uniform bootstrap weights and never resorting to uniform sampling or numerical zero-filling.
  To our knowledge, OWB is the first resampling-based method that simultaneously achieves exact BLUE optimality, conditional Bayesian posterior mean interpretation, empirical Bayes shrinkage of precision parameters, asymptotic efficiency via FGLS, consistent weighted bootstrap inference, and provable zero-NaN completion under minimal data assumptions.

</details>


### [2] [Spatiotemporal double machine learning to estimate the impact of Cambodian land concessions on deforestation](https://arxiv.org/abs/2602.18570)
*Anika Arifin,Duncan DeProfio,Layla Lammers,Benjamin Shapiro,Brian J Reich,Henry Uddyback,Joshua M Gray*

Main category: stat.ME

TL;DR: 该论文提出了一种改进的双重空间回归方法，结合时间维度分析柬埔寨土地特许权对森林砍伐的政策影响，通过模拟研究验证其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 环境政策评估需要同时考虑空间和时间因素。传统方法如双重差分回归虽然能在某些条件下处理时空相关的处理效应，但难以解决同时影响处理和结果的未观测混杂变量问题。

Method: 改进双重空间回归方法，结合时间维度分析政策干预的空间效应。使用贝叶斯加性回归树结合空间嵌入进行大规模模拟研究，然后应用于柬埔寨土地特许权对森林砍伐的影响评估。

Result: 模拟研究表明，在某些条件下，改进的DSR模型在解决未观测空间混杂方面优于标准方法。实际应用成功评估了柬埔寨土地特许权政策对森林砍伐的因果影响。

Conclusion: 改进的DSR方法为环境政策评估提供了更有效的工具，能够更好地处理时空相关的处理和未观测混杂变量，在柬埔寨土地特许权政策评估中表现出色。

Abstract: Environmental policy evaluation frequently requires thoughtful consideration of space and time in causal inference. We use novel statistical methods to analyze the causal effect of land concessions on deforestation rates in Cambodia. Standard approaches, such as difference-in-differences regression, effectively address spatiotemporally-correlated treatments under some conditions, but they are limited in their ability to account for unobserved confounders affecting both treatment and outcome. Double Spatial Regression (DSR) is an approach that uses double machine learning to address these scenarios. DSR resolves the confounding variables for both treatment and outcome, comparing the residuals to estimate treatment effectiveness. We improve upon DSR by considering time in our analysis of policy interventions with spatial effects. We conduct a large-scale simulation study using Bayesian Additive Regression Trees (BART) with spatial embeddings and find that, under certain conditions, our DSR model outperforms standard approaches for addressing unobserved spatial confounding. We then apply our method to evaluate the policy impacts of land concessions on deforestation in Cambodia.

</details>


### [3] [balnet: Pathwise Estimation of Covariate Balancing Propensity Scores](https://arxiv.org/abs/2602.18577)
*Erik Sverdrup,Trevor Hastie*

Main category: stat.ME

TL;DR: balnet是一个R包，用于通过逻辑协变量平衡损失函数进行可扩展的路径式协变量平衡倾向得分估计


<details>
  <summary>Details</summary>
Motivation: 需要开发一个可扩展的工具来估计协变量平衡倾向得分，特别是在处理空间像素级平衡等复杂应用场景时，传统方法可能难以应对

Method: 使用Yang和Hastie(2024)的通用弹性网络求解器计算正则化路径，支持具有非光滑惩罚的凸损失函数，以及组惩罚和特征特定惩罚因子。对于lasso惩罚，balnet计算从最大观测协变量不平衡到用户指定比例的规则化平衡路径

Result: 开发了balnet R包，能够进行可扩展的路径式协变量平衡倾向得分估计，并成功应用于空间像素级平衡，为构建合成控制权重以估计处理组平均处理效应提供了工具

Conclusion: balnet提供了一个灵活且可扩展的框架，用于通过逻辑协变量平衡损失函数估计协变量平衡倾向得分，特别适用于复杂应用如空间像素级平衡，为因果推断中的权重构建提供了有效工具

Abstract: We present balnet, an R package for scalable pathwise estimation of covariate balancing propensity scores via logistic covariate balancing loss functions. Regularization paths are computed with Yang and Hastie (2024)'s generic elastic net solver, supporting convex losses with non-smooth penalties, as well as group penalties and feature-specific penalty factors. For lasso penalization, balnet computes a regularized balance path from the largest observed covariate imbalance to a user-specified fraction of this maximum. We illustrate the method with an application to spatial pixel-level balancing for constructing synthetic control weights for the average treatment effect on the treated, using satellite data on wildfires.

</details>


### [4] [Hybrid combinations of parametric and empirical likelihoods](https://arxiv.org/abs/2602.18651)
*Nils Lid Hjort,Ian W. McKeague,Ingrid Van Keilegom*

Main category: stat.ME

TL;DR: 提出一种基于参数似然和经验似然的混合似然方法，通过平衡参数a在模型效率和稳健性之间取得折衷


<details>
  <summary>Details</summary>
Motivation: 在参数模型的基础上引入额外的估计方程，既利用参数模型的效率优势，又通过经验似然获得一定的稳健性，解决传统参数方法对模型误设敏感的问题

Method: 构建混合似然函数H_n(θ)=L_n(θ)^{1-a}R_n(μ(θ))^a，其中L_n是参数似然，R_n是经验似然，a∈[0,1)是平衡参数，μ通过参数模型视角考虑

Result: 建立了混合似然估计量的渐近正态性和Wilks定理版本，研究了参数模型误设下的扩展结果，并提出了平衡参数a的选择方法

Conclusion: 混合似然方法成功地在参数模型的效率和经验似然的稳健性之间取得了平衡，为统计推断提供了一种灵活且具有理论保证的折衷方案

Abstract: This paper develops a hybrid likelihood (HL) method based on a compromise between parametric and nonparametric likelihoods. Consider the setting of a parametric model for the distribution of an observation $Y$ with parameter $θ$. Suppose there is also an estimating function $m(\cdot,μ)$ identifying another parameter $μ$ via $E\,m(Y,μ)=0$, at the outset defined independently of the parametric model. To borrow strength from the parametric model while obtaining a degree of robustness from the empirical likelihood method, we formulate inference about $θ$ in terms of the hybrid likelihood function $H_n(θ)=L_n(θ)^{1-a}R_n(μ(θ))^a$. Here $a\in[0,1)$ represents the extent of the compromise, $L_n$ is the ordinary parametric likelihood for $θ$, $R_n$ is the empirical likelihood function, and $μ$ is considered through the lens of the parametric model. We establish asymptotic normality of the corresponding HL estimator and a version of the Wilks theorem. We also examine extensions of these results under misspecification of the parametric model, and propose methods for selecting the balance parameter $a$.

</details>


### [5] [Minimally Discrete and Minimally Randomized p-Values](https://arxiv.org/abs/2602.18656)
*Joshua Habiger,Pratyaydipta Rudra*

Main category: stat.ME

TL;DR: 本文提出"最小离散"自然p值、中p值和"最小随机"p值，旨在减少离散分布数据中p值构造的保守性和随机性，提高下游统计方法的效率。


<details>
  <summary>Details</summary>
Motivation: 当数据具有离散分布时，传统的自然p值、中p值和随机p值存在保守性过高或随机性过大的问题，这会影响元分析、多重假设检验等下游统计方法的效率。

Method: 提出并研究"最小离散"自然p值、中p值和"最小随机"p值。MD p值在随机序和凸序上优于传统p值，MR p值在保持零假设下均匀分布的同时减少了随机变量的额外变异。

Result: MD p值在随机序和凸序上优于非MD对应物，导致下游方法保守性降低但仍有效。MR p值在保持均匀分布的同时减少了独立辅助变量带来的额外变异。

Conclusion: 这些新p值构造方法有望通过更高效的p值构建促进新的元分析和多重检验方法的发展，并为处理不可避免的"离散效应"建立理论研究的黄金标准。

Abstract: In meta analysis, multiple hypothesis testing and many other methods, p-values are utilized as inputs and assumed to be uniformly distributed over the unit interval under the null hypotheses. If data used to generate p-values have discrete distributions then either natural, mid- or randomized p-values are typically utilized. Natural and mid-p-values can allow for valid, albeit conservative, downstream methods since under the null hypothesis they are dominated by uniform distributions in the stochastic and convex order, respectively. Randomized p-values need not lead to conservative procedures since they permit a uniform distributions under the null hypotheses through the generation of independent auxiliary variates. However, the auxiliary variates necessarily add variation to procedures. This manuscript introduces and studies ``minimally discrete'' (MD) natural p-values, MD mid-p-values and ``minimally randomized'' (MR) p-values. It is shown that MD p-values dominate their non-MD counterparts in the stochastic and convex order, and hence lead to less conservative, yet still valid, downstream methods. Likewise, MR p-values dominate their non-MR counterparts in that they are still uniformly distributed under the null hypotheses, but the added variation attributable to the independently generated auxiliary variate is smaller. It is anticipated that results here will facilitate the construction of new meta-analysis and multiple testing methods via more efficient p-value construction, and facilitate theoretical study of existing and new methods by establishing gold standards for addressing the unavoidable detrimental ``discreteness effect''.

</details>


### [6] [Optimal and Structure-Adaptive CATE Estimation with Kernel Ridge Regression](https://arxiv.org/abs/2602.18958)
*Seok-Jin Kim*

Main category: stat.ME

TL;DR: 提出一种估计条件平均处理效应(CATE)的最优算法，当响应函数位于再生核希尔伯特空间(RKHS)时，通过两阶段核岭回归方法，在对比函数比干扰函数结构更简单的情况下达到极小极大速率。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，条件平均处理效应(CATE)估计通常面临干扰函数（如倾向得分和基线响应）的高维复杂性挑战。然而，实际中对比函数（处理效应函数）往往比干扰函数结构更简单，现有方法未能充分利用这种结构简化。

Method: 提出统一的两阶段核岭回归(KRR)方法：第一阶段估计干扰函数，第二阶段在对比函数空间进行核岭回归。通过模型选择步骤在候选对比空间和正则化水平上进行选择，实现自适应到未知的CATE正则性。

Result: 该方法在样本量和重叠性方面达到由对比函数复杂度而非干扰函数类决定的极小极大速率。模型选择步骤产生oracle不等式，能够自适应到未知的CATE正则性。

Conclusion: 当对比函数比干扰函数结构更简单时，提出的两阶段核岭回归方法能够实现最优的CATE估计，并自适应到未知的CATE正则性，为因果推断中的异质性处理效应估计提供了理论保证。

Abstract: We propose an optimal algorithm for estimating conditional average treatment effects (CATEs) when response functions lie in a reproducing kernel Hilbert space (RKHS). We study settings in which the contrast function is structurally simpler than the nuisance functions: (i) it lies in a lower-complexity RKHS with faster eigenvalue decay, (ii) it satisfies a source condition relative to the nuisance kernel, or (iii) it depends on a known low-dimensional covariate representation. We develop a unified two-stage kernel ridge regression (KRR) method that attains minimax rates governed by the complexity of the contrast function rather than the nuisance class, in terms of both sample size and overlap. We also show that a simple model-selection step over candidate contrast spaces and regularization levels yields an oracle inequality, enabling adaptation to unknown CATE regularity.

</details>


### [7] [Better Assumptions, Stronger Conclusions: The Case for Ordinal Regression in HCI](https://arxiv.org/abs/2602.18660)
*Brandon Victor Syiem,Eduardo Velloso*

Main category: stat.ME

TL;DR: 该论文分析了HCI领域中使用序数数据（如李克特量表）的统计方法现状，指出当前方法使用混乱且缺乏反思，并推荐使用累积链接（混合）模型作为更合适的分析方法。


<details>
  <summary>Details</summary>
Motivation: 尽管序数测量在HCI中广泛使用（如李克特量表），但HCI研究人员在分析此类数据的统计方法上缺乏共识。参数和非参数方法都被广泛使用，但对其假设和适用性的反思有限。

Method: 1. 检查近期HCI工作中报告序数测量统计分析的研究；2. 突出常用方法，讨论其局限性；3. 强调关键假设和疏忽；4. 倡导并详细说明使用累积链接（混合）模型分析序数数据；5. 提供使用R对公开数据集应用CLM/CLMM的实际工作示例。

Result: 论文揭示了HCI领域在序数数据分析方法上的不一致性和局限性，指出了常用方法存在的问题，并展示了累积链接（混合）模型作为更合适替代方案的优势。

Conclusion: 该研究有助于更好地理解HCI中分析序数数据的统计方法，并帮助巩固未来工作的实践标准。累积链接（混合）模型被推荐为分析序数数据的更合适方法，通过实际示例展示了其应用价值。

Abstract: Despite the widespread use of ordinal measures in HCI, such as Likert-items, there is little consensus among HCI researchers on the statistical methods used for analysing such data. Both parametric and non-parametric methods have been extensively used within the discipline, with limited reflection on their assumptions and appropriateness for such analyses. In this paper, we examine recent HCI works that report statistical analyses of ordinal measures. We highlight prevalent methods used, discuss their limitations and spotlight key assumptions and oversights that diminish the insights drawn from these methods. Finally, we champion and detail the use of cumulative link (mixed) models (CLM/CLMM) for analysing ordinal data. Further, we provide practical worked examples of applying CLM/CLMMs using R to published open-sourced datasets. This work contributes towards a better understanding of the statistical methods used to analyse ordinal data in HCI and helps to consolidate practices for future work.

</details>


### [8] [Change point analysis of high-dimensional data using random projections](https://arxiv.org/abs/2602.19988)
*Yi Xu,Yeonwoo Rho*

Main category: stat.ME

TL;DR: 提出基于随机投影的高维数据变点检测方法，通过将高维时间序列投影到一维空间，利用单变量时间序列的丰富方法库，通过多次随机投影和多重比较方法结合结果。


<details>
  <summary>Details</summary>
Motivation: 高维数据变点检测面临挑战，传统方法难以直接应用。通过随机投影将高维数据降维到一维，可以利用成熟的单变量时间序列分析方法，解决高维数据变点检测问题。

Method: 使用随机投影将高维时间序列映射到一维空间，多次重复投影过程，对每次投影结果应用单变量变点检测方法，然后通过多重比较方法（如FDR控制）结合所有投影结果，最后通过重复实验取估计位置的众数作为最终变点估计。

Result: 模拟结果显示该方法在检验水平和检验功效方面表现更好，变点位置估计更准确。但随机投影会引入估计位置的变异性，通过重复实验和取众数可以增强稳定性。澳大利亚温度数据集的应用验证了方法的实用性。

Conclusion: 虽然目前仅限于单变点场景，但随机投影方法在高维数据变点分析中具有实用价值，通过降维策略有效利用了单变量时间序列分析的丰富方法库。

Abstract: This paper develops a novel change point identification method for high-dimensional data using random projections. By projecting high-dimensional time series into a one-dimensional space, we are able to leverage the rich literature for univariate time series. We propose applying random projections multiple times and then combining the univariate test results using existing multiple comparison methods. Simulation results suggest that the proposed method tends to have better size and power, with more accurate location estimation. At the same time, random projections may introduce variability in the estimated locations. To enhance stability in practice, we recommend repeating the procedure, and using the mode of the estimated locations as a guide for the final change point estimate. An application to an Australian temperature dataset is presented. This study, though limited to the single change point setting, demonstrates the usefulness of random projections in change point analysis.

</details>


### [9] [Distributional Discontinuity Design](https://arxiv.org/abs/2602.19290)
*Kyle Schindl,Larry Wasserman*

Main category: stat.ME

TL;DR: 提出分布断点设计框架，用Wasserstein距离衡量断点处条件结果分布的因果效应，可分解为位置、尺度、偏度等形状分量，并扩展到分布扭结设计。


<details>
  <summary>Details</summary>
Motivation: 传统断点设计和扭结设计通常只分析平均效应，但处理可能改变整个结果分布的形状。需要能够捕捉分布层面因果效应的框架。

Method: 引入分布断点设计框架，使用Wasserstein距离作为断点处条件结果分布间分布偏移的度量。将Wasserstein距离正交分解为L-矩的平方差，量化位置、尺度、偏度等形状分量的贡献。扩展到分布扭结设计，评估政策扭结处的Wasserstein导数。

Result: Wasserstein距离弱界定了平均处理效应，仅在处理效应纯可加时相等。提供了模糊扭结设计的新识别结果。在实际数据应用中，与传统因果估计量相比展示了分布效应的优势。

Conclusion: 分布断点设计框架提供了比传统平均效应更全面的因果效应分析，能够捕捉分布形状变化和效应异质性，为政策评估提供了更丰富的工具。

Abstract: Regression discontinuity and kink designs are typically analyzed through mean effects, even when treatment changes the shape of the entire outcome distribution. To address this, we introduce distributional discontinuity designs, a framework for estimating causal effects for a scalar outcome at the boundary of a discontinuity in treatment assignment. Our estimand is the Wasserstein distance between limiting conditional outcome distributions; a single scale-interpretable measure of distribution shift. We show that this weakly bounds the average treatment effect, where equality holds if and only if the treatment effect is purely additive; thus, departure from equality measures effect heterogeneity. To further encode effect heterogeneity we show that the Wasserstein distance admits an orthogonal decomposition into squared differences in $L$-moments, thereby quantifying the contribution from location, scale, skewness, and higher-order shape components to the overall distributional distance. Next, we extend this framework to distributional kink designs by evaluating the Wasserstein derivative at a policy kink; this describes the flow of probability mass through the kink. In the case of fuzzy kink designs, we derive new identification results. Finally, we apply our methods on real data by re-analyzing two natural experiments to compare our distributional effects to traditional causal estimands.

</details>


### [10] [Bayesian calendar-time survival analysis with epidemic curve priors and variant-specific infection hazards](https://arxiv.org/abs/2602.18677)
*Angela M Dahl,Elizabeth R Brown*

Main category: stat.ME

TL;DR: 开发了一个贝叶斯日历时间生存模型，用于传染病预防研究，能处理快速变化的感染风险、估计生物标志物保护阈值、处理不同病毒变体的竞争风险，并整合流行病曲线先验信息。


<details>
  <summary>Details</summary>
Motivation: 在传染病流行期间进行预防研究时，感染风险会随着流行病曲线的变化而快速改变，传统生存模型难以处理这种动态风险环境，需要开发能够整合流行病学信息和处理多种病毒变体的新方法。

Method: 开发了贝叶斯日历时间生存模型，包含：1) 估计生物标志物保护阈值；2) 将不同病毒变体感染视为竞争风险；3) 将现有流行病曲线估计整合为基线风险函数的先验信息；4) 即使在随访数据有限的时间段也能估计干预效果。

Result: 通过模拟研究验证了该方法的优势，并将其应用于COVID-19疫苗观察性研究数据，证明该方法能够有效处理动态感染风险环境下的生存分析问题。

Conclusion: 该贝叶斯日历时间生存模型为传染病流行期间的预防研究提供了强大的分析工具，能够整合流行病学信息、处理多种病毒变体竞争风险，并在数据有限的情况下提供可靠的干预效果估计。

Abstract: In this paper, we develop a Bayesian calendar-time survival model motivated by infectious disease prevention studies occurring during an epidemic, when the risk of infection can change rapidly as the epidemic curve shifts. For studies in which a biomarker is the predictor of interest, we include the option to estimate a threshold of protection for the biomarker. If the intervention is hypothesized to have different associations with several circulating viral variants, or if the infectiousness of the dominant variant(s) changes over the course of the study, we treat infection from different variants as competing risks. We also introduce a novel method for incorporating existing epidemic curve estimates into an informative prior for the baseline hazard function, enabling estimation of the intervention's association with infection risk during periods of calendar time with minimal follow-up in one or more comparator groups. We demonstrate the strengths of this method via simulations, and we apply it to data from an observational COVID-19 vaccine study.

</details>


### [11] [Zero Variance Portfolio](https://arxiv.org/abs/2602.19462)
*Jinyuan Chang,Yi Ding,Zhentao Shi,Bo Zhang*

Main category: stat.ME

TL;DR: 论文提出Ridgelet估计器用于高维投资组合优化，在资产数量超过样本量的过参数化情况下，能实现样本外泛化并达到最优风险，而传统伪逆方法则失效。


<details>
  <summary>Details</summary>
Motivation: 当资产数量超过样本大小时，最小方差投资组合会在训练数据上产生病态的零样本内方差，这导致样本外泛化能力差。需要一种在过参数化情况下仍能保持泛化能力的方法。

Method: 提出一种新颖的"Ridgelet"估计器来学习零方差投资组合的权重。该方法在过参数化情况下（资产数量主导样本大小）能够实现样本外泛化，并展示双下降现象。

Result: Ridgelet方法在过参数化情况下能实现样本外泛化并达到最优风险，而使用伪逆的"Ridgeless"估计器无法实现样本内插值，且偏离样本外最优性。广泛的模拟和实证研究表明Ridgelet在高维投资组合优化中具有竞争力。

Conclusion: 在高维投资组合优化中，当资产数量超过样本大小时，Ridgelet估计器能够克服传统方法的局限性，实现样本外泛化并达到最优风险，为过参数化情况下的投资组合优化提供了有效解决方案。

Abstract: When the number of assets is larger than the sample size, the minimum variance portfolio interpolates the training data, delivering pathological zero in-sample variance. We show that if the weights of the zero variance portfolio are learned by a novel ``Ridgelet'' estimator, in a new test data this portfolio enjoys out-of-sample generalizability. It exhibits the double descent phenomenon and can achieve optimal risk in the overparametrized regime when the number of assets dominates the sample size. In contrast, a ``Ridgeless'' estimator which invokes the pseudoinverse fails in-sample interpolation and diverges away from out-of-sample optimality. Extensive simulations and empirical studies demonstrate that the Ridgelet method performs competitively in high-dimensional portfolio optimization.

</details>


### [12] [Expected Shortfall Regression via Optimization](https://arxiv.org/abs/2602.18865)
*Yuanzhi Li,Shushu Zhang,Xuming He*

Main category: stat.ME

TL;DR: 本文提出了一种新的线性期望损失回归方法，通过优化方法估计尾部分布，无需对条件分位数模型做额外假设，相比现有方法具有异质性自适应权重和效率优势。


<details>
  <summary>Details</summary>
Motivation: 期望损失回归能够捕捉协变量对响应分布尾部的异质性影响，但现有方法（如运筹学中的超分位数回归）与期望损失回归并不一致，需要开发更准确的方法。

Method: 提出基于优化的线性期望损失回归方法，使用隐式定义的损失函数，通过分箱技术提供初始估计器，建立具有异质性自适应权重的估计框架。

Result: 证明了所提估计器的一致性和渐近正态性，模拟研究表明该方法相比现有线性期望损失回归方法通常能获得效率提升。

Conclusion: 该方法为线性期望损失回归提供了有效的优化框架，无需额外假设条件分位数模型，通过异质性自适应权重实现了更好的统计效率。

Abstract: To provide a comprehensive summary of the tail distribution, the expected shortfall is defined as the average over the tail above (or below) a certain quantile of the distribution. The expected shortfall regression captures the heterogeneous covariate-response relationship and describes the covariate effects on the tail of the response distribution. Based on a critical observation that the superquantile regression from the operations research literature does not coincide with the expected shortfall regression, we propose and validate a novel optimization-based approach for the linear expected shortfall regression, without additional assumptions on the conditional quantile models. While the proposed loss function is implicitly defined, we provide a prototype implementation of the proposed approach with some initial expected shortfall estimators based on binning techniques. With practically feasible initial estimators, we establish the consistency and the asymptotic normality of the proposed estimator. The proposed approach achieves heterogeneity-adaptive weights and therefore often offers efficiency gain over existing linear expected shortfall regression approaches in the literature, as demonstrated through simulation studies.

</details>


### [13] [The generalized underlap coefficient with an application in clustering](https://arxiv.org/abs/2602.19473)
*Zhaoxi Zhang,Vanda Inacio,Sara Wade*

Main category: stat.ME

TL;DR: 本文推广了多组分离度量——重叠系数（UNL）到多元变量，建立了其与总变差的关系，将其解释为组标签与变量间的依赖度量，提出了重要性采样估计器，并展示了在聚类分析中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统分布差异度量大多针对两组比较，缺乏适用于多组比较的度量方法。需要发展能够量化多组间分布分离的统计工具，特别是在聚类分析和协变量依赖混合模型等应用中。

Method: 将重叠系数（UNL）推广到多元变量，建立其与总变差的理论联系，将UNL解释为组标签与变量间的依赖度量，提出基于重要性采样的估计器，可与灵活的密度估计器结合使用。

Result: 建立了UNL的关键性质及其与总变差的关系，展示了UNL作为依赖度量的特性，并与互信息进行了比较。提出的估计器在实际聚类应用中验证了有效性，特别是在评估协变量依赖混合模型中的单权重假设方面。

Conclusion: UNL是一个有效的多组分布分离度量工具，特别适用于聚类分析中评估组别与协变量间的依赖关系，为统计学习和科学发现提供了新的分析框架。

Abstract: Quantifying distributional separation across groups is fundamental in statistical learning and scientific discovery, yet most classical discrepancy measures are tailored to two-group comparisons. We generalize the underlap coefficient (UNL), a multi-group separation measure, to multivariate variables. We establish key properties of UNL and provide an explicit connection to the total variation. We further interpret the UNL as a dependence measure between a group label and variables of interest and compare it with mutual information. We propose an importance sampling estimator of the UNL that can be combined with flexible density estimators. The utility of the UNL for assessing partition-covariate dependence in clustering is highlighted in detail, where it is particularly useful for evaluating the single-weights assumption in covariate-dependent mixture models. Finally we illustrate the application of the UNL in clustering using two real world datasets.

</details>


### [14] [Optimality of the Half-Order Exponent in the Turing-Good Identities for Bayes Factors](https://arxiv.org/abs/2602.19838)
*Kensuke Okada*

Main category: stat.ME

TL;DR: 贝叶斯因子的蒙特卡洛计算中，使用Turing-Good恒等式作为诊断工具时，半阶（平方根）幂是唯一的最小最大稳定选择，能保证分布无关的最坏情况下二阶矩有限，提供平衡的双样本诊断。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯因子广泛使用蒙特卡洛方法计算，但重尾采样分布可能导致数值验证不可靠。Turing-Good恒等式为贝叶斯因子的幂提供精确矩等式，但幂的选择成为统计设计参数，需要理论指导。

Method: 开发蒙特卡洛评估恒等式的非渐近方差理论，证明半阶（平方根）幂是唯一的最小最大稳定选择：它在两种模型方向上均衡变异性，且是唯一保证在所有相互绝对连续模型对中分布无关最坏情况下二阶矩有限的选择。

Result: 得到平衡的双样本半阶诊断，具有模型标记对称性和固定计算预算下的均匀方差界；在小重叠区域保证不低于标准单侧Turing检查的效率。二项贝叶斯因子工作流的模拟显示稳定的有限样本行为和模拟器-评估器不匹配的敏感性。

Conclusion: 半阶重叠视角与归一化常数比和重要性采样退化摘要的稳定原语相联系，为贝叶斯因子蒙特卡洛验证提供了理论保证的诊断工具。

Abstract: Bayes factors are widely computed by Monte Carlo, yet heavy-tailed sampling distributions can make numerical validation unreliable. The Turing--Good identities provide exact moment equalities for powers of a Bayes factor (a density ratio). When these identities are used as Good-check diagnostics, the power choice becomes a statistical design parameter. We develop a nonasymptotic variance theory for Monte Carlo evaluation of the identities and show that the half-order (square-root) power is uniquely minimax-stable: it equalizes variability across the two model orientations and is the only choice that guarantees finite second moments in a distribution-free worst-case sense over all mutually absolutely continuous model pairs. This yields a balanced two-sample half-order diagnostic that is symmetric in model labeling and has a uniform variance bound at fixed computational budget; in small-overlap regimes it is guaranteed to be no less efficient than the standard one-sided Turing check. Simulations for binomial Bayes factor workflows illustrate stable finite-sample behavior and sensitivity to simulator--evaluator mismatches. We further connect the half-order overlap viewpoint to stable primitives for normalizing-constant ratios and importance-sampling degeneracy summaries.

</details>


### [15] [Latent Moment Models for Recurrent Binary Outcomes: A Bayesian and Quasi-Distributional Approach](https://arxiv.org/abs/2602.18988)
*Niloofar Ramezani,Lori P. Selby,Pascal Nitiema,Jeffrey R. Wilson*

Main category: stat.ME

TL;DR: 提出两种用于建模重复二元结果（如再入院）的统计框架，通过时变潜分布的位置、尺度、偏度和峰度来捕捉风险动态变化，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如广义线性混合模型）只能估计平均风险，无法捕捉风险随时间变化的变异性、不对称性和尾部行为，而这些特征在临床决策中很重要。

Method: 提出两种框架：1) BLaS-Recurrent：使用sinh-arcsinh分布的贝叶斯模型估计潜矩轨迹；2) QuaD-Recurrent：通过非参数曲面将模拟矩向量映射到事件概率的准分布方法。两者都支持时变协变量、序列相关和多成员结构。

Result: 模拟研究显示比标准模型有更好的校准性、可解释性和鲁棒性。应用于MIMIC-IV数据库的ICU再入院数据，发现了传统方法遗漏的临床有意义模式，如右偏的升级和扩散加宽。

Conclusion: 这些模型为医疗领域的纵向二元结果提供了可解释的、分布敏感的工具，同时明确承认潜"矩"能总结但不能唯一确定基础分布。

Abstract: Recurrent binary outcomes within individuals, such as hospital readmissions, often reflect latent risk processes that evolve over time. Conventional methods like generalized linear mixed models and generalized estimating equations estimate average risk but fail to capture temporal changes in variability, asymmetry, and tail behavior. We introduce two statistical frameworks that model each binary event as the outcome of a thresholded value drawn from a time-varying latent distribution defined by its location, scale, skewness, and kurtosis. Rather than treating these four quantities as nonparametric moment estimators, we model them as interpretable latent moments within a flexible latent distributional family. The first, BLaS-Recurrent, is a Bayesian model using the sinh-arcsinh distribution (a parametric family that provides explicit control over asymmetry and tail weight) to estimate latent moment trajectories; the second, QuaD-Recurrent, is a quasi-distributional approach that maps simulated moment vectors to event probabilities using a flexible nonparametric surface. Both models support time-dependent covariates, serial correlation, and multiple membership structures. Simulation studies show improved calibration, interpretability, and robustness over standard models. Applied to ICU readmission data from the MIMIC-IV database, both approaches uncover clinically meaningful patterns in latent risk, such as right-skewed escalation and widening dispersion, that are missed by traditional methods. These models provide interpretable, distribution-sensitive tools for longitudinal binary outcomes in healthcare while explicitly acknowledging that latent "moments" summarize but do not uniquely determine the underlying distribution.

</details>


### [16] [Conformal Risk Control for Non-Monotonic Losses](https://arxiv.org/abs/2602.20151)
*Anastasios N. Angelopoulos*

Main category: stat.ME

TL;DR: 提出了一种扩展的共形风险控制方法，适用于多维参数和非单调损失函数，其保证取决于算法的稳定性


<details>
  <summary>Details</summary>
Motivation: 原始的共形风险控制方法只能处理一维参数和单调损失函数，需要扩展到更一般的场景，包括多维参数和非单调损失函数

Method: 提出了适用于通用算法的风险控制保证框架，该框架依赖于算法的稳定性，不稳定算法的保证更宽松

Result: 该方法应用于多个实际问题：选择性图像分类、肿瘤分割的FDR和IOU控制、以及使用经验风险最小化对重叠种族和性别群体的累犯预测进行多组去偏

Conclusion: 扩展了共形风险控制的范围，使其能够处理更复杂的损失函数和多维参数，为实际应用提供了理论保证

Abstract: Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.

</details>


### [17] [Adaptive Weighting for Time-to-Event Continual Reassessment Method: Improving Safety in Phase I Dose-Finding Through Data-Driven Delay Distribution Estimation](https://arxiv.org/abs/2602.19012)
*Robert Amevor,Emmanuel Kubuafor,Dennis Baidoo*

Main category: stat.ME

TL;DR: AW-TITE方法通过自适应权重改进TITE-CRM，显著降低患者过量给药风险，同时保持MTD选择准确性。


<details>
  <summary>Details</summary>
Motivation: 免疫疗法和靶向药物导致延迟性毒性增加，传统TITE-CRM使用固定线性权重无法反映实际延迟模式，可能在剂量递增期间使患者面临过度风险。

Method: 用自适应权重替代TITE-CRM的固定权重，基于Weibull时序模型通过最大似然估计获得闭式权重更新，实现实时计算。

Result: AW-TITE相比TITE-CRM减少40.6%的患者过量给药，MTD选择准确性相当；相比算法方法（mTPI、3+3、BOIN）MTD识别率更高，性能在各种敏感性分析中保持稳健。

Conclusion: 自适应权重方法能实际提高I期试验安全性同时保持MTD选择准确性，计算需求最小且可实时使用。

Abstract: Background: Phase I dose-finding trials increasingly encounter delayed-onset toxicities, especially with immunotherapies and targeted agents. The time-to-event continual reassessment method (TITE-CRM) handles incomplete follow-up using fixed linear weights, but this ad hoc approach doesn't reflect actual delay patterns and may expose patients to excessive risk during dose escalation.
  Methods: We replace TITE-CRM's fixed weights with adaptive weights, posterior predictive probabilities derived from the evolving toxicity delay distribution. Under a Weibull timing model, we get closed-form weight updates through maximum likelihood estimation, making real-time implementation straightforward. We tested our method (AW-TITE) against TITE-CRM and standard designs (3+3, mTPI, BOIN) across three dose-toxicity scenarios through simulation (N = 30 patients, 2,000 replications). We also examined robustness across varying accrual rates, sample sizes, shape parameters, observation windows, and priors.
  Results: Our AW-TITE reduced patient overdosing by 40.6% compared to TITE-CRM (mean fraction above MTD: 0.202 vs 0.340; 95% CI: -0.210 to -0.067, p < 0.001) while maintaining comparable MTD selection accuracy (mean difference: +0.023, p = 0.21). Against algorithm-based methods, AW-TITE achieved higher MTD identification: +32.6% vs mTPI, +19.8% vs 3+3, and +5.6% vs BOIN. Performance remained robust across all sensitivity analyses.
  Conclusions: Adaptive weighting offers a practical way to improve Phase I trial safety while preserving MTD selection accuracy. The method requires minimal computation and is ready for real-time use.

</details>


### [18] [Estimation and Statistical Inference for Generalized Multilayer Latent Space Model](https://arxiv.org/abs/2602.19129)
*Zhaozhe Liu,Gongjun Xu,Haoran Zhang*

Main category: stat.ME

TL;DR: 提出多层有向网络的灵活隐空间模型，通过Tucker低秩分解表示网络张量，开发新的展开融合方法进行估计，建立了估计量的相合性和渐近正态性


<details>
  <summary>Details</summary>
Motivation: 多层网络在多个科学领域广泛应用，但其推断理论仍不完善。现有方法无法处理多层有向网络的各种边类型和层特定结构，需要开发新的统计推断框架

Method: 提出多层有向网络的隐空间模型，每个节点分配发送和接收行为的两个隐位置，每层有连接矩阵控制层特定结构。通过非线性链接函数将网络表示为张量，采用Tucker低秩分解。开发了新的展开融合方法进行参数估计

Result: 建立了估计的隐位置和连接矩阵的相合性和渐近正态性，为多层网络应用中的统计推断任务（如构建置信区域、检验层结构是否相同）提供了理论基础。通过模拟研究和真实数据验证了方法的有效性

Conclusion: 提出的模型和方法为多层网络分析提供了灵活的统计推断框架，能够处理复杂的网络结构，填补了多层网络推断理论的空白，具有广泛的实际应用价值

Abstract: Multilayer networks have become increasingly ubiquitous across diverse scientific fields, ranging from social sciences and biology to economics and international relations. Despite their broad applications, the inferential theory for multilayer networks remains underdeveloped. In this paper, we propose a flexible latent space model for multilayer directed networks with various edge types, where each node is assigned with two latent positions capturing sending and receiving behaviors, and each layer has a connection matrix governing the layer-specific structure. Through nonlinear link functions, the proposed model represents the structure of a multilayer network as a tensor, which admits a Tucker low-rank decomposition. This formulation poses significant challenges on the estimation and statistical inference for the latent positions and connection matrices, where existing techniques are inapplicable. To tackle this issue, a novel unfolding and fusion method is developed to facilitate estimation. We establish both consistency and asymptotic normality for the estimated latent positions and connection matrices, which paves the way for statistical inference tasks in multilayer network applications, such as constructing confidence regions for the latent positions and testing whether two network layers share the same structure. We validate the proposed method through extensive simulation studies and demonstrate its practical utility on real-world data.

</details>


### [19] [Generalized entropy calibration for inference with partially observed data: A unified framework](https://arxiv.org/abs/2602.19203)
*Mst Moushumi Pervin,Hengfang Wang,Jae Kwang Kim*

Main category: stat.ME

TL;DR: 提出基于广义熵校准加权的统一框架，用于处理缺失数据问题，构建双重稳健估计量，在缺失随机机制下通过机器学习方法提高估计效率


<details>
  <summary>Details</summary>
Motivation: 缺失数据是统计学中的普遍问题，需要开发一个统一的框架来处理缺失随机机制下的参数估计，克服现有方法的局限性

Method: 基于广义熵校准加权，通过最小化凸熵构建权重，约束包括：(1)数据自适应校准函数的平衡约束，使用机器学习预测器交叉拟合；(2)倾向得分模型的去偏约束

Result: 估计量具有双重稳健性：只要结果回归模型或倾向得分模型之一正确设定，估计量就保持一致性；当两个模型都正确设定时，达到半参数效率界。该方法在模拟和实际数据应用中表现出比现有方法更高的效率和数值稳定性

Conclusion: 提出的广义熵校准加权框架为缺失数据问题提供了统一的解决方案，在多种重要场景中具有广泛适用性，特别是在结果回归模型误设时优于经典的增强逆概率加权估计量

Abstract: Missing data is an universal problem in statistics. We develop a unified framework for estimating parameters defined by general estimating equations under a missing-at-random (MAR) mechanism, based on generalized entropy calibration weighting. We construct weights by minimizing a convex entropy subject to (i) balancing constraints on a data-adaptive calibration function, estimated using flexible machine-learning predictors with cross-fitting, and (ii) a debiasing constraint involving the fitted propensity score (PS) model. The resulting estimator is doubly robust, remaining consistent if either the outcome regression (OR) or the PS model is correctly specified, and attains the semiparametric efficiency bound when both models are correctly specified. Our formulation encompasses classical inverse probability weighting (IPW) and augmented IPW (AIPW) as special cases and accommodates a broad class of entropy functions. We illustrate the versatility of the approach in three important settings: semi-supervised learning with unlabeled outcomes, regression analysis with missing covariates, and causal effect estimation in observational studies. Extensive simulation studies and real-data applications demonstrate that the proposed estimators achieve greater efficiency and numerical stability than existing methods. In particular, the proposed estimator outperforms the classical AIPW estimator under the OR model misspecification.

</details>


### [20] [Statistical Measures for Explainable Aspect-Based Sentiment Analysis: A Case Study on Environmental Discourse in Reddit](https://arxiv.org/abs/2602.19216)
*Luisa Stracqualursi,Patrizia Agati*

Main category: stat.ME

TL;DR: 本文提出一个统计、模型无关的框架，用于评估ABSA模型的行为透明度和可信度，通过多个指标分析模型在无标签数据场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然基于Transformer的ABSA模型在细粒度情感分析上表现出色，但其黑盒性质限制了可解释性，在无标签数据的实际应用中存在风险。需要一种方法来评估模型的透明度和可信度。

Method: 提出一个统计、模型无关的框架，使用多个指标：极性分布的熵、基于软计数的优势分数、源间情感分歧等。通过bootstrap重采样和敏感性分析验证指标的鲁棒性。

Result: 在环保主题Reddit社区的案例研究中，提出的指标能够提供模型确定性、决策性和跨源变异性的可解释诊断。结果表明统计指标可以补充传统方法。

Conclusion: 基于软输出的统计指标提供了一种计算高效的方法，可在无标签数据场景下验证、监控和解释ABSA模型，增强模型透明度和可信度。

Abstract: Aspect-Based Sentiment Analysis (ABSA) provides a fine-grained understanding of opinions by linking sentiment to specific aspects in text. While transformer-based models excel at this task, their black-box nature limits their interpretability, posing risks in real-world applications without labeled data. This paper introduces a statistical, model-agnostic framework to assess the behavioral transparency and trustworthiness of ABSA models. Our framework relies on several metrics, such as the entropy of polarity distributions, soft-count-based dominance scores, and sentiment divergence between sources, whose robustness is validated through bootstrap resampling and sensitivity analysis. A case study on environmentally focused Reddit communities illustrates how the proposed indicators provide interpretable diagnostics of model certainty, decisiveness, and cross-source variability. The results show that statistical indicators computed on soft outputs can complement traditional approaches, offering a computationally efficient methodology for validating, monitoring, and interpreting ABSA models in contexts where labeled data are unavailable.

</details>


### [21] [A likelihood approach to proper analysis of secondary outcomes in matched case-control studies](https://arxiv.org/abs/2602.19220)
*Shanshan Liu,Guoqing Diao*

Main category: stat.ME

TL;DR: 提出针对匹配病例对照研究中次要结局分析的新统计方法，解决传统方法因抽样偏差导致的估计偏倚问题


<details>
  <summary>Details</summary>
Motivation: 匹配病例对照研究在流行病学中广泛应用，但分析次要结局时，传统统计方法（如最小二乘回归）会因病例对照设计和匹配导致的抽样不均而产生误导性结果

Method: 提出新的统计方法，充分考虑研究设计和抽样方案，确保次要结局数据分析的准确性。方法提供一致的估计和准确的置信区间覆盖概率

Result: 通过模拟研究和糖尿病患者的实际应用验证了新方法的优势。新方法能提供更准确可靠的估计结果

Conclusion: 提出的新方法能有效解决匹配病例对照研究中次要结局分析的问题，提供公开可用的R代码实现，有助于流行病学研究获得更可靠的结果

Abstract: Matched case-control studies are commonly employed in epidemiological research for their convenience and efficiency. Analysis of secondary outcomes can yield valuable insights into biological pathways and help identify genetic variants of importance. Naive analysis using standard statistical methods, such as least-squares regression for quantitative traits, can be misleading because they fail to account for unequal sampling induced by the case-control design and matching. In this paper, we propose novel statistical methods that appropriately reflect the study design and sampling scheme in the analysis of secondary outcome data. The new methods provide consistent estimation and accurate coverage probabilities for the confidence interval estimators. We demonstrate the advantages of the new methods through simulation studies and a real application with diabetes patients. R code implementing the proposed methods is publicly available.

</details>


### [22] [CoMET: A Compressed Bayesian Mixed-Effects Model for High-Dimensional Tensors](https://arxiv.org/abs/2602.19236)
*Sreya Sarkar,Kshitij Khare,Sanvesh Srivastava*

Main category: stat.ME

TL;DR: CoMET提出了一种用于高维重复测量数据的贝叶斯混合效应张量模型，通过结构化随机投影和低秩分解实现计算高效的后验推断


<details>
  <summary>Details</summary>
Motivation: 现有高维混合效应方法主要关注向量值协变量的惩罚估计，缺乏支持张量值固定效应和随机效应协变量的计算可行的贝叶斯采样框架

Method: CoMET采用模式化随机投影压缩随机效应协方差，利用低秩张量分解和边缘结构Horseshoe先验进行固定效应选择，实现高效的折叠Gibbs采样

Result: CoMET在计算复杂度上近似线性增长，理论保证后验预测风险衰减为零，在模拟研究和面部表情预测、音乐情感建模等应用中优于惩罚方法

Conclusion: CoMET为高维重复测量数据提供了一种计算高效、理论保证的贝叶斯混合效应张量建模框架，填补了现有方法的空白

Abstract: Mixed-effects models are fundamental tools for analyzing clustered and repeated-measures data, but existing high-dimensional methods largely focus on penalized estimation with vector-valued covariates. Bayesian alternatives in this regime are limited, with no sampling-based mixed-effects framework that supports tensor-valued fixed- and random-effects covariates while remaining computationally tractable. We propose the Compressed Mixed-Effects Tensor (CoMET) model for high-dimensional repeated-measures data with scalar responses and tensor-valued covariates. CoMET performs structured, mode-wise random projection of the random-effects covariance, yielding a low-dimensional covariance parameter that admits simple Gaussian prior specification and enables efficient imputation of compressed random-effects. For the mean structure, CoMET leverages a low-rank tensor decomposition and margin-structured Horseshoe priors to enable fixed-effects selection. These design choices lead to an efficient collapsed Gibbs sampler whose computational complexity grows approximately linearly with the tensor covariate dimensions. We establish high-dimensional theoretical guarantees by identifying regularity conditions under which CoMET's posterior predictive risk decays to zero. Empirically, CoMET outperforms penalized competitors across a range of simulation studies and two benchmark applications involving facial-expression prediction and music emotion modeling.

</details>


### [23] [Localized conformal model selection](https://arxiv.org/abs/2602.19284)
*Yuhao Wang,Tengyao Wang*

Main category: stat.ME

TL;DR: 提出局部化共形模型选择框架，通过对称模型选择和数据依赖的安全索引集，在保持有限样本边际覆盖的同时适应空间异质性和模型复杂性


<details>
  <summary>Details</summary>
Motivation: 现有分布自由预测方法通常使用固定模型，难以适应空间异质性和模型复杂性变化，需要在保持预测有效性的同时实现局部自适应

Method: 使用上下代理区间在标定点上进行对称模型选择，构建包含oracle模型并保持可交换性的数据依赖安全索引集，采用集成程序

Result: 模拟实验显示，与最佳固定模型相比，区间长度显著减少，特别是在异质性和低噪声设置下效果更明显

Conclusion: 局部化共形模型选择框架在保持精确有限样本边际覆盖的同时，成功适应空间异质性和模型复杂性，实现了预测性能的提升

Abstract: We propose a localized conformal model selection framework that integrates local adaptivity with post-selection validity for distribution-free prediction. By performing model selection symmetrically across calibration points using upper and lower surrogate intervals, we construct a data-dependent safe index set that contains the oracle model and preserves exchangeability. The resulting ensemble procedure retains exact finite-sample marginal coverage while adapting to spatial heterogeneity and model complexity. Simulations demonstrate substantial reductions in interval length compared to the best fixed model, especially in heterogeneous and low-noise settings.

</details>


### [24] [Identification and estimation of the conditional average treatment effect with nonignorable missing covariates, treatment, and outcome](https://arxiv.org/abs/2602.19378)
*Shuozhi Zuo,Yixin Wang,Fan Yang*

Main category: stat.ME

TL;DR: 论文建立了在多元MNAR机制下CATE的非参数识别，开发了估计器并提出了敏感性分析框架


<details>
  <summary>Details</summary>
Motivation: 处理效应异质性对政策评估、社会科学和精准医学至关重要，但在观察性研究中，协变量、处理和结果常存在缺失。当缺失依赖于未观测值时（MNAR），标准方法会导致CATE估计偏差

Method: 建立了在协变量、处理和结果都可能为MNAR的多元MNAR机制下CATE的非参数识别；开发了非参数和参数估计器；提出了评估缺失假设违反时稳健性的敏感性分析框架

Result: 实现了在多元MNAR机制下CATE的非参数识别，提供了估计方法和敏感性分析工具

Conclusion: 该研究为处理效应异质性分析提供了在MNAR缺失机制下的识别、估计和敏感性分析方法，增强了观察性研究结果的可靠性

Abstract: Treatment effect heterogeneity is central to policy evaluation, social science, and precision medicine, where interventions can affect individuals differently. In observational studies, covariates, treatment, and outcomes are often only partially observed. When missingness depends on unobserved values (missing not at random; MNAR), standard methods can yield biased estimates of the conditional average treatment effect (CATE). This paper establishes nonparametric identification of the CATE under multivariate MNAR mechanisms that allow covariates, treatment, and outcomes to be MNAR. It also develops nonparametric and parametric estimators and proposes a sensitivity analysis framework for assessing robustness to violations of the missingness assumptions.

</details>


### [25] [Variable selection via knockoffs for clustered data](https://arxiv.org/abs/2602.19398)
*Silvia Bacci,Leonardo Grilli,Carla Rampichini*

Main category: stat.ME

TL;DR: 提出一种针对聚类数据（如重复测量数据）的knockoffs变量选择方法，通过将预测变量分解为聚类水平和观测水平两个部分，分别进行变量选择


<details>
  <summary>Details</summary>
Motivation: 在聚类数据中，变量选择很复杂，因为有些预测变量在观测水平（level 1）测量，有些在聚类水平（level 2）测量。传统的变量选择方法直接应用于包含两个水平预测变量的完整数据矩阵时效果不佳。

Method: 采用两步法：1) 将每个level 1预测变量分解为聚类均值和偏离聚类均值的分量；2) 分别在两个水平上进行变量选择：level 1数据矩阵包含偏离聚类均值的分量，level 2数据矩阵包含level 1预测变量的聚类均值和level 2预测变量。

Result: 模拟研究比较了sequential knockoff、derandomized knockoff和Lasso方法。所有方法应用于完整数据矩阵时都失败，但分别应用于level 1和level 2数据矩阵时表现更好。sequential knockoffs方法显著优于Lasso和derandomized knockoffs。

Conclusion: 在聚类数据框架中实施knockoffs方法是可行、灵活且有效的，能够处理多水平预测变量的复杂结构，在错误发现率和统计功效方面表现满意。

Abstract: We extend the knockoffs method for selecting predictors to clustered data (cross-sectional or repeated measures). In the setting of clustered data, variable selection is complex because some predictors are measured at the observation level (level 1), whereas others are measured at the cluster level (level 2), so their values are constant within clusters. The solution we propose is to conduct variable selection separately at the two levels. To this end, we suggest a two-step approach: (i) decompose each level 1 predictor into level 2 and level 1 components by replacing it with the cluster mean and the deviation from the cluster mean; (ii) perform variable selection separately at the two levels, where the level 1 data matrix includes the deviations from the cluster means and the level 2 data matrix includes the cluster means of level 1 predictors and the level 2 predictors. To evaluate the performance of the proposed approach, we conduct a simulation study comparing the sequential knockoff, the derandomized knockoff, and the Lasso. The study shows satisfactory results in terms of false discovery rate and power. All methods fail when applied to the complete data matrix, including both level 1 and level 2 predictors. In contrast, all methods perform better when applied to the level 1 and level 2 data matrices separately. Moreover, the sequential knockoffs method performs substantially better than the Lasso and the derandomized knockoffs. Our proposal to implement the knockoffs method in a clustered data framework is feasible, flexible, and effective.

</details>


### [26] [Local depth-based classification of directional data](https://arxiv.org/abs/2602.19648)
*Giuseppe Gismondi,Rebecca Rivieccio,Giuseppe Pandolfo*

Main category: stat.ME

TL;DR: 提出一种使用局部数据深度函数在DD-plot中对方向性数据进行分类的方法


<details>
  <summary>Details</summary>
Motivation: 方向性数据在许多应用中自然表示为单位向量或单位超球面上的观测值，需要有效的分类方法

Method: 使用局部数据深度函数应用于DD-plot（深度vs深度图）来分类方向性数据

Result: 通过广泛的模拟研究和两个真实数据示例验证了所提方法的有效性

Conclusion: 局部数据深度函数在DD-plot中为方向性数据分类提供了有效的工具

Abstract: Directional data arise in many applications where observations are naturally represented as unit vectors or as observations on the surface of a unit hypersphere. In this context, statistical depth functions provide a center--outward ordering of the data. This work aims at proposing the use of a local notion of data depth function to be applied in the DD-plot (Depth vs. Depth plot) to classify directional data. The proposed method is investigated through an extensive simulation study and two real-data examples.

</details>


### [27] [Individualized Causal Effects under Network Interference with Combinatorial Treatments](https://arxiv.org/abs/2602.19738)
*Yunping Lu,Haoang Chi,Qirui Hu,Zhiheng Zhang*

Main category: stat.ME

TL;DR: 提出一个统一框架，用于在高维组合治疗和网络干扰下进行个体化因果推断，通过全局潜在结果模拟器结合根网络配置、双重稳健正交化和稀疏谱学习来处理指数级干预空间。


<details>
  <summary>Details</summary>
Motivation: 现代因果决策需要在网络中估计个体化治疗效果，其中干预是高维组合向量。网络干扰、效应异质性和多维治疗分别已有研究，但它们的交集产生了指数级大的干预空间，使得标准识别工具和低维暴露映射不可行。

Method: 1) 使用根网络配置利用局部平滑性；2) 双重稳健正交化减轻网络位置和协变量的混杂；3) 稀疏谱学习高效估计2^p维治疗空间上的响应曲面。还将网络效应分解为自身治疗、结构和交互分量。

Result: 提供了有限样本误差界和渐近一致性保证，表明个体化因果推断在高维网络设置中仍然可行，无需压缩干预空间。

Conclusion: 该框架成功解决了网络干扰、效应异质性和高维组合治疗的交集问题，使个体化因果推断在指数级干预空间中保持可行性，为复杂网络环境中的因果决策提供了实用工具。

Abstract: Modern causal decision-making increasingly demands individualized treatment-effect estimation in networks where interventions are high-dimensional, combinatorial vectors. While network interference, effect heterogeneity, and multi-dimensional treatments have been studied separately, their intersection yields an exponentially large intervention space that makes standard identification tools and low-dimensional exposure mappings untenable. We bridge this gap with a unified framework that constructs a \emph{global potential-outcome emulator} for unit-level inference. Our method combines (1) rooted network configurations to leverage local smoothness, (2) doubly robust orthogonalization to mitigate confounding from network position and covariates, and (3) sparse spectral learning to efficiently estimate response surfaces over the $2^p$-dimensional treatment space. We also decompose networked effects into own-treatment, structural, and interaction components, and provide finite-sample error bounds and asymptotic consistency guarantees. Overall, we show that individualized causal inference remains feasible in high-dimensional networked settings without collapsing the intervention space.

</details>


### [28] [Orthogonal Uplift Learning with Permutation-Invariant Representations for Combinatorial Treatments](https://arxiv.org/abs/2602.19851)
*Xinyan Su,Jiacan Gao,Mingyuan Ma,Xiao Xu,Xinrui Wan,Tianqi Gu,Enyun Yu,Jiecheng Guo,Zhiheng Zhang*

Main category: stat.ME

TL;DR: 提出一种用于组合治疗（政策）的提升估计框架，通过基于因果语义的治疗表示和正交化低秩模型，提高在长尾政策场景下的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的干预通常是组合性的（如营销政策包含多个上下文依赖的行动分布），但现有方法大多依赖分类或不透明的编码，限制了在罕见或新部署政策下的鲁棒性和泛化能力。

Method: 将每个政策表示为它在上下文-行动组件上诱导的混合分布，通过置换不变聚合进行嵌入；将此表示整合到正交化低秩提升模型中，将Robinson式分解扩展到学习的向量值治疗。

Result: 实验表明该方法在大型随机平台数据上提高了提升估计的准确性和稳定性，特别是在长尾政策场景下。

Conclusion: 提出的框架能够表达政策诱导的因果效应，对干扰估计误差具有正交鲁棒性，并且在小的政策扰动下保持稳定，为组合治疗的提升估计提供了有效的解决方案。

Abstract: We study uplift estimation for combinatorial treatments. Uplift measures the pure incremental causal effect of an intervention (e.g., sending a coupon or a marketing message) on user behavior, modeled as a conditional individual treatment effect. Many real-world interventions are combinatorial: a treatment is a policy that specifies context-dependent action distributions rather than a single atomic label. Although recent work considers structured treatments, most methods rely on categorical or opaque encodings, limiting robustness and generalization to rare or newly deployed policies. We propose an uplift estimation framework that aligns treatment representation with causal semantics. Each policy is represented by the mixture it induces over contextaction components and embedded via a permutation-invariant aggregation. This representation is integrated into an orthogonalized low-rank uplift model, extending Robinson-style decompositions to learned, vector-valued treatments. We show that the resulting estimator is expressive for policy-induced causal effects, orthogonally robust to nuisance estimation errors, and stable under small policy perturbations. Experiments on large-scale randomized platform data demonstrate improved uplift accuracy and stability in long-tailed policy regimes

</details>


### [29] [Transfer Learning with Network Embeddings under Structured Missingness](https://arxiv.org/abs/2602.19922)
*Mengyan Li,Xiaoou Li,Kenneth D Mandl,Tianxi Cai*

Main category: stat.ME

TL;DR: TransNEST是一个用于结构化缺失数据下的迁移学习框架，通过整合源站点和目标站点的图数据与先验组结构来构建和优化网络嵌入，特别适用于多站点异构数据场景。


<details>
  <summary>Details</summary>
Motivation: 现代数据驱动应用依赖多站点收集的异构数据集，但数据可用性、特征表示和基础人群的差异导致结构化缺失，现有迁移学习方法往往忽略这种结构，限制了跨站点捕获有意义关系的能力。

Method: 提出TransNEST框架，整合源站点和目标站点的图数据与先验组结构来构建和优化网络嵌入，能够适应站点特定特征，自适应捕获组内异质性和站点间差异，并在部分特征重叠情况下改进嵌入估计。

Result: 建立了TransNEST估计器的收敛率，在模拟中展示了强大的有限样本性能。在电子健康记录研究中，TransNEST将特征嵌入从综合医院系统迁移到儿科医院系统，利用分层本体结构改进儿科嵌入，在识别儿科特定关系特征对方面比基准方法获得最佳准确率。

Conclusion: TransNEST是一个有效的迁移学习框架，能够处理结构化缺失数据，在多站点异构数据场景中实现更好的知识迁移和特征嵌入优化，特别适用于医疗健康等需要跨机构知识共享的领域。

Abstract: Modern data-driven applications increasingly rely on large, heterogeneous datasets collected across multiple sites. Differences in data availability, feature representation, and underlying populations often induce structured missingness, complicating efforts to transfer information from data-rich settings to those with limited data. Many transfer learning methods overlook this structure, limiting their ability to capture meaningful relationships across sites. We propose TransNEST (Transfer learning with Network Embeddings under STructured missingness), a framework that integrates graphical data from source and target sites with prior group structure to construct and refine network embeddings. TransNEST accommodates site-specific features, captures within-group heterogeneity and between-site differences adaptively, and improves embedding estimation under partial feature overlap. We establish the convergence rate for the TransNEST estimator and demonstrate strong finite-sample performance in simulations. We apply TransNEST to a multi-site electronic health record study, transferring feature embeddings from a general hospital system to a pediatric hospital system. Using a hierarchical ontology structure, TransNEST improves pediatric embeddings and supports more accurate pediatric knowledge extraction, achieving the best accuracy for identifying pediatric-specific relational feature pairs compared with benchmark methods.

</details>


### [30] [Covariance estimation for derivatives of functional data using an additive penalty in P-splines](https://arxiv.org/abs/2602.20029)
*Yueyun Zhu,Steven Golovkine,Norma Bargary,Andrew J. Simpkin*

Main category: stat.ME

TL;DR: 提出一种结合FACE算法和附加惩罚项的P样条方法，用于估计函数型数据的协方差导数，并应用于导数型FPCA，扩展到多元情况（DMFPCA），在人类运动数据分析中表现良好。


<details>
  <summary>Details</summary>
Motivation: 函数型数据导数估计在导数型功能主成分分析（FPCA）中很重要。现有P样条方法通过附加惩罚项改进导数估计，但需要更高效的协方差估计方法。

Method: 将快速协方差估计（FACE）算法与带附加惩罚项的P样条结合，提出协方差导数估计方法。基于此开发导数型FPCA算法，并扩展到多元情况（DMFPCA）。

Result: 新算法在模拟中与现有FPCAder()函数比较表现良好。DMFPCA应用于人类关节角度数据，导数型得分能有效区分不同运动任务。

Conclusion: 提出的方法结合FACE和附加惩罚P样条，能有效估计函数型数据协方差导数，支持导数型FPCA分析，在多元扩展中展示实用价值。

Abstract: P-splines provide a flexible and computationally efficient smoothing framework and are commonly used for derivative estimation in functional data. Including an additive penalty term in P-splines has been shown to improve estimates of derivatives. We propose a method which incorporates the fast covariance estimation (FACE) algorithm with an additive penalty in P-splines. The proposed method is used to estimate derivatives of covariance for functional data, which play an important role in derivative-based functional principal component analysis (FPCA). Following this, we provide an algorithm for estimating the eigenfunctions and their corresponding scores in derivative-based FPCA. For comparison, we evaluate our algorithm against an existing function \texttt{FPCAder()} in simulation. In addition, we extend the algorithm to multivariate cases, referred to as derivative multivariate functional principal component analysis (DMFPCA). DMFPCA is applied to joint angles in human movement data, where the derivative-based scores demonstrate strong performance in distinguishing locomotion tasks.

</details>


### [31] [Improving the Power of Bonferroni Adjustments under Joint Normality and Exchangeability](https://arxiv.org/abs/2602.20118)
*Caleb Hiltunen,Yeonwoo Rho*

Main category: stat.ME

TL;DR: 提出了一种针对联合正态且可交换检验统计量的Bonferroni校正改进方法，在稀疏备择假设下具有更高功效


<details>
  <summary>Details</summary>
Motivation: Bonferroni校正在处理多重检验时虽然流行，但在检验统计量相关时功效较低，需要改进

Method: 提出Bonferroni校正的实用修改方法，适用于联合正态且可交换的检验统计量

Result: 模拟研究表明该方法在稀疏备择假设下具有更高功效，且理论证明能控制族错误率

Conclusion: 该方法为实践者提供了直观且功效更高的多重检验校正工具，特别适用于稀疏信号检测

Abstract: Bonferroni's correction is a popular tool to address multiplicity but is notorious for its low power when tests are dependent. This paper proposes a practical modification of Bonferroni's correction when test statistics are jointly normal and exchangeable. This method is intuitive to practitioners and achieves higher power in sparse alternatives, as our simulations suggest. We also prove that this method successfully controls the family-wise error rate at any significance level.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [32] [Multiclass Calibration Assessment and Recalibration of Probability Predictions via the Linear Log Odds Calibration Function](https://arxiv.org/abs/2602.18573)
*Amy Vennos,Xin Xing,Christopher T. Franck*

Main category: stat.ML

TL;DR: 提出MCLLO方法用于多类别概率预测的重新校准，包括假设检验评估校准性，无需模型内部访问，且易于解释。


<details>
  <summary>Details</summary>
Motivation: 现有多类别重新校准方法存在三个限制：(1)只能比较多个模型间的校准性，无法直接评估单个模型；(2)需要访问模型内部（如神经网络logits）；(3)输出结果难以理解。

Method: 提出多类别线性对数几率（MCLLO）重新校准方法，包含似然比假设检验来评估校准性，无需模型内部访问，适用于广泛的分类问题，且结果易于解释。

Result: 通过模拟实验和三个真实案例（CNN图像分类、随机森林肥胖分析、回归模型生态学）验证MCLLO有效性。与四种现有方法比较，使用假设检验和预期校准误差指标，显示MCLLO单独使用或与其他方法结合均表现良好。

Conclusion: MCLLO方法克服了现有方法的局限性，提供了一种无需模型内部访问、易于解释的多类别概率预测重新校准方案，具有广泛适用性。

Abstract: Machine-generated probability predictions are essential in modern classification tasks such as image classification. A model is well calibrated when its predicted probabilities correspond to observed event frequencies. Despite the need for multicategory recalibration methods, existing methods are limited to (i) comparing calibration between two or more models rather than directly assessing the calibration of a single model, (ii) requiring under-the-hood model access, e.g., accessing logit-scale predictions within the layers of a neural network, and (iii) providing output which is difficult for human analysts to understand. To overcome (i)-(iii), we propose Multicategory Linear Log Odds (MCLLO) recalibration, which (i) includes a likelihood ratio hypothesis test to assess calibration, (ii) does not require under-the-hood access to models and is thus applicable on a wide range of classification problems, and (iii) can be easily interpreted. We demonstrate the effectiveness of the MCLLO method through simulations and three real-world case studies involving image classification via convolutional neural network, obesity analysis via random forest, and ecology via regression modeling. We compare MCLLO to four comparator recalibration techniques utilizing both our hypothesis test and the existing calibration metric Expected Calibration Error to show that our method works well alone and in concert with other methods.

</details>


### [33] [Stochastic Gradient Variational Inference with Price's Gradient Estimator from Bures-Wasserstein to Parameter Space](https://arxiv.org/abs/2602.18718)
*Kyurae Kim,Qiang Fu,Yi-An Ma,Jacob R. Gardner,Trevor Campbell*

Main category: stat.ML

TL;DR: 本文通过理论分析表明，Wasserstein VI和Black-box VI在收敛性上可以达到相同的状态最优复杂度，关键在于使用Price梯度（利用二阶信息）而非参数空间方法本身。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，在高斯变分族中，Wasserstein VI（WVI）的收敛性保证优于Black-box VI（BBVI），这暗示了测度空间方法可能有独特优势。本文旨在探究这种优势的来源，并缩小两种方法之间的理论差距。

Method: 通过理论分析，识别出WVI优越性的真正来源是它使用的特定梯度估计器（Price梯度），该估计器利用目标对数密度的二阶信息（Hessian矩阵）。研究表明BBVI只需稍作修改也能使用相同的梯度估计器，而WVI也可以使用仅需一阶梯度的重参数化梯度。

Result: 获得了WVI和BBVI相同的状态最优迭代复杂度保证，证明两种方法在理论收敛性上是等价的。实证结果表明，使用Price梯度是性能提升的主要来源，而不是测度空间方法本身。

Conclusion: WVI和BBVI的性能差异主要源于使用的梯度估计器类型，而非方法本身的理论优势。Price梯度（利用二阶信息）是性能提升的关键，两种方法都可以灵活选择使用不同的梯度估计器。

Abstract: For approximating a target distribution given only its unnormalized log-density, stochastic gradient-based variational inference (VI) algorithms are a popular approach. For example, Wasserstein VI (WVI) and black-box VI (BBVI) perform gradient descent in measure space (Bures-Wasserstein space) and parameter space, respectively. Previously, for the Gaussian variational family, convergence guarantees for WVI have shown superiority over existing results for black-box VI with the reparametrization gradient, suggesting the measure space approach might provide some unique benefits. In this work, however, we close this gap by obtaining identical state-of-the-art iteration complexity guarantees for both. In particular, we identify that WVI's superiority stems from the specific gradient estimator it uses, which BBVI can also leverage with minor modifications. The estimator in question is usually associated with Price's theorem and utilizes second-order information (Hessians) of the target log-density. We will refer to this as Price's gradient. On the flip side, WVI can be made more widely applicable by using the reparametrization gradient, which requires only gradients of the log-density. We empirically demonstrate that the use of Price's gradient is the major source of performance improvement.

</details>


### [34] [Bounds and Identification of Joint Probabilities of Potential Outcomes and Observed Variables under Monotonicity Assumptions](https://arxiv.org/abs/2602.18762)
*Naoya Hashimoto,Yuta Kawakami,Jin Tian*

Main category: stat.ML

TL;DR: 该论文提出新的单调性假设来界定和识别潜在结果与观测变量的联合概率及其线性组合，针对离散处理和离散有序结果场景，通过线性规划方法求解边界，并通过数值实验验证方法有效性。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，评估潜在结果与观测变量的联合概率及其线性组合是一个基本挑战。现有方法在离散处理和离散有序结果场景下存在局限性，需要新的理论框架来更好地界定和识别这些概率。

Method: 1. 提出新的单调性假设族；2. 将边界问题构建为线性规划问题；3. 引入专门用于实现识别的新单调性假设；4. 通过数值实验验证方法；5. 使用真实世界数据集进行应用演示。

Result: 论文开发了新的理论框架，能够有效界定和识别潜在结果与观测变量的联合概率。数值实验验证了所提方法的有效性，并在真实数据集上展示了实际应用价值。

Conclusion: 该研究为因果推断中联合概率的界定和识别问题提供了新的解决方案，特别是在离散处理和离散有序结果场景下，提出的单调性假设和线性规划方法具有理论和实践意义。

Abstract: Evaluating joint probabilities of potential outcomes and observed variables, and their linear combinations, is a fundamental challenge in causal inference. This paper addresses the bounding and identification of these probabilities in settings with discrete treatment and discrete ordinal outcome. We propose new families of monotonicity assumptions and formulate the bounding problem as a linear programming problem. We further introduce a new monotonicity assumption specifically to achieve identification. Finally, we present numerical experiments to validate our methods and demonstrate their application using real-world datasets.

</details>


### [35] [Federated Measurement of Demographic Disparities from Quantile Sketches](https://arxiv.org/abs/2602.18870)
*Arthur Charpentier,Agathe Fernandes Machado,Olivier Côté,François Hu*

Main category: stat.ML

TL;DR: 提出了一种联邦审计人口统计均等性的方法，通过分数分布的Wasserstein-Frechet方差衡量差异，并开发了通信高效的协议来估计全局差异及其分解。


<details>
  <summary>Details</summary>
Motivation: 由于隐私法规限制，数据通常以孤岛形式存在，无法共享，这使得在联邦学习环境中审计公平性变得困难。需要一种能够在保护隐私的同时，在联邦设置下评估人口统计均等性的方法。

Method: 使用分数分布的Wasserstein-Frechet方差衡量敏感群体间的差异，提出了平方Wasserstein距离的ANOVA式分解，将选择效应与跨孤岛异质性分离。设计了单轮通信协议，每个孤岛仅共享组计数和本地分数分布的量化摘要。

Result: 理论证明得到了局部和全局指标之间的紧致界限。实验表明，几十个分位数就足以恢复全局差异并诊断其来源，在合成数据和COMPAS数据集上验证了方法的有效性。

Conclusion: 该方法能够在联邦学习环境中高效审计人口统计均等性，通过量化分解帮助理解局部-全局不匹配的来源，为隐私保护的公平性评估提供了实用工具。

Abstract: Many fairness goals are defined at a population level that misaligns with siloed data collection, which remains unsharable due to privacy regulations. Horizontal federated learning (FL) enables collaborative modeling across clients with aligned features without sharing raw data. We study federated auditing of demographic parity through score distributions, measuring disparity as a Wasserstein--Frechet variance between sensitive-group score laws, and expressing the population metric in federated form that makes explicit how silo-specific selection drives local-global mismatch. For the squared Wasserstein distance, we prove an ANOVA-style decomposition that separates (i) selection-induced mixture effects from (ii) cross-silo heterogeneity, yielding tight bounds linking local and global metrics. We then propose a one-shot, communication-efficient protocol in which each silo shares only group counts and a quantile summary of its local score distributions, enabling the server to estimate global disparity and its decomposition, with $O(1/k)$ discretization bias ($k$ quantiles) and finite-sample guarantees. Experiments on synthetic data and COMPAS show that a few dozen quantiles suffice to recover global disparity and diagnose its sources.

</details>


### [36] [Implicit Bias and Convergence of Matrix Stochastic Mirror Descent](https://arxiv.org/abs/2602.18997)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: stat.ML

TL;DR: 论文研究了矩阵参数的随机镜像下降算法在过参数化情况下的收敛性和隐式偏差，证明了算法会指数收敛到全局插值器，并且收敛到初始化条件下Bregman散度最小的唯一解。


<details>
  <summary>Details</summary>
Motivation: 研究矩阵参数随机镜像下降算法在多类分类和矩阵补全问题中的行为，特别是在过参数化情况下，理解矩阵镜像函数如何影响算法的收敛性和隐式偏差。

Method: 使用矩阵参数的随机镜像下降算法，分析其在过参数化情况下的收敛性，研究矩阵镜像函数ψ(·)对算法行为的影响，并推广经典向量SMD的隐式偏差结果到矩阵情况。

Result: 证明了矩阵SMD在过参数化情况下指数收敛到全局插值器，并且收敛到初始化条件下Bregman散度最小的唯一解，揭示了矩阵镜像映射在高维多输出问题中的归纳偏差作用。

Conclusion: 矩阵镜像下降算法在过参数化情况下具有明确的收敛性和隐式偏差特性，矩阵镜像函数决定了算法在高维多输出问题中的归纳偏差，为多类分类和矩阵补全问题提供了理论保证。

Abstract: We investigate Stochastic Mirror Descent (SMD) with matrix parameters and vector-valued predictions, a framework relevant to multi-class classification and matrix completion problems. Focusing on the overparameterized regime, where the total number of parameters exceeds the number of training samples, we prove that SMD with matrix mirror functions $ψ(\cdot)$ converges exponentially to a global interpolator. Furthermore, we generalize classical implicit bias results of vector SMD by demonstrating that the matrix SMD algorithm converges to the unique solution minimizing the Bregman divergence induced by $ψ(\cdot)$ from initialization subject to interpolating the data. These findings reveal how matrix mirror maps dictate inductive bias in high-dimensional, multi-output problems.

</details>


### [37] [Attention Deficits in Language Models: Causal Explanations for Procedural Hallucinations](https://arxiv.org/abs/2602.19239)
*Ahmed Karim,Fatima Sheaib,Zein Khamis,Maggie Chlon,Jad Awada,Leon Chlon*

Main category: stat.ML

TL;DR: 大语言模型在执行复杂程序后，在最后一步报告自己刚计算出的值时会出现错误，这种现象被称为"程序性幻觉"。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在长上下文绑定任务中出现的"程序性幻觉"现象，即模型无法执行可验证的、基于提示的规范，即使正确答案就在上下文中。

Method: 通过分析长上下文绑定任务中的错误类型，将错误分解为Stage 2A（门控错误）和Stage 2B（绑定错误）。使用线性探针分析最终层残差流，通过可用与已用互信息以及伪先验干预来形式化"存在但未使用"的现象。

Result: 在硬任务中，Stage 2B错误占主导地位；线性探针显示正确答案在残差流中编码但未被使用；通过oracle检查点干预（在查询附近重新陈述真实绑定）可以几乎消除长距离的Stage 2B错误。

Conclusion: 大语言模型的程序性幻觉主要是路由失败问题，正确答案通常已在模型中编码但未被正确使用，通过适当的干预可以显著改善这种错误。

Abstract: Large language models can follow complex procedures yet fail at a seemingly trivial final step: reporting a value they themselves computed moments earlier. We study this phenomenon as \emph{procedural hallucination}: failure to execute a verifiable, prompt-grounded specification even when the correct value is present in context.
  In long-context binding tasks with a known single-token candidate set, we find that many errors are readout-stage routing failures. Specifically, failures decompose into Stage~2A (gating) errors, where the model does not enter answer mode, and Stage~2B (binding) errors, where it enters answer mode but selects the wrong candidate (often due to recency bias). In the hard regime, Stage~2B accounts for most errors across model families in our tasks (Table~1).
  On Stage~2B error trials, a linear probe on the final-layer residual stream recovers the correct value far above chance (e.g., 74\% vs.\ 2\% on Qwen2.5-3B; Table~2), indicating that the answer is encoded but not used. We formalize ``present but not used'' via available vs.\ used mutual information and pseudo-prior interventions, yielding output-computable diagnostics and information-budget certificates.
  Finally, an oracle checkpointing intervention that restates the true binding near the query can nearly eliminate Stage~2B failures at long distance (e.g., Qwen2.5-3B $0/400 \rightarrow 399/400$ at $k = 1024$; Table~8).

</details>


### [38] [Scaling Laws for Precision in High-Dimensional Linear Regression](https://arxiv.org/abs/2602.19241)
*Dechen Zhang,Xuan Tang,Yingyu Liang,Difan Zou*

Main category: stat.ML

TL;DR: 该论文从理论上研究了低精度训练的缩放规律，揭示了乘性和加性量化在有效模型大小上的关键差异。


<details>
  <summary>Details</summary>
Motivation: 低精度训练需要在模型大小、数据集大小和数值精度之间进行权衡优化。虽然经验缩放定律表明量化会影响有效容量，但其理论机制尚不明确，需要建立理论框架来指导实际硬件约束下的训练协议优化。

Method: 在高维草图线性回归框架内研究低精度训练的缩放规律，分析乘性（信号相关）和加性（信号独立）两种量化方案，通过理论分析揭示它们对缩放行为的影响机制。

Result: 发现两种量化方案都引入加性误差并降低有效数据大小，但在有效模型大小上存在关键差异：乘性量化保持全精度模型大小，而加性量化减少有效模型大小。数值实验验证了理论发现。

Conclusion: 通过严格刻画模型规模、数据集大小和量化误差之间的复杂相互作用，为在实际硬件约束下优化训练协议提供了原则性的理论基础。

Abstract: Low-precision training is critical for optimizing the trade-off between model quality and training costs, necessitating the joint allocation of model size, dataset size, and numerical precision. While empirical scaling laws suggest that quantization impacts effective model and data capacities or acts as an additive error, the theoretical mechanisms governing these effects remain largely unexplored. In this work, we initiate a theoretical study of scaling laws for low-precision training within a high-dimensional sketched linear regression framework. By analyzing multiplicative (signal-dependent) and additive (signal-independent) quantization, we identify a critical dichotomy in their scaling behaviors. Our analysis reveals that while both schemes introduce an additive error and degrade the effective data size, they exhibit distinct effects on effective model size: multiplicative quantization maintains the full-precision model size, whereas additive quantization reduces the effective model size. Numerical experiments validate our theoretical findings. By rigorously characterizing the complex interplay among model scale, dataset size, and quantization error, our work provides a principled theoretical basis for optimizing training protocols under practical hardware constraints.

</details>


### [39] [Goal-Oriented Influence-Maximizing Data Acquisition for Learning and Optimization](https://arxiv.org/abs/2602.19578)
*Weichi Yao,Bianca Dumitrascu,Bryan R. Goldsmith,Yixin Wang*

Main category: stat.ML

TL;DR: GOIMDA是一种主动数据采集算法，通过最大化输入对用户指定目标函数的影响来选择数据，避免显式后验推断，利用逆曲率实现不确定性感知。


<details>
  <summary>Details</summary>
Motivation: 主动数据采集在深度神经网络的学习和优化任务中至关重要，但现有方法大多依赖难以可靠获得的预测不确定性估计，因此需要一种避免显式后验推断但仍能保持不确定性感知的方法。

Method: 提出GOIMDA算法，通过最大化输入对用户指定目标函数（如测试损失、预测熵或优化器推荐设计值）的期望影响来选择数据。利用一阶影响函数推导出可处理的采集规则，结合目标梯度、训练损失曲率和候选输入对模型参数的敏感性。

Result: 理论证明对于广义线性模型，GOIMDA近似于预测熵最小化，并包含目标对齐和预测偏差的校正项。实证表明，在学习和优化任务中，GOIMDA比基于不确定性的主动学习和高斯过程贝叶斯优化基线方法，用更少的标记样本或函数评估达到目标性能。

Conclusion: GOIMDA提供了一种有效的主动数据采集方法，无需维护贝叶斯后验分布，通过逆曲率实现不确定性感知，在多种任务中显著提高了数据采集效率。

Abstract: Active data acquisition is central to many learning and optimization tasks in deep neural networks, yet remains challenging because most approaches rely on predictive uncertainty estimates that are difficult to obtain reliably. To this end, we propose Goal-Oriented Influence- Maximizing Data Acquisition (GOIMDA), an active acquisition algorithm that avoids explicit posterior inference while remaining uncertainty-aware through inverse curvature. GOIMDA selects inputs by maximizing their expected influence on a user-specified goal functional, such as test loss, predictive entropy, or the value of an optimizer-recommended design. Leveraging first-order influence functions, we derive a tractable acquisition rule that combines the goal gradient, training-loss curvature, and candidate sensitivity to model parameters. We show theoretically that, for generalized linear models, GOIMDA approximates predictive-entropy minimization up to a correction term accounting for goal alignment and prediction bias, thereby, yielding uncertainty-aware behavior without maintaining a Bayesian posterior. Empirically, across learning tasks (including image and text classification) and optimization tasks (including noisy global optimization benchmarks and neural-network hyperparameter tuning), GOIMDA consistently reaches target performance with substantially fewer labeled samples or function evaluations than uncertainty-based active learning and Gaussian-process Bayesian optimization baselines.

</details>


### [40] [Manifold-Aligned Generative Transport](https://arxiv.org/abs/2602.19600)
*Xinyu Tian,Xiaotong Shen*

Main category: stat.ML

TL;DR: MAGT是一种流式生成器，通过单次前向传递从低维基分布到数据空间的流形对齐传输，在固定高斯平滑水平下训练，使用自归一化重要性采样近似分数，实现高效采样和概率集中


<details>
  <summary>Details</summary>
Motivation: 高维生成建模本质上是流形学习问题，现有方法存在局限性：扩散模型需要多次迭代去噪步骤且可能泄露离支撑集；归一化流受可逆性和维度保持限制。需要平衡支撑集保真度和采样效率

Method: MAGT学习从低维基分布到数据空间的单次流形对齐传输，在固定高斯平滑水平下训练，使用有限潜在锚点集和自归一化重要性采样近似分数，获得可处理的训练目标

Result: MAGT能够单次前向传递采样，将概率集中在学习到的支撑集附近，诱导相对于流形体积测度的内在密度，在合成和基准数据集上提高了保真度和流形集中度，采样速度显著快于扩散模型

Conclusion: MAGT通过流形对齐传输解决了生成建模中的支撑集保真度和采样效率平衡问题，建立了有限样本Wasserstein界，为生成样本提供了原则性的似然评估方法

Abstract: High-dimensional generative modeling is fundamentally a manifold-learning problem: real data concentrate near a low-dimensional structure embedded in the ambient space. Effective generators must therefore balance support fidelity -- placing probability mass near the data manifold -- with sampling efficiency. Diffusion models often capture near-manifold structure but require many iterative denoising steps and can leak off-support; normalizing flows sample in one pass but are limited by invertibility and dimension preservation. We propose MAGT (Manifold-Aligned Generative Transport), a flow-like generator that learns a one-shot, manifold-aligned transport from a low-dimensional base distribution to the data space. Training is performed at a fixed Gaussian smoothing level, where the score is well-defined and numerically stable. We approximate this fixed-level score using a finite set of latent anchor points with self-normalized importance sampling, yielding a tractable objective. MAGT samples in a single forward pass, concentrates probability near the learned support, and induces an intrinsic density with respect to the manifold volume measure, enabling principled likelihood evaluation for generated samples. We establish finite-sample Wasserstein bounds linking smoothing level and score-approximation accuracy to generative fidelity, and empirically improve fidelity and manifold concentration across synthetic and benchmark datasets while sampling substantially faster than diffusion models.

</details>


### [41] [Smoothness Adaptivity in Constant-Depth Neural Networks: Optimal Rates via Smooth Activations](https://arxiv.org/abs/2602.19691)
*Yuhao Liu,Zilin Wang,Lei Wu,Shaobo Zhang*

Main category: stat.ML

TL;DR: 光滑激活函数在深度学习中普遍存在，但其相对于非光滑激活的理论优势尚不明确。本文证明光滑激活网络能自动利用目标函数任意高阶光滑性，达到最优逼近和估计误差率，而非光滑激活（如ReLU）则缺乏这种适应性，其逼近阶受深度限制。


<details>
  <summary>Details</summary>
Motivation: 研究光滑激活函数相对于非光滑激活的理论优势，探索激活函数光滑性在神经网络逼近和统计学习中的作用机制。

Method: 通过构造性逼近框架，为Sobolev空间中的目标函数构建显式神经网络逼近器，并严格控制参数范数和模型规模，确保在经验风险最小化下的统计可学习性。

Result: 证明光滑激活网络能自动利用任意高阶目标函数光滑性，达到最优逼近和估计误差率（对数因子内），而非光滑激活网络（如ReLU）的逼近阶受深度限制，需要深度增长才能捕获高阶光滑性。

Conclusion: 激活函数光滑性是实现统计最优性的基本机制，可作为深度的替代方案。光滑激活网络具有自适应能力，能自动利用目标函数的高阶光滑性，而非光滑激活网络则缺乏这种适应性。

Abstract: Smooth activation functions are ubiquitous in modern deep learning, yet their theoretical advantages over non-smooth counterparts remain poorly understood. In this work, we characterize both approximation and statistical properties of neural networks with smooth activations over the Sobolev space $W^{s,\infty}([0,1]^d)$ for arbitrary smoothness $s>0$. We prove that constant-depth networks equipped with smooth activations automatically exploit arbitrarily high orders of target function smoothness, achieving the minimax-optimal approximation and estimation error rates (up to logarithmic factors). In sharp contrast, networks with non-smooth activations, such as ReLU, lack this adaptivity: their attainable approximation order is strictly limited by depth, and capturing higher-order smoothness requires proportional depth growth. These results identify activation smoothness as a fundamental mechanism, alternative to depth, for attaining statistical optimality. Technically, our results are established via a constructive approximation framework that produces explicit neural network approximators with carefully controlled parameter norms and model size. This complexity control ensures statistical learnability under empirical risk minimization (ERM) and removes the impractical sparsity constraints commonly required in prior analyses.

</details>


### [42] [Ensemble Machine Learning and Statistical Procedures for Dynamic Predictions of Time-to-Event Outcomes](https://arxiv.org/abs/2602.19761)
*Nina van Gerwen,Sten Willemsen,Bettina E. Hansen,Christophe Corpechot,Marco Carbone,Cynthia Levy,Maria-Carlota Londõno,Atsushi Tanaka,Palak Trivedi,Alejandra Villamil,Gideon Hirschfield,Dimitris Rizopoulos*

Main category: stat.ML

TL;DR: 该研究将Super Learner集成学习框架扩展到动态预测领域，用于结合不同模型的预测结果，以改善原发性胆汁性胆管炎患者的生存预测准确性。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要准确的动态预测来评估原发性胆汁性胆管炎患者的肝衰竭风险。现有方法（联合建模、landmarking、机器学习）各有优劣，没有单一方法能始终最优，因此需要一种能结合不同模型优势的集成方法。

Method: 扩展Super Learner集成学习框架，通过交叉验证和针对特定应用的目标函数（如平方损失），构建候选算法库中预测的最优加权组合，特别关注适合动态预测的适当目标函数。

Result: Super Learner能够灵活结合来自具有不同假设的多样化模型输出，在原发性胆汁性胆管炎应用中展现出独特优势，其预测性能等于或优于任何单独拟合的模型。

Conclusion: Super Learner框架为动态预测提供了一种有效的集成方法，能够结合不同模型的优势，在临床决策支持中实现更准确、更灵活的生存预测。

Abstract: Dynamic predictions for longitudinal and time-to-event outcomes have become a versatile tool in precision medicine. Our work is motivated by the application of dynamic predictions in the decision-making process for primary biliary cholangitis patients. For these patients, serial biomarker measurements (e.g., bilirubin and alkaline phosphatase levels) are routinely collected to inform treating physicians of the risk of liver failure and guide clinical decision-making. Two popular statistical approaches to derive dynamic predictions are joint modelling and landmarking. However, recently, machine learning techniques have also been proposed. Each approach has its merits, and no single method exists to outperform all others. Consequently, obtaining the best possible survival estimates is challenging. Therefore, we extend the Super Learner framework to combine dynamic predictions from different models and procedures. Super Learner is an ensemble learning technique that allows users to combine different prediction algorithms to improve predictive accuracy and flexibility. It uses cross-validation and different objective functions of performance (e.g., squared loss) that suit specific applications to build the optimally weighted combination of predictions from a library of candidate algorithms. In our work, we pay special attention to appropriate objective functions for Super Learner to obtain the most optimal weighted combination of dynamic predictions. In our primary biliary cholangitis application, Super Learner presented unique benefits due to its ability to flexibly combine outputs from a diverse set of models with varying assumptions for equal or better predictive performance than any model fit separately.

</details>


### [43] [Path-conditioned training: a principled way to rescale ReLU neural networks](https://arxiv.org/abs/2602.19799)
*Arthur Lebeurrier,Titouan Vayer,Rémi Gribonval*

Main category: stat.ML

TL;DR: 提出一种基于几何准则的神经网络参数重缩放方法，通过路径提升框架对齐核函数来改善训练动态


<details>
  <summary>Details</summary>
Motivation: 尽管ReLU神经网络参数存在重缩放对称性，但现有方法缺乏系统利用这种对称性的原则性方法。虽然不同缩放权重可实现相同函数，但训练动态差异显著，需要新视角来利用这一现象。

Method: 基于路径提升框架构建紧凑因子化表示，引入几何准则重缩放神经网络参数，通过最小化该准则实现核函数对齐策略，并推导出高效的对齐算法。

Result: 分析了网络架构和初始化尺度如何共同影响方法输出，数值实验表明该方法具有加速训练的潜力。

Conclusion: 提出了一种原则性的神经网络参数重缩放方法，通过几何准则和核函数对齐改善了训练动态，为利用ReLU网络的缩放对称性提供了新视角。

Abstract: Despite recent algorithmic advances, we still lack principled ways to leverage the well-documented rescaling symmetries in ReLU neural network parameters. While two properly rescaled weights implement the same function, the training dynamics can be dramatically different. To offer a fresh perspective on exploiting this phenomenon, we build on the recent path-lifting framework, which provides a compact factorization of ReLU networks. We introduce a geometrically motivated criterion to rescale neural network parameters which minimization leads to a conditioning strategy that aligns a kernel in the path-lifting space with a chosen reference. We derive an efficient algorithm to perform this alignment. In the context of random network initialization, we analyze how the architecture and the initialization scale jointly impact the output of the proposed method. Numerical experiments illustrate its potential to speed up training.

</details>


### [44] [Dirichlet Scale Mixture Priors for Bayesian Neural Networks](https://arxiv.org/abs/2602.19859)
*August Arnstad,Leiv Rønneberg,Geir Storvik*

Main category: stat.ML

TL;DR: 提出一种新的贝叶斯神经网络先验分布——狄利克雷尺度混合先验，通过结构化稀疏诱导收缩解决BNN现有问题，在相关数据、中等小数据场景表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络存在可解释性差、预测过度自信、对抗攻击脆弱等问题。贝叶斯神经网络虽能缓解这些问题，但先验分布设定困难且常被忽略。现有先验分布存在局限性，需要更有效的结构化稀疏先验。

Method: 提出狄利克雷尺度混合先验，这是一种结构化、稀疏诱导的收缩先验。理论上推导了DSM先验的依赖结构和收缩特性，并展示了其在神经网络几何结构下的表现。通过隐式特征选择实现网络稀疏化。

Result: 在模拟和真实数据实验中，DSM先验通过隐式特征选择鼓励稀疏网络，对抗攻击下表现鲁棒，预测性能有竞争力且有效参数大幅减少。在相关、中等小数据场景优势最明显，更易于权重剪枝。重尾收缩机制还能缓解冷后验效应。

Conclusion: DSM先验为贝叶斯神经网络提供了一种原则性的替代方案，解决了现有先验分布的问题，特别适用于相关数据、中等小数据场景，具有稀疏性、鲁棒性和预测性能的平衡优势。

Abstract: Neural networks are the cornerstone of modern machine learning, yet can be difficult to interpret, give overconfident predictions and are vulnerable to adversarial attacks. Bayesian neural networks (BNNs) provide some alleviation of these limitations, but have problems of their own. The key step of specifying prior distributions in BNNs is no trivial task, yet is often skipped out of convenience. In this work, we propose a new class of prior distributions for BNNs, the Dirichlet scale mixture (DSM) prior, that addresses current limitations in Bayesian neural networks through structured, sparsity-inducing shrinkage. Theoretically, we derive general dependence structures and shrinkage results for DSM priors and show how they manifest under the geometry induced by neural networks. In experiments on simulated and real world data we find that the DSM priors encourages sparse networks through implicit feature selection, show robustness under adversarial attacks and deliver competitive predictive performance with substantially fewer effective parameters. In particular, their advantages appear most pronounced in correlated, moderately small data regimes, and are more amenable to weight pruning. Moreover, by adopting heavy-tailed shrinkage mechanisms, our approach aligns with recent findings that such priors can mitigate the cold posterior effect, offering a principled alternative to the commonly used Gaussian priors.

</details>


### [45] [JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks](https://arxiv.org/abs/2602.20153)
*Jakob Heiss,Sören Lambrecht,Jakob Weissteiner,Hanna Wutte,Žan Žurič,Josef Teichmann,Bin Yu*

Main category: stat.ML

TL;DR: 提出JUCAL算法，联合校准分类器集成中的偶然不确定性和认知不确定性，显著优于现有校准方法


<details>
  <summary>Details</summary>
Motivation: 现有校准方法（如温度缩放和保形方法）无法平衡偶然不确定性和认知不确定性，导致预测不确定性表征不准确，在某些区域过度自信而在其他区域信心不足

Method: 提出JUCAL算法，通过优化验证集上的负对数似然，联合校准两个常数来加权和缩放认知不确定性和偶然不确定性；适用于任何训练好的分类器集成，计算开销小，无需访问模型内部参数

Result: JUCAL在多种文本分类任务上显著优于SOTA校准方法，NLL和预测集大小分别降低达15%和20%；仅使用5个模型的集成就能在NLL和预测集大小上优于温度缩放校准的50个模型集成，推理成本降低达10倍

Conclusion: JUCAL是校准分类器集成的新首选方法，能有效平衡两种不确定性，提高校准质量并降低计算成本

Abstract: We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models' internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [46] [Statistical methods for reference-free single-molecule localisation microscopy](https://arxiv.org/abs/2602.18727)
*Jack Peyton,Benjamin Davis,Emily Gribbin,Daniel Rolfe,Hannah Mitchell*

Main category: stat.AP

TL;DR: 提出一个基于贝叶斯和空间统计的端到端分析框架，用于MINFLUX单分子成像数据的统计建模和结构推断，无需先验模板即可实现高精度聚类和结构重建。


<details>
  <summary>Details</summary>
Motivation: MINFLUX单分子成像技术虽然能实现<5 nm的高精度定位，但由于发射体密度变化、生物标记不完整、测量误差以及虚假检测等问题，从点云数据中进行结构推断非常困难。现有方法通常需要主观参数调整和预设结构模板，限制了无参考的发现能力。

Method: 开发了一个统计基础的端到端分析框架，结合贝叶斯和空间统计方法，包括：1）不确定性感知的测量聚类到发射体组；2）快速识别分子结构超群；3）无需大量先验知识的数据集中重复结构重建。

Result: 在模拟和真实MINFLUX数据集上验证，发射体聚类和中心检测在所有评估条件下保持高性能（发射体子集分配准确率>0.75），结构推断在高标记效率下实现可靠判别（F1约0.9）。成功实现了Nup96和DNA-Origami 3x3网格的无模板重建。

Conclusion: 该统计框架为MINFLUX单分子成像数据提供了一种强大的分析工具，能够克服现有方法的局限性，实现无模板的高精度结构推断，为生物分子结构的发现研究开辟了新途径。

Abstract: MINFLUX (Minimal Photon Flux) is a single-molecule imaging technique capable of resolving fluorophores at a precision of <5 nm. Interpretation of the point patterns generated by this technique presents challenges due to variable emitter density, incomplete bio-labelling of target molecules and their detection, error prone measurement processes, and the presence of spurious (non-structure associated) fluorescent detections. Together, these challenges ensure structural inferences from single-molecule imaging datasets are non-trivial in the absence of strong a priori information, for all but the smallest of point patterns. In addition, current methods often require subjective parameter tuning and presuppose known structural templates, limiting reference-free discovery. We present a statistically grounded, end-to-end analysis framework. Focusing on MINFLUX derived datasets and leveraging Bayesian and spatial statistical methods, a pipeline is presented that demonstrates 1) uncertainty aware clustering of measurements into emitter groups that performs better than current gold standards, 2) rapid identification of molecular structure supergroups, and 3) reconstruction of repeating structures within the dataset without substantial prior knowledge. This pipeline is demonstrated using simulated and real MINFLUX datasets, where emitter clustering and centre detection maintain high performance (emitter subset assignment accuracy > 0.75) across all conditions evaluated, while structural inference achieves reliable discrimination (F1 approx. 0.9) at high labelling efficiency. Template-free reconstruction of Nup96 and DNA-Origami 3x3 grids are achieved.

</details>


### [47] [Prognostics of Multisensor Systems with Unknown and Unlabeled Failure Modes via Bayesian Nonparametric Process Mixtures](https://arxiv.org/abs/2602.19263)
*Kani Fu,Sanduni S Disanayaka Mudiyanselage,Chunli Dai,Minhee Kim*

Main category: stat.AP

TL;DR: 提出一种贝叶斯非参数框架，结合狄利克雷过程混合模型和神经网络，用于制造系统中未知故障模式的发现与预测


<details>
  <summary>Details</summary>
Motivation: 现代制造系统常出现多种不可预测的故障行为，但现有预测模型通常假设故障模式固定且已知，这限制了数字孪生在预测性维护中的应用，特别是在高混合或自适应生产环境中，新故障模式可能出现且故障标签可能不可用。

Method: 提出贝叶斯非参数框架，将用于无监督故障模式发现的狄利克雷过程混合模块与基于神经网络的预测模块相结合。关键创新在于迭代反馈机制，两个模块相互迭代更新，动态推断、扩展或合并故障模式，同时提供高预测精度。

Result: 在仿真和飞机发动机数据集上的实验表明，该方法与现有方法相比具有竞争力或显著优势。同时展现出强大的在线适应能力，适合复杂制造环境中基于数字孪生的系统健康管理。

Conclusion: 该方法能够有效处理未知故障模式的发现与预测，为复杂制造环境中的数字孪生系统健康管理提供了鲁棒的解决方案。

Abstract: Modern manufacturing systems often experience multiple and unpredictable failure behaviors, yet most existing prognostic models assume a fixed, known set of failure modes with labeled historical data. This assumption limits the use of digital twins for predictive maintenance, especially in high-mix or adaptive production environments, where new failure modes may emerge, and the failure mode labels may be unavailable.
  To address these challenges, we propose a novel Bayesian nonparametric framework that unifies a Dirichlet process mixture module for unsupervised failure mode discovery with a neural network-based prognostic module. The key innovation lies in an iterative feedback mechanism to jointly learn two modules. These modules iteratively update one another to dynamically infer, expand, or merge failure modes as new data arrive while providing high prognostic accuracy.
  Experiments on both simulation and aircraft engine datasets show that the proposed approach performs competitively with or significantly better than existing approaches. It also exhibits robust online adaptation capabilities, making it well-suited for digital-twin-based system health management in complex manufacturing environments.

</details>


### [48] [Dynamic Elasticity Between Forest Loss and Carbon Emissions: A Subnational Panel Analysis of the United States](https://arxiv.org/abs/2602.19329)
*Keonvin Park*

Main category: stat.AP

TL;DR: 该研究通过构建美国次国家级行政单元2001-2023年的森林损失与碳排放面板数据，发现森林损失对碳排放具有显著正弹性，且长期弹性远大于短期效应，表明重复森林损失事件具有累积影响。


<details>
  <summary>Details</summary>
Motivation: 准确量化森林损失与碳排放之间的关系对于环境监测和政策评估至关重要。虽然许多研究记录了森林退化的空间模式，但对次国家级尺度上森林损失与碳排放之间动态弹性的理解有限。

Method: 基于Hansen全球森林变化数据集，构建了美国次国家级行政单元2001-2023年的年度森林损失和碳排放面板数据，应用固定效应和动态面板回归技术来分离区域内变异并考虑碳排放的时间持续性。

Result: 森林损失对碳排放具有显著的正短期弹性，且碳排放表现出强烈的时间持续性。重要的是，考虑自回归动态的估计长期弹性远大于短期效应，表明重复森林损失事件具有累积影响。

Conclusion: 研究结果强调了在评估土地覆盖变化的环境响应时建模时间动态的重要性。提出的动态弹性框架为分析环境变化过程提供了稳健且可解释的工具，可为区域监测系统和碳核算框架提供信息。

Abstract: Accurate quantification of the relationship between forest loss and associated carbon emissions is critical for both environmental monitoring and policy evaluation. Although many studies have documented spatial patterns of forest degradation, there is limited understanding of the dynamic elasticity linking tree cover loss to carbon emissions at subnational scales. In this paper, we construct a comprehensive panel dataset of annual forest loss and carbon emission estimates for U.S. subnational administrative units from 2001 to 2023, based on the Hansen Global Forest Change dataset. We apply fixed effects and dynamic panel regression techniques to isolate within-region variation and account for temporal persistence in emissions. Our results show that forest loss has a significant positive short-run elasticity with carbon emissions, and that emissions exhibit strong persistence over time. Importantly, the estimated long-run elasticity, accounting for autoregressive dynamics, is substantially larger than the short-run effect, indicating cumulative impacts of repeated forest loss events. These findings highlight the importance of modeling temporal dynamics when assessing environmental responses to land cover change. The dynamic elasticity framework proposed here offers a robust and interpretable tool for analyzing environmental change processes, and can inform both regional monitoring systems and carbon accounting frameworks.

</details>


### [49] [Network-Level Travel Time Prediction Considering The Effects of Weather and Seasonality](https://arxiv.org/abs/2602.19351)
*Yufei Ai,Yao Yu,Wenjing Pu,Lu Gao,Yihao Ren*

Main category: stat.AP

TL;DR: 该研究提出了一种使用机器学习模型预测网络级旅行时间指数（TTI）的框架，在华盛顿特区6年数据上验证，发现岭回归模型在短期和长期预测中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 准确预测旅行时间信息对出行者有帮助，但现有方法可能不够精确，需要开发能考虑天气和季节性影响的网络级预测框架。

Method: 提出机器学习框架预测网络级TTI，使用华盛顿特区6年超过50,000个TTI数据进行案例研究，评估不同机器学习模型性能，并识别天气和季节性影响。

Result: 岭回归模型在短期和长期预测中均优于其他机器学习模型，框架能有效识别天气和季节性对旅行时间的影响。

Conclusion: 机器学习框架能有效预测网络级旅行时间指数，岭回归是最佳模型选择，该研究为交通管理部门和出行者提供了实用的预测工具。

Abstract: Accurately predicting travel time information can be helpful for travelers. This study proposes a framework for predicting network-level travel time index (TTI) using machine learning models. A case study was performed on more than 50,000 TTI data collected from the Washington DC area over 6 years. The proposed approach is also able to identify the effects of weather and seasonality. The performances of the machine learning models were assessed and compared with each other. It was shown that the ridge regression model outperformed the other models in both short-term and long-term predictions.

</details>


### [50] [Reliability of stochastic capacity estimates](https://arxiv.org/abs/2602.19370)
*Igor Mikolasek*

Main category: stat.AP

TL;DR: 该论文研究交通容量估计的可靠性，发现至少需要50次记录到的交通崩溃事件，推荐临时测量需要100-200次，此时平均相对误差低于5%。


<details>
  <summary>Details</summary>
Motivation: 随机交通容量在交通建模和控制中很重要，但现有估计方法存在缺陷，且需要足够数据。由于交通崩溃事件罕见，记录到的崩溃次数决定了样本大小，这对临时交通基础设施和永久瓶颈路段的设计决策至关重要。

Method: 使用已知容量分布的合成数据，应用修正的最大似然估计器处理不同样本。总共创建360个人工测量数据来估计容量分布，然后量化与预定义分布的偏差。

Result: 结果表明至少需要50次记录到的交通崩溃事件；临时测量推荐最小100-200次。超过这个数量后，改进效果有限，预期平均相对误差低于5%。

Conclusion: 为获得可靠的交通容量估计，需要足够的崩溃事件记录，建议临时测量至少收集100-200次崩溃事件数据，以确保估计的准确性和可靠性。

Abstract: Stochastic traffic capacity is used in traffic modelling and control for unidirectional sections of road infrastructure, although some of the estimation methods have recently proved flawed. However, even sound estimation methods require sufficient data. Because breakdowns are rare, the number of recorded breakdowns effectively determines sample size. This is especially relevant for temporary traffic infrastructure, but also for permanent bottlenecks (e.g., on- and off-ramps), where practitioners must know when estimates are reliable enough for control or design decisions. This paper studies this reliability along with the impact of censored data using synthetic data with a known capacity distribution. A corrected maximum-likelihood estimator is applied to varied samples. In total, 360 artificial measurements are created and used to estimate the capacity distribution, and the deviation from the pre-defined distribution is then quantified. Results indicate that at least 50 recorded breakdowns are necessary; 100-200 are the recommended minimum for temporary measurements. Beyond this, further improvements are marginal, with the expected average relative error below 5 %.

</details>


### [51] [Real-time Win Probability and Latent Player Ability via STATS X in Team Sports](https://arxiv.org/abs/2602.19513)
*Yasutaka Shimizu,Atsushi Yamanobe*

Main category: stat.AP

TL;DR: 提出一个基于统计的实时胜率评估和球员评估框架，使用逐分钟累积数据，通过连续优势指标（T-score）和随机过程（T-process）分析比赛动态，并定义球员贡献指数STATS X。


<details>
  <summary>Details</summary>
Motivation: 为基于比分的团队运动提供统计上严谨的实时胜率评估和球员评估框架，捕捉比赛动态，为AI驱动的体育分析奠定基础。

Method: 引入连续优势指标T-score将最终比分映射到与胜负结果一致的实数值，将其表述为时间演化的随机过程T-process，由标准化累积统计数据驱动，并定义潜在贡献指数STATS X量化球员在优势区间内的参与度。

Result: 建立了能够进行顺序、解析可处理的实时胜率更新的框架，将竞争优势分解为可解释的统计成分，分离球队基础实力与比赛特定表现波动，提供结构化的评估框架。

Conclusion: 该框架为AI驱动的体育分析提供了基础，通过提供具有明确概率解释的结构化时间序列表示，支持灵活的学习机制和高维数据整合，同时保持统计一致性和可解释性。

Abstract: This study proposes a statistically grounded framework for real-time win probability evaluation and player assessment in score-based team sports, based on minute-by-minute cumulative box-score data. We introduce a continuous dominance indicator (T-score) that maps final scores to real values consistent with win/lose outcomes, and formulate it as a time-evolving stochastic representation (T-process) driven by standardized cumulative statistics. This structure captures temporal game dynamics and enables sequential, analytically tractable updates of in-game win probability. Through this stochastic formulation, competitive advantage is decomposed into interpretable statistical components. Furthermore, we define a latent contribution index, STATS X, which quantifies a player's involvement in favorable dominance intervals identified by the T-process. This allows us to separate a team's baseline strength from game-specific performance fluctuations and provides a coherent, structural evaluation framework for both teams and players. While we do not implement AI methods in this paper, our framework is positioned as a foundational step toward hybrid integration with AI. By providing a structured time-series representation of dominance with an explicit probabilistic interpretation, the framework enables flexible learning mechanisms and incorporation of high-dimensional data, while preserving statistical coherence and interpretability. This work provides a basis for advancing AI-driven sports analytics.

</details>


### [52] [Decomposing Crowd Wisdom: Domain-Specific Calibration Dynamics in Prediction Markets](https://arxiv.org/abs/2602.19520)
*Nam Anh Le*

Main category: stat.AP

TL;DR: 预测市场作为概率预测工具存在校准问题，价格不能直接作为真实概率，存在系统性偏差，特别是政治市场持续低估，且偏差模式因平台、领域、时间和交易规模而异。


<details>
  <summary>Details</summary>
Motivation: 预测市场被广泛用作概率预测工具，但其有效性取决于校准程度——即市场价格是否准确反映真实概率。目前缺乏对预测市场校准问题的系统性研究，特别是不同平台、领域和时间维度上的校准模式。

Method: 使用Kalshi和Polymarket两个平台的2.92亿笔交易、32.7万个二元合约数据，通过结构化分解方法分析校准问题。采用贝叶斯分层模型验证频率分解结果，识别校准的四个组成部分：普遍的时间效应、领域特定偏差、领域与时间交互效应、交易规模尺度效应。

Result: 1. Kalshi平台校准方差87.3%可由四个组件解释；2. 政治市场存在持续低估偏差，价格向50%压缩；3. 交易规模效应在Kalshi政治市场显著（Δ=0.53），但在Polymarket不显著（Δ=0.11）；4. 贝叶斯模型验证了频率分解结果（96.3%后验预测覆盖率）。

Conclusion: 预测市场价格不能直接作为真实概率，存在系统性校准偏差。偏差方向取决于预测内容、时间和交易者类型。消费者若将市场价格视为面值概率会系统性误解预测信息，不同平台存在特定的市场微观结构差异。

Abstract: Prediction markets are increasingly used as probability forecasting tools, yet their usefulness depends on calibration, specifically whether a contract trading at 70 cents truly implies a 70% probability. Using 292 million trades across 327,000 binary contracts on Kalshi and Polymarket, this paper shows that calibration is a structured, multidimensional phenomenon. On Kalshi, calibration decomposes into four components (a universal horizon effect, domain-specific biases, domain-by-horizon interactions and a trade-size scale effect) that together explain 87.3% of calibration variance. The dominant pattern is persistent underconfidence in political markets, where prices are chronically compressed toward 50%, and this bias generalises across both exchanges. However, the trade-size scale effect, whereby large trades are associated with amplified underconfidence in politics on Kalshi ($Δ= 0.53$, 95% confidence interval [0.29, 0.75]), does not replicate on Polymarket ($Δ= 0.11$, [-0.15, 0.39]), suggesting platform-specific microstructure. A Bayesian hierarchical model confirms the frequentist decomposition with 96.3% posterior predictive coverage. Consumers of prediction market prices who treat them as face-value probabilities will systematically misinterpret them, and the direction of misinterpretation depends on what is being predicted, when and by whom.

</details>


### [53] [Spatio-temporal modeling of urban extreme rainfall events at high resolution](https://arxiv.org/abs/2602.19774)
*Chloé Serre-Combe,Nicolas Meyer,Thomas Opitz,Gwladys Toulemonde*

Main category: stat.AP

TL;DR: 提出了一种用于高分辨率城市降雨的新型时空随机模型，结合了真实的边际行为和灵活的极值依赖结构，用于洪水风险评估。


<details>
  <summary>Details</summary>
Motivation: 降水及其时空累积建模对洪水风险评估至关重要。现有模型需要改进以准确捕捉极端降雨事件的时空结构，特别是在城市尺度上。

Method: 使用扩展广义帕累托分布(EGPD)描述降雨强度，无需阈值选择；基于空间极值理论，用r-帕累托过程建模极端事件的依赖结构，包含非可分离变异函数和事件特定的平流；采用基于联合超限的复合似然估计参数，从雷达再分析获取经验平流速度。

Result: 该模型准确再现了蒙彼利埃OMSEV网络中观测到的极端降雨时空结构，能够生成用于洪水风险评估的真实随机情景。

Conclusion: 提出的新型时空随机模型成功结合了真实的边际分布和灵活的极值依赖结构，为城市尺度洪水风险评估提供了有效的工具。

Abstract: Modeling precipitation and its accumulation over time and space is essential for flood risk assessment. We here analyze rainfall data collected over several years through a microscale precipitation sensor network in Montpellier, France, by the OMSEV observatory. A novel spatio-temporal stochastic model is proposed for high-resolution urban rainfall and combines realistic marginal behavior and flexible extremal dependence structure. Rainfall intensities are described by the Extended Generalized Pareto Distribution (EGPD), capturing both moderate and extreme events without threshold selection. Based on spatial extreme-value theory, dependence during extreme episodes is modeled by an r-Pareto process with a non-separable variogram including episode-specific advection, allowing the displacement of rainfall cells to be represented explicitly. Parameters are estimated by a composite likelihood based on joint exceedances, and empirical advection velocities are derived from radar reanalysis. The model accurately reproduces the spatio-temporal structure of extreme rainfall observed in the Montpellier OMSEV network and enables realistic stochastic scenario generation for flood risk assessment.

</details>


### [54] [A Bayesian Framework for Post-disruption Travel Time Prediction in Metro Networks](https://arxiv.org/abs/2602.19952)
*Shayan Nazemi,Aurélie Labbe,Stefan Steiner,Pratheepa Jeganathan,Martin Trépanier,Léo R. Belzile*

Main category: stat.AP

TL;DR: 本文提出贝叶斯时空模型预测地铁系统中断后的列车运行时间，考虑列车交互、车头时距不平衡和非高斯分布特性，在蒙特利尔地铁数据上验证了模型优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 交通中断后即使事故已解决，仍会导致列车运行不规则，给乘客等待和旅行时间带来巨大不确定性。准确预测中断后旅行时间对交通运营商和乘客信息系统至关重要。

Method: 开发贝叶斯时空建模框架，将旅行时间分解为延误和行程分量，采用移动平均误差结构捕捉连续列车间的依赖关系，使用偏态正态和偏态t分布灵活处理异方差性、偏度和厚尾特征。

Result: 实证结果显示中断后旅行时间存在明显的分布不对称性（随行驶距离变化）和显著的列车间误差依赖性。所提模型在点预测精度和不确定性量化方面均优于基准规范，偏态t模型对较长行程表现最稳健。

Conclusion: 预测城市轨道交通系统中断后旅行时间时，同时纳入分布灵活性和误差依赖性至关重要。偏态t模型因其对异方差性、偏度和厚尾行为的灵活处理而表现最佳。

Abstract: Disruptions are an inherent feature of transportation systems, occurring unpredictably and with varying durations. Even after an incident is reported as resolved, disruptions can induce irregular train operations that generate substantial uncertainty in passenger waiting and travel times. Accurately forecasting post-disruption travel times therefore remains a critical challenge for transit operators and passenger information systems. This paper develops a Bayesian spatiotemporal modeling framework for post-disruption train travel times that explicitly captures train interactions, headway imbalance, and non-Gaussian distributional characteristics observed during recovery periods. The proposed model decomposes travel times into delay and journey components and incorporates a moving-average error structure to represent dependence between consecutive trains. Skew-normal and skew-$t$ distributions are employed to flexibly accommodate heteroskedasticity, skewness, and heavy-tailed behavior in post-disruption travel times. The framework is evaluated using high-resolution track-occupancy and disruption log data from the Montréal metro system, covering two lines in both travel directions. Empirical results indicate that post-disruption travel times exhibit pronounced distributional asymmetries that vary with traveled distance, as well as significant error dependence across trains. The proposed models consistently outperform baseline specifications in both point prediction accuracy and uncertainty quantification, with the skew-$t$ model demonstrating the most robust performance for longer journeys. These findings underscore the importance of incorporating both distributional flexibility and error dependence when forecasting post-disruption travel times in urban rail systems.

</details>


### [55] [A Two-Step Spatio-Temporal Framework for Turbine-Height Wind Estimation at Unmonitored Sites from Sparse Meteorological Data](https://arxiv.org/abs/2602.19954)
*Eamonn Organ,Maeve Upton,Denis Allard,Lionel Benoit,James Sweeney*

Main category: stat.AP

TL;DR: 提出一个两步时空框架，仅使用稀疏气象站的开源观测数据来估计风力涡轮机轮毂高度的风速，通过高度外推和空间插值相结合的方法，提供高分辨率的风速时间序列和空间风图。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设参考高度和轮毂高度测量在同一位置，这在气象站和风电场空间分离的实际操作环境中适用性有限。需要一种仅使用稀疏气象站开源观测数据就能准确估计涡轮机高度风速的方法。

Method: 开发两步时空框架：1）使用再分析数据训练非参数广义加性模型进行垂直高度外推；2）使用空间高斯过程模型将这些轮毂高度估计值插值到风电场位置，同时明确传播高度外推阶段的不确定性。

Result: 该方法能够构建高分辨率、亚小时级的涡轮机高度风速时间序列和空间风图，提供经过校准的不确定性估计，涵盖垂直外推和空间插值误差。在爱尔兰七个运营风电场的验证中，相对于ERA5再分析数据显示出更高的准确性。

Conclusion: 提出的框架仅依赖实时开源数据，就能提供比现有再分析产品更准确的涡轮机高度风速估计，具有实际应用价值，特别适用于风电场和气象站空间分离的操作环境。

Abstract: Accurate estimates of wind speeds at wind turbine hub heights are crucial for both wind resource assessment and day-to-day management of electricity grids with high renewable penetration. In the absence of direct measurements, parametric models are commonly used to extrapolate wind speeds from observed heights to turbine heights. Recent literature has proposed extensions to allow for spatially or temporally varying vertical wind gradients, that is, the rate at which wind speed changes with height. However, these approaches typically assume that reference height and hub height measurements are available at the same locations, which limits their applicability in operational settings where meteorological stations and wind farms are spatially separated. In this paper, we develop a two-step spatio-temporal framework to estimate turbine height wind speeds using only open-access observations from sparse meteorological stations. First, a non-parametric generalized additive model is trained on reanalysis data to perform vertical height extrapolation. Second, a spatial Gaussian process model interpolates these hub-height estimates to wind farm locations while explicitly propagating uncertainty from the height extrapolation stage. The proposed framework enables the construction of high-resolution, sub-hourly turbine-height wind speed time series and spatial wind maps using data available in real time, capabilities not provided by existing reanalysis products. We further provide calibrated uncertainty estimates that account for both vertical extrapolation and spatial interpolation errors. The approach is validated using hub-height measurements from seven operational wind farms in Ireland, demonstrating improved accuracy relative to ERA5 reanalysis while relying solely on real-time, open-access data.

</details>
