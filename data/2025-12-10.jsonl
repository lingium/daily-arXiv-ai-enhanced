{"id": "2512.06116", "categories": ["stat.AP", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.06116", "abs": "https://arxiv.org/abs/2512.06116", "authors": ["Y. Park", "F. Wu", "X. Feng", "S. Yang", "E. H. Wang", "B. Yao", "C. Moon", "G. Xiao", "Q. Li"], "title": "Spatial Analysis for AI-segmented Histopathology Images: Methods and Implementation", "comment": "33 pages, 2 figures", "summary": "Quantitatively characterizing the spatial organization of cells and their interaction is essential for understanding cancer progression and immune response. Recent advances in machine intelligence have enabled large-scale segmentation and classification of cell nuclei from digitized histopathology slides, generating massive point pattern and marked point pattern datasets. However, accessible tools for quantitative analysis of such complex cellular spatial organization remain limited. In this paper, we first review 27 traditional spatial summary statistics, areal indices, and topological features applicable to point pattern data. Then, we introduce SASHIMI (Spatial Analysis for Segmented Histopathology Images using Machine Intelligence), a browser-based tool for real-time spatial analysis of artificial intelligence (AI)-segmented histopathology images. SASHIMI computes a comprehensive suite of mathematically grounded descriptors, including spatial statistics, proximity-based measures, grid-level similarity indices, spatial autocorrelation measures, and topological descriptors, to quantify cellular abundance and cell-cell interaction. Applied to two cancer datasets, oral potentially malignant disorders (OPMD) and non-small-cell lung cancer (NSCLC), SASHIMI identified multiple spatial features significantly associated with patient survival outcomes. SASHIMI provides an accessible and reproducible platform for single-cell-level spatial profiling of tumor morphological architecture, offering a robust framework for quantitative exploration of tissue organization across cancer types."}
{"id": "2512.06127", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.06127", "abs": "https://arxiv.org/abs/2512.06127", "authors": ["Nancy Kasamala", "Arthur Mukwaya", "Nana Kankam Gyimah", "Judith Mwakalonge", "Gurcan Comert", "Saidi Siuhi", "Akinbobola Jegede"], "title": "Mode Choice Heterogeneity Among Zero-Vehicle Households: A Latent Class Cluster Approach", "comment": null, "summary": "In transportation planning, Zero-Vehicle Households (ZVHs) are often treated as a uniform group with limited mobility options and assumed to rely heavily on walking or public transit. However, such assumptions overlook the diverse travel strategies ZVHs employ in response to varying trip needs and sociodemographic factors. This study addresses this gap by applying a weighted Latent Class Cluster Analysis (LCCA) to data from the 2022 National Household Travel Survey (NHTS) to uncover distinct mobility patterns within the ZVH population. Using travel mode and trip purpose as indicators and demographic, economic, and built environment variables as covariates, we identified three latent classes :Shared mobility errand workers (36.3%), who primarily use transit and ridehailing for commuting and essential activities; car based shoppers (29.9%), who depend on informal vehicle access for longer discretionary trips and active travel Shoppers (33.8%), who rely on walking or cycling for short, local shopping oriented travel. These behavioral findings enable policymakers to develop differentiated planning solutions to the specific needs of each segment among the ZVHs population across varied geographic and demographic settings."}
{"id": "2512.06210", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06210", "abs": "https://arxiv.org/abs/2512.06210", "authors": ["Daniel Mittermaier", "Tobias Bohne", "Martin Hofer", "Daniel Racek"], "title": "Forests of Uncertaint(r)ees: Using tree-based ensembles to estimate probability distributions of future conflict", "comment": "18 pages, 4 figures, 3 tables. Replication code available at https://github.com/ccew-unibw/uncertaintrees", "summary": "Predictions of fatalities from violent conflict on the PRIO-GRID-month (pgm) level are characterized by high levels of uncertainty, limiting their usefulness in practical applications. We discuss the two main sources of uncertainty for this prediction task, the nature of violent conflict and data limitations, embedding this in the wider literature on uncertainty quantification in machine learning. We develop a strategy to quantify uncertainty in conflict forecasting, shifting from traditional point predictions to full predictive distributions. Our approach compares and combines multiple tree-based classifiers and distributional regressors in a custom auto-ML setup, estimating distributions for each pgm individually. We also test the integration of regional models in spatial ensembles as a potential avenue to reduce uncertainty. The models are able to consistently outperform a suite of benchmarks derived from conflict history in predictions up to one year in advance, with performance driven by regions where conflict was observed. With our evaluation, we emphasize the need to understand how a metric behaves for a given prediction problem, in our case characterized by extremely high zero-inflatedness. While not resulting in better predictions, the integration of smaller models does not decrease performance for this prediction task, opening avenues to integrate data sources with less spatial coverage in the future."}
{"id": "2512.06450", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.06450", "abs": "https://arxiv.org/abs/2512.06450", "authors": ["Mauli Pant", "Linda Fernandez", "Indranil Sahoo"], "title": "Spatio-temporal Shared-Field Modeling of Beluga and Bowhead Whale Sightings Using a Joint Marked Log-Gaussian Cox Process", "comment": null, "summary": "We analyze a decade of aerial survey whale sighting data (2010-2019) to model the spatio-temporal distributions and group sizes of beluga (Delphinapterus leucas) and bowhead (Balaena mysticetus) whales in the United States Arctic. To jointly model these species, we develop a multi-species Log-Gaussian Cox Process (LGCP) in which species specific intensity surfaces are linked through a shared latent spatial Gaussian field. This structure allows the model to capture broad spatial patterns common to both species while still accommodating species level responses to environmental covariates and seasonal variation. The latent field is represented using the Stochastic Partial Differential Equation (SPDE) approach with an anisotropic Matern covariance, implemented on an ocean constrained triangulated mesh so that spatial dependence aligns with marine geography. Whale group size is incorporated through a marked point process extension with species specific negative binomial marks, allowing occurrence and group sizes to be jointly analyzed within a unified framework. Inference is carried out using the Integrated Nested Laplace Approximation (INLA), enabling efficient model fitting over a decade of survey effort. The results highlight persistent multi-species hotspots and distinct environmental associations for each species, demonstrating the value of shared field LGCPs for joint species distribution modeling in data sparse and heterogeneous survey settings."}
{"id": "2512.06522", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.06522", "abs": "https://arxiv.org/abs/2512.06522", "authors": ["Di Wu", "Jacob Bien", "Snigdha Panigrahi"], "title": "Hierarchical Clustering With Confidence", "comment": "44 Pages, 9 Figures, 2 Algorithms", "summary": "Agglomerative hierarchical clustering is one of the most widely used approaches for exploring how observations in a dataset relate to each other. However, its greedy nature makes it highly sensitive to small perturbations in the data, often producing different clustering results and making it difficult to separate genuine structure from spurious patterns. In this paper, we show how randomizing hierarchical clustering can be useful not just for measuring stability but also for designing valid hypothesis testing procedures based on the clustering results.\n  We propose a simple randomization scheme together with a method for constructing a valid p-value at each node of the hierarchical clustering dendrogram that quantifies evidence against performing the greedy merge. Our test controls the Type I error rate, works with any hierarchical linkage without case-specific derivations, and simulations show it is substantially more powerful than existing selective inference approaches. To demonstrate the practical utility of our p-values, we develop an adaptive $α$-spending procedure that estimates the number of clusters, with a probabilistic guarantee on overestimation. Experiments on simulated and real data show that this estimate yields powerful clustering and can be used, for example, to assess clustering stability across multiple runs of the randomized algorithm."}
{"id": "2512.06270", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06270", "abs": "https://arxiv.org/abs/2512.06270", "authors": ["Nifei Lin", "Heng Luo", "L. Jeff Hong"], "title": "Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions", "comment": null, "summary": "In this work, we study contextual strongly convex simulation optimization and adopt an \"optimize then predict\" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $Γ$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $Γ^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach."}
{"id": "2512.06314", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.06314", "abs": "https://arxiv.org/abs/2512.06314", "authors": ["Shenghao Qin", "Bowen Gang", "Tiejun Tong", "Hengjian Cui"], "title": "The Bag-and-Whisker Plot: A New Bagplot for Bivariate Data", "comment": null, "summary": "The bagplot, also known as the \"bag-and-bolster plot\", is a notable extension of the boxplot from univariate to bivariate data. Although widely used, its practical application is hindered by two key limitations: the fixed inflation factor for outlier detection that does not adapt to the sample size, and the unstable convex hull used to visualize its fence. In this paper, we propose a new bagplot, namely the \"bag-and-whisker plot'', as an improvement method to address these limitations. Our framework recasts outlier detection as a multiple testing problem, yielding a data-adaptive fence that controls statistical error rates and enhances the reliability of outlier identification. To further resolve graphical instability, we introduce a refined visualization that abandons the convex hull (the bolster) with a direct rendering of the statistical fence, complemented by granular whiskers that effectively illustrate the data's spread. Extensive simulations and real-world data analyses demonstrate that our new bagplot exhibits superior adaptivity and robustness compared to the existing standard, and thus can be highly recommended for practical use."}
{"id": "2512.07080", "categories": ["stat.AP", "q-bio.QM", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.07080", "abs": "https://arxiv.org/abs/2512.07080", "authors": ["Madison D. Griffin", "Grace S. Chiu", "Roger L. Mann", "Melissa J. Southworth", "John K. Thomas"], "title": "Big shells, bigger data: cohort analysis of Chesapeake Bay Crassostrea virginica reefs", "comment": "29 pages, 7 figures", "summary": "Oysters in Virginia Chesapeake Bay oyster reefs are \"age-truncated\", possibly due to a combination of historical overfishing, disease epizootics, environmental degradation, and climate change. Research has suggested that oysters exhibit resilience to environmental stressors; however, that evidence is based on the current limited understanding of oyster lifespan. Until this paper, the Virginia Oyster Stock Assessment and Replenishment Archive (VOSARA), a spatially and temporally expansive dataset (222 reefs across 2003-2023) of shell lengths (SL, mm), had yet to be examined comprehensively in the context of resilience. We develop a novel method using Gaussian mixture modeling (GMM) to identify the age groups in each reef using yearly SL data and then link those age groups over time to identify cohorts and estimate their lifespan. Sixty-four reefs (29%) are deemed to have sufficient data (at least 300 oysters sampled for a minimum of 8 consecutive years) for this analysis. We fit univariate GMMs for each year ($t$) and reef ($r$) for each of the seven river strata ($R$) to estimate 1) the mean and standard deviation of SL for each $a_{Rrt}$th age group, and 2) the mixture percentage of each $a_{Rrt}$th age group. We link age groups across time to infer age cohorts by developing a mechanistic algorithm that prevents the shrinking of shell length when an $a_{Rrt}$th group becomes an ($a_{R,r,t+1}$)th group. Our method shows promise in identifying oyster cohorts and estimating lifespan solely using SL data. Our results show signals of resiliency in almost all river systems: oyster cohorts live longer and grow larger in the mid-to-late 2010s compared to the early 2000s."}
{"id": "2512.06777", "categories": ["stat.OT"], "pdf": "https://arxiv.org/pdf/2512.06777", "abs": "https://arxiv.org/abs/2512.06777", "authors": ["Tommaso Costa"], "title": "Evidence and Elimination: A Bayesian Interpretation of Falsification in Scientific Practice", "comment": null, "summary": "The classical conception of falsification presents scientific theories as entities that are decisively refuted when their predictions fail. This picture has long been challenged by both philosophical analysis and scientific practice, yet the relationship between Popper's eliminative view of theory testing and Bayesian model comparison remains insufficiently articulated. This paper develops a unified account in which falsification is reinterpreted as a Bayesian process of model elimination. A theory is not rejected because it contradicts an observation in a logical sense; it is eliminated because it assigns vanishing integrated probability to the data in comparison with an alternative model. This reinterpretation resolves the difficulties raised by the Duhem-Quine thesis, clarifies the status of auxiliary hypotheses, and explains why ad hoc modifications reduce rather than increase theoretical credibility. The analysis is illustrated through two classical episodes in celestial mechanics, the discovery of Neptune and the anomalous precession of Mercury. In the Neptune case, an auxiliary hypothesis internal to Newtonian gravity dramatically increases the marginal likelihood of the theory, preserving it from apparent refutation. In the Mercury case, no permissible auxiliary modification can rescue the Newtonian model, while general relativity assigns high probability to the anomaly without adjustable parameters. The resulting posterior collapse provides a quantitative realisation of Popper's eliminative criterion. Bayesian model comparison therefore supplies the mathematical structure that Popper's philosophy lacked and offers a coherent account of scientific theory change as a process of successive eliminations within a space of competing models."}
{"id": "2512.06553", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06553", "abs": "https://arxiv.org/abs/2512.06553", "authors": ["Peiyao Cai", "Chengyu Cui", "Felipe Maia Polo", "Seamus Somerstep", "Leshem Choshen", "Mikhail Yurochkin", "Moulinath Banerjee", "Yuekai Sun", "Kean Ming Tan", "Gongjun Xu"], "title": "A Latent Variable Framework for Scaling Laws in Large Language Models", "comment": null, "summary": "We propose a statistical framework built on latent variable modeling for scaling laws of large language models (LLMs). Our work is motivated by the rapid emergence of numerous new LLM families with distinct architectures and training strategies, evaluated on an increasing number of benchmarks. This heterogeneity makes a single global scaling curve inadequate for capturing how performance varies across families and benchmarks. To address this, we propose a latent variable modeling framework in which each LLM family is associated with a latent variable that captures the common underlying features in that family. An LLM's performance on different benchmarks is then driven by its latent skills, which are jointly determined by the latent variable and the model's own observable features. We develop an estimation procedure for this latent variable model and establish its statistical properties. We also design efficient numerical algorithms that support estimation and various downstream tasks. Empirically, we evaluate the approach on 12 widely used benchmarks from the Open LLM Leaderboard (v1/v2)."}
{"id": "2512.06956", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.06956", "abs": "https://arxiv.org/abs/2512.06956", "authors": ["Denis Belomestny", "Alexey Naumov", "Sergey Samsonov"], "title": "Statistical analysis of Inverse Entropy-regularized Reinforcement Learning", "comment": "27 pages", "summary": "Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $π^\\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory."}
{"id": "2512.06348", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.06348", "abs": "https://arxiv.org/abs/2512.06348", "authors": ["Xiaoyu Ma", "Likun Zhang", "Christopher K. Wikle"], "title": "Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders", "comment": null, "summary": "Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Niño/Southern Oscillation (ENSO) index."}
{"id": "2512.06412", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.06412", "abs": "https://arxiv.org/abs/2512.06412", "authors": ["Ying Niu", "Zhao Chen", "Christina Dan Wang", "Yuwei Zhao"], "title": "Goodness-of-fit Tests for Heavy-tailed Random Fields", "comment": null, "summary": "We develop goodness-of-fit tests for max-stable random fields, which are used to model heavy-tailed spatial data. The test statistics are constructed based on the Fourier transforms of the indicators of extreme values in the heavy-tailed spatial data, whose asymptotic distribution is a Gaussian random field under a hypothesized max-stable random field. Since the covariance structure of the limiting Gaussian random field lacks an explicit expression, we propose a stationary bootstrap procedure for spatial fields to approximate critical values. Simulation studies confirm the theoretical distributional results, and applications to PM2.5 and temperature data illustrate the practical utility of the proposed method for model assessment."}
{"id": "2512.06654", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.06654", "abs": "https://arxiv.org/abs/2512.06654", "authors": ["Zhaojin Nan", "Ran Chen"], "title": "Disentangling the Mediation Pathways of Depression in Asian Students and Workers", "comment": "19 pages", "summary": "Depression is a major global mental health issue shaped by cultural, demographic, and occupational factors. This study compares predictors of depression across student and worker populations using datasets from India, Malaysia, and China. The India dataset was split into student and worker groups, while the Malaysia dataset includes only students and the China (CHARLS) dataset includes only workers. After harmonizing variables, we applied logistic regression, random forest, and causal forest models to identify key predictors and subgroup-specific effects, and conducted causal mediation analysis (CMA) to assess whether variables operate through intermediaries such as perceived pressure. Among students, pressure, age, workload, financial stress, mental health history, and satisfaction were significant predictors; similar factors emerged for workers. Notably, age showed opposite effects across groups: younger students were more likely to experience depression, whereas older workers showed higher risk. Model performance showed moderate internal accuracy but weaker external generalizability across countries, with random forest outperforming logistic regression. Causal forest results indicated limited heterogeneity in the effect of pressure, while CMA showed that pressure does not mediate the effect of age but operates more directly, and satisfaction influences depression partly through pressure. Overall, pressure consistently emerged as the strongest predictor, suggesting that interventions targeting academic and occupational stress may help reduce depressive symptoms."}
{"id": "2512.06435", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06435", "abs": "https://arxiv.org/abs/2512.06435", "authors": ["Mara Sherlin Talento", "Jordan Richards", "Raphael Huser", "Hernando Ombao"], "title": "Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals", "comment": null, "summary": "We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without."}
{"id": "2512.06428", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.06428", "abs": "https://arxiv.org/abs/2512.06428", "authors": ["Yuwen Wang", "Shiwen Ye", "Jingnan Zhang", "Junhui Wang"], "title": "Community detection in heterogeneous signed networks", "comment": null, "summary": "Network data has attracted growing interest across scientific domains, prompting the development of various network models. Existing network analysis methods mainly focus on unsigned networks, whereas signed networks, consisting of both positive and negative edges, have been frequently encountered in practice but much less investigated. In this paper, we formally define strong and weak balance in signed networks, and propose a signed block $β$-model, which is capable of modeling strong- and weak-balanced signed networks simultaneously. We establish the identifiability of the proposed model by leveraging properties of bipartite graphs, and develop an efficient alternating updating algorithm to optimize the resulting log-likelihood function. More importantly, we establish the asymptotic consistencies of the proposed model in terms of both probability estimation and community detection. Its advantages are also demonstrated through extensive numerical experiments and the application to a real-world international relationship network."}
{"id": "2512.06682", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.06682", "abs": "https://arxiv.org/abs/2512.06682", "authors": ["Boyang Xu", "Yunyi Kang", "Xinyu Zhao", "Hao Yan", "Feng Ju"], "title": "Partially Observable Markov Decision Process Framework for Operating Condition Optimization Using Real-Time Degradation Signals", "comment": "Accepted for publication in Journal of Quality Technology", "summary": "In many engineering systems, proper predictive maintenance and operational control are essential to increase efficiency and reliability while reducing maintenance costs. However, one of the major challenges is that many sensors are used for system monitoring. Analyzing these sensors simultaneously for better predictive maintenance optimization is often very challenging. In this paper, we propose a systematic decision-making framework to improve the system performance in manufacturing practice, considering the real-time degradation signals generated by multiple sensors. Specifically, we propose a partially observed Markov decision process (POMDP) model to generate the optimal capacity and predictive maintenance policies, given the fact that the observation of the system state is imperfect. Such work provides a systematic approach that focuses on jointly controlling the operating conditions and preventive maintenance utilizing the real-time machine deterioration signals by incorporating the degradation constraint and non-observable states. We apply this technique to the bearing degradation data and NASA aircraft turbofan engine dataset, demonstrating the effectiveness of the proposed method."}
{"id": "2512.06615", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06615", "abs": "https://arxiv.org/abs/2512.06615", "authors": ["Kaichen Shen", "Wei Zhu"], "title": "Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions", "comment": null, "summary": "We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability."}
{"id": "2512.06514", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.06514", "abs": "https://arxiv.org/abs/2512.06514", "authors": ["Jie Wu", "Bo Zhang", "Daoji Li", "Zemin Zheng"], "title": "Simultaneous Heterogeneity and Reduced-rank Learning for Multivariate Response Regression", "comment": null, "summary": "Heterogeneous data are now ubiquitous in many applications in which correctly identifying the subgroups from a heterogeneous population is critical. Although there is an increasing body of literature on subgroup detection, existing methods mainly focus on the univariate response setting. In this paper, we propose a joint heterogeneity and reduced-rank learning framework to simultaneously identify the subgroup structure and estimate the covariate effects for heterogeneous multivariate response regression. In particular, our approach uses rank-constrained pairwise fusion penalization and conducts the subgroup analysis without requiring prior knowledge regarding the individual subgroup memberships. We implement the proposed approach by an alternating direction method of multipliers (ADMM) algorithm and show its convergence. We also establish the asymptotic properties for the resulting estimators under mild and interpretable conditions. A predictive information criterion is proposed to select the rank of the coefficient matrix with theoretical support. The effectiveness of the proposed approach is demonstrated through simulation studies and a real data application."}
{"id": "2512.07074", "categories": ["stat.AP", "hep-ex", "hep-ph", "physics.data-an", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.07074", "abs": "https://arxiv.org/abs/2512.07074", "authors": ["Huanbiao Zhu", "Krish Desai", "Mikael Kuusela", "Vinicius Mikuni", "Benjamin Nachman", "Larry Wasserman"], "title": "Machine Learning-based Unfolding for Cross Section Measurements in the Presence of Nuisance Parameters", "comment": null, "summary": "Statistically correcting measured cross sections for detector effects is an important step across many applications. In particle physics, this inverse problem is known as \\textit{unfolding}. In cases with complex instruments, the distortions they introduce are often known only implicitly through simulations of the detector. Modern machine learning has enabled efficient simulation-based approaches for unfolding high-dimensional data. Among these, one of the first methods successfully deployed on experimental data is the \\textsc{OmniFold} algorithm, a classifier-based Expectation-Maximization procedure. In practice, however, the forward model is only approximately specified, and the corresponding uncertainty is encoded through nuisance parameters. Building on the well-studied \\textsc{OmniFold} algorithm, we show how to extend machine learning-based unfolding to incorporate nuisance parameters. Our new algorithm, called Profile \\textsc{OmniFold}, is demonstrated using a Gaussian example as well as a particle physics case study using simulated data from the CMS Experiment at the Large Hadron Collider."}
{"id": "2512.06795", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06795", "abs": "https://arxiv.org/abs/2512.06795", "authors": ["Gyu Yeol Kim", "Min-hwan Oh"], "title": "ADAM Optimization with Adaptive Batch Selection", "comment": "Published at ICLR 2025", "summary": "Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods."}
{"id": "2512.06522", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.06522", "abs": "https://arxiv.org/abs/2512.06522", "authors": ["Di Wu", "Jacob Bien", "Snigdha Panigrahi"], "title": "Hierarchical Clustering With Confidence", "comment": "44 Pages, 9 Figures, 2 Algorithms", "summary": "Agglomerative hierarchical clustering is one of the most widely used approaches for exploring how observations in a dataset relate to each other. However, its greedy nature makes it highly sensitive to small perturbations in the data, often producing different clustering results and making it difficult to separate genuine structure from spurious patterns. In this paper, we show how randomizing hierarchical clustering can be useful not just for measuring stability but also for designing valid hypothesis testing procedures based on the clustering results.\n  We propose a simple randomization scheme together with a method for constructing a valid p-value at each node of the hierarchical clustering dendrogram that quantifies evidence against performing the greedy merge. Our test controls the Type I error rate, works with any hierarchical linkage without case-specific derivations, and simulations show it is substantially more powerful than existing selective inference approaches. To demonstrate the practical utility of our p-values, we develop an adaptive $α$-spending procedure that estimates the number of clusters, with a probabilistic guarantee on overestimation. Experiments on simulated and real data show that this estimate yields powerful clustering and can be used, for example, to assess clustering stability across multiple runs of the randomized algorithm."}
{"id": "2512.07080", "categories": ["stat.AP", "q-bio.QM", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.07080", "abs": "https://arxiv.org/abs/2512.07080", "authors": ["Madison D. Griffin", "Grace S. Chiu", "Roger L. Mann", "Melissa J. Southworth", "John K. Thomas"], "title": "Big shells, bigger data: cohort analysis of Chesapeake Bay Crassostrea virginica reefs", "comment": "29 pages, 7 figures", "summary": "Oysters in Virginia Chesapeake Bay oyster reefs are \"age-truncated\", possibly due to a combination of historical overfishing, disease epizootics, environmental degradation, and climate change. Research has suggested that oysters exhibit resilience to environmental stressors; however, that evidence is based on the current limited understanding of oyster lifespan. Until this paper, the Virginia Oyster Stock Assessment and Replenishment Archive (VOSARA), a spatially and temporally expansive dataset (222 reefs across 2003-2023) of shell lengths (SL, mm), had yet to be examined comprehensively in the context of resilience. We develop a novel method using Gaussian mixture modeling (GMM) to identify the age groups in each reef using yearly SL data and then link those age groups over time to identify cohorts and estimate their lifespan. Sixty-four reefs (29%) are deemed to have sufficient data (at least 300 oysters sampled for a minimum of 8 consecutive years) for this analysis. We fit univariate GMMs for each year ($t$) and reef ($r$) for each of the seven river strata ($R$) to estimate 1) the mean and standard deviation of SL for each $a_{Rrt}$th age group, and 2) the mixture percentage of each $a_{Rrt}$th age group. We link age groups across time to infer age cohorts by developing a mechanistic algorithm that prevents the shrinking of shell length when an $a_{Rrt}$th group becomes an ($a_{R,r,t+1}$)th group. Our method shows promise in identifying oyster cohorts and estimating lifespan solely using SL data. Our results show signals of resiliency in almost all river systems: oyster cohorts live longer and grow larger in the mid-to-late 2010s compared to the early 2000s."}
{"id": "2512.06945", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06945", "abs": "https://arxiv.org/abs/2512.06945", "authors": ["Nabil Alami", "Jad Zakharia", "Souhaib Ben Taieb"], "title": "Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets", "comment": null, "summary": "Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines."}
{"id": "2512.06601", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.06601", "abs": "https://arxiv.org/abs/2512.06601", "authors": ["Mengqi Lin", "Colin Fogarty"], "title": "Controlling the False Discovery Proportion in Matched Observational Studies", "comment": null, "summary": "We provide an approach to exploratory data analysis in matched observational studies with a single intervention and multiple endpoints. In such settings, the researcher would like to explore evidence for actual treatment effects among these variables while accounting not only for the possibility of false discoveries, but also for the potential impact of unmeasured confounding. For any candidate subset of hypotheses about these outcomes, we provide sensitivity sets for the proportion of the hypotheses within the subset which are actually true. The resulting sensitivity statements are valid simultaneously over all possible choices for the rejected set, allowing the researcher to search for promising subsets of hypotheses that maintain a large estimated fraction of true discoveries even if hidden bias is present. The approach is well suited to sensitivity analysis, as conclusions that some fraction of outcomes are affected by the treatment exhibit larger robustness to unmeasured confounding than findings that any particular outcome is affected. We show how a sequence of integer programs, in tandem with screening steps, facilitate the efficient computation of the required sensitivity sets. We illustrate the practical utility of our method through both simulation studies and a data example on the long-term impacts of childhood abuse."}
{"id": "2512.07185", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.07185", "abs": "https://arxiv.org/abs/2512.07185", "authors": ["Jaka Nugraha", "Noyyn Sun", "Xinlin Zhao", "Vindi Kusuma Wardani", "Inna Koblianska", "Jiunn-Woei Lian"], "title": "Facilitating Conditions as an Enabler, Not a Direct Motivator: A Robustness and Mediation Analysis of E-Learning Adoption", "comment": null, "summary": "Despite substantial institutional investment in e-learning infrastructure, student engagement often fails to meet expectations--a persistent paradox that challenges the established direct relationship between Facilitating Conditions (FC) and behavioral intention within the classic UTAUT framework. To resolve this theoretical puzzle, we reconceptualized the role of FC through an empirical study of 470 Indonesian university students. Our robust, multi-stage analytical approach first confirmed the significant influence of established drivers--Performance Expectancy (beta=0.190), Effort Expectancy (beta=0.198), Social Influence (beta=0.151), and Perceived Enjoyment (beta=0.472)--on Behavioral Intention (BI), which in turn strongly predicted Use Behavior (beta=0.666). Crucially, however, the direct effect of FC on BI proved non-significant (beta=-0.085). A subsequent mediation model revealed FC's true function as a foundational enabling construct that operates indirectly by powerfully enhancing both Performance Expectancy (beta=0.556) and Effort Expectancy (beta=0.419). Our findings demonstrate that the value of technological infrastructure lies not in its mere presence, but in its dynamic capacity to enable learning and optimize user experience. This research advances a refined \"enabling pathway\" theoretical framework, guiding administrators to shift the focus of technological investment from merely providing tools to strategically crafting learning experiences."}
{"id": "2512.06950", "categories": ["stat.ML", "cs.LG", "physics.space-ph"], "pdf": "https://arxiv.org/pdf/2512.06950", "abs": "https://arxiv.org/abs/2512.06950", "authors": ["Enrico Camporeale"], "title": "PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios", "comment": null, "summary": "The challenge of \\textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity.\n  We introduce \\textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \\emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \\textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \\emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples.\n  We use a real-world space weather example, where PARIS reduces the training set by up to 75\\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression."}
{"id": "2512.06621", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.06621", "abs": "https://arxiv.org/abs/2512.06621", "authors": ["Yongqiang Tang"], "title": "Monotone data augmentation algorithm for longitudinal continuous, binary and ordinal outcomes: a unifying approach", "comment": "4 tables", "summary": "The monotone data augmentation (MDA) algorithm has been widely used to impute missing data for longitudinal continuous outcomes. Compared to a full data augmentation approach, the MDA scheme accelerates the mixing of the Markov chain, reduces computational costs per iteration, and aids in missing data imputation under nonignorable dropouts. We extend the MDA algorithm to the multivariate probit (MVP) model for longitudinal binary and ordinal outcomes. The MVP model assumes the categorical outcomes are discretized versions of underlying longitudinal latent Gaussian outcomes modeled by a mixed effects model for repeated measures. A parameter expansion strategy is employed to facilitate the posterior sampling, and expedite the convergence of the Markov chain in MVP. The method enables the sampling of the regression coefficients and covariance matrix for longitudinal continuous, binary and ordinal outcomes in a unified manner. This property aids in understanding the algorithm and developing computer codes for MVP. We also introduce independent Metropolis-Hasting samplers to handle complex priors, and evaluate how the choice between flat and diffuse normal priors for regression coefficients influences parameter estimation and missing data imputation. Numerical examples are used to illustrate the methodology."}
{"id": "2512.07429", "categories": ["stat.AP", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2512.07429", "abs": "https://arxiv.org/abs/2512.07429", "authors": ["Nina Effenberger", "Maxim Samarin", "Maybritt Schillinger", "Reto Knutti"], "title": "Bridging CORDEX and CMIP6: Machine Learning Downscaling for Wind and Solar Energy Droughts in Central Europe", "comment": null, "summary": "Reliable regional climate information is essential for assessing the impacts of climate change and for planning in sectors such as renewable energy; yet, producing high-resolution projections through coordinated initiatives like CORDEX that run multiple physical regional climate models is both computationally demanding and difficult to organize. Machine learning emulators that learn the mapping between global and regional climate fields offer a promising way to address these limitations. Here we introduce the application of such an emulator: trained on CMIP5 and CORDEX simulations, it reproduces regional climate model data with sufficient accuracy. When applied to CMIP6 simulations not seen during training, it also produces realistic results, indicating stable performance. Using CORDEX data, CMIP5 and CMIP6 simulations, as well as regional data generated by two machine learning models, we analyze the co-occurrence of low wind speed and low solar radiation and find indications that the number of such energy drought days is likely to decrease in the future. Our results highlight that downscaling with machine learning emulators provides an efficient complement to efforts such as CORDEX, supplying the higher-resolution information required for impact assessments."}
{"id": "2512.06956", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.06956", "abs": "https://arxiv.org/abs/2512.06956", "authors": ["Denis Belomestny", "Alexey Naumov", "Sergey Samsonov"], "title": "Statistical analysis of Inverse Entropy-regularized Reinforcement Learning", "comment": "27 pages", "summary": "Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $π^\\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory."}
{"id": "2512.06872", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.06872", "abs": "https://arxiv.org/abs/2512.06872", "authors": ["Yingying Fan", "Zihan Wang", "Waverly Wei"], "title": "SLOACI: Surrogate-Leveraged Online Adaptive Causal Inference", "comment": null, "summary": "Adaptive experimental designs have gained increasing attention across a range of domains. In this paper, we propose a new methodological framework, surrogate-leveraged online adaptive causal inference (SLOACI), which integrates predictive surrogate outcomes into adaptive designs to enhance efficiency. For downstream analysis, we construct the adaptive augmented inverse probability weighting estimator for the average treatment effect using collected data. Our procedure remains robust even when surrogates are noisy or weak. We provide a comprehensive theoretical foundation for SLOACI. Under the asymptotic regime, we show that the proposed estimator attains the semiparametric efficiency bound. From a non-asymptotic perspective, we derive a regret bound to provide practical insights. We also develop a toolbox of sequential testing procedures that accommodates both asymptotic and non-asymptotic regimes, allowing experimenters to choose the perspective that best aligns with their practical needs. Extensive simulations and a synthetic case study are conducted to showcase the superior finite-sample performance of our method."}
{"id": "2512.07467", "categories": ["stat.AP", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.07467", "abs": "https://arxiv.org/abs/2512.07467", "authors": ["Ben Moews"], "title": "Permanent and transitory crime risk in variable-density hot spot analysis", "comment": "26 pages, 4 figures, 3 tables", "summary": "Crime prevention measures, aiming for the effective and efficient spending of public resources, rely on the empirical analysis of spatial and temporal data for public safety outcomes. We perform a variable-density cluster analysis on crime incident reports in the City of Chicago for the years 2001--2022 to investigate changes in crime share composition for hot spots of different densities. Contributing to and going beyond the existing wealth of research on criminological applications in the operational research literature, we study the evolution of crime type shares in clusters over the course of two decades and demonstrate particularly notable impacts of the COVID-19 pandemic and its associated social contact avoidance measures, as well as a dependence of these effects on the primary function of city areas. Our results also indicate differences in the relative difficulty to address specific crime types, and an analysis of spatial autocorrelations further shows variations in incident uniformity between clusters and outlier areas at different distance radii. We discuss our findings in the context of the interplay between operational research and criminal justice, the practice of hot spot policing and public safety optimization, and the factors contributing to, and challenges and risks due to, data biases as an often neglected factor in criminological applications."}
{"id": "2512.06960", "categories": ["stat.ML", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.06960", "abs": "https://arxiv.org/abs/2512.06960", "authors": ["Jitendra K Tugnait"], "title": "Learning Conditional Independence Differential Graphs From Time-Dependent Data", "comment": "20 pages, 4 figures, 2 tables. To be published in IEEE Access, 2025", "summary": "Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric."}
{"id": "2512.07019", "categories": ["stat.ME", "cs.AI", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.07019", "abs": "https://arxiv.org/abs/2512.07019", "authors": ["Zhiyu Xu", "Jia Liu", "Yixin Wang", "Yuqi Gu"], "title": "Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model."}
{"id": "2512.07531", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.07531", "abs": "https://arxiv.org/abs/2512.07531", "authors": ["Natalia Ortega", "Peter WG Tennant", "Darren C Greenwood", "Octavio Pano", "Christina C Dahm", "Russell J de Souza", "Daniel B Ibsen", "Conor J MacDonald", "Deirdre K Tobias", "Georgia D Tomova"], "title": "Meta-analyses of dietary exposures must consider energy adjustment: recommendations from a meta-scientific review", "comment": null, "summary": "In observational studies of dietary exposures, the energy adjustment strategy has a critical impact on the effect being estimated. Adjusting for total energy intake or expressing the exposure as a percentage of total energy, leads to a substitution effect being estimated. This impacts the interpretation of primary studies and meta-analyses. Unless energy adjustment strategies are considered, meta-analyses may end up pooling estimates for incomparable effects. This meta-scientific review aimed to investigate the extent to which meta-analyses of dietary exposures may be pooling incomparable effects by reviewing the energy adjustment strategies. We identified all meta-analyses examining the relationship between saturated fat and fish and cardiovascular disease. The two most recent and two most cited reviews for each exposure were examined, along with all primary studies. Information on the study aims, targeted effects, and interpretations were summarized. The eight meta-analyses summarised results from 82 primary studies including 144 unique models. Only one meta-analysis explicitly considered the energy adjustment strategy of the primary studies to determine eligibility for a substitution subgroup analysis. None of the meta-analyses acknowledged that they were pooling estimates for different effects. 82% of the models from the primary studies were implicitly estimating substitution effects but this was not explicitly stated in most study aims, interpretation or conclusions. Our meta-scientific review found little evidence that the energy adjustment strategies of the primary studies were being considered in the synthesis or interpretation of evidence. Consequently, the pooled estimates reflect ill-defined quantities with unclear interpretations. We offer recommendations to improve the conduct of future meta-analyses and the quality of evidence that informs nutritional recommendations."}
{"id": "2512.07306", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07306", "abs": "https://arxiv.org/abs/2512.07306", "authors": ["Thierry Petit", "Arnault Pachot"], "title": "Exact Synthetic Populations for Scalable Societal and Market Modeling", "comment": "Submitted for peer review on December 7, 2025", "summary": "We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data."}
{"id": "2512.07058", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.07058", "abs": "https://arxiv.org/abs/2512.07058", "authors": ["Bora Kim", "Myoung-jae Lee"], "title": "Easy-to-Implement Two-Way Effect Decomposition for Any Outcome Variable with Endogenous Mediator", "comment": null, "summary": "Given a binary treatment D and a binary mediator M, mediation analysis decomposes the total effect of D on an outcome Y into the direct and indirect effects. Typically, both D and M are assumed to be exogenous, but this paper allows M to be endogenous while maintaining the exogeneity of D, which holds certainly if D is randomized. The endogeneity problem of M is then overcome using a binary instrumental variable Z. We derive a nonparametric \"causal reduced form (CRF)\" for Y with either (D,Z,DZ) or (D,M,DZ) as the regressors. The CRF enables estimating the direct and indirect effects easily with ordinary least squares or instrumental variable estimator, instead of matching or inverse probability weighting that have difficulties in finding the asymptotic distribution or in dealing with near-zero denominators. Not just this ease in implementation, our approach is applicable to any Y (binary, count, continuous, etc.). Simulation and empirical studies illustrate our approach."}
{"id": "2512.07019", "categories": ["stat.ME", "cs.AI", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.07019", "abs": "https://arxiv.org/abs/2512.07019", "authors": ["Zhiyu Xu", "Jia Liu", "Yixin Wang", "Yuqi Gu"], "title": "Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model."}
{"id": "2512.07335", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07335", "abs": "https://arxiv.org/abs/2512.07335", "authors": ["Paul Wilsens", "Katrien Antonio", "Gerda Claeskens"], "title": "Machine learning in an expectation-maximisation framework for nowcasting", "comment": null, "summary": "Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant."}
{"id": "2512.07083", "categories": ["stat.ME", "econ.EM"], "pdf": "https://arxiv.org/pdf/2512.07083", "abs": "https://arxiv.org/abs/2512.07083", "authors": ["Gabriel Saco"], "title": "Finite-Sample Failures and Condition-Number Diagnostics in Double Machine Learning", "comment": "32 pages. Theoretical results and Monte Carlo study. Comments welcome", "summary": "Standard Double Machine Learning (DML; Chernozhukov et al., 2018) confidence intervals can exhibit substantial finite-sample coverage distortions when the underlying score equations are ill-conditioned, even if nuisance functions are estimated with state-of-the-art methods. Focusing on the partially linear regression (PLR) model, we show that a simple, easily computed condition number for the orthogonal score, denoted kappa_DML := 1 / |J_theta|, largely determines when DML inference is reliable. Our first result derives a nonasymptotic, Berry-Esseen-type bound showing that the coverage error of the usual DML t-statistic is of order n^{-1/2} + sqrt(n) * r_n, where r_n is the standard DML remainder term summarizing nuisance estimation error. Our second result provides a refined linearization in which both estimation error and confidence interval length scale as kappa_DML / sqrt(n) + kappa_DML * r_n, so that ill-conditioning directly inflates both variance and bias. These expansions yield three conditioning regimes - well-conditioned, moderately ill-conditioned, and severely ill-conditioned - and imply that informative, shrinking confidence sets require kappa_DML = o_p(sqrt(n)) and kappa_DML * r_n -> 0. We conduct Monte Carlo experiments across overlap levels, nuisance learners (OLS, Lasso, random forests), and both low- and high-dimensional (p > n) designs. Across these designs, kappa_DML is highly predictive of finite-sample performance: well-conditioned designs with kappa_DML < 1 deliver near-nominal coverage with short intervals, whereas severely ill-conditioned designs can exhibit large bias and coverage around 40% for nominal 95% intervals, despite flexible nuisance fitting. We propose reporting kappa_DML alongside DML estimates as a routine diagnostic of score conditioning, in direct analogy to condition-number checks and weak-instrument diagnostics in IV settings."}
{"id": "2512.07739", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.07739", "abs": "https://arxiv.org/abs/2512.07739", "authors": ["Lucy D'Agostino McGowan", "Sarah C. Lotspeich", "Michael G. Hudgens"], "title": "Symmetric Vaccine Efficacy", "comment": null, "summary": "Traditional measures of vaccine efficacy (VE) are inherently asymmetric, constrained above by $1$ but unbounded below. As a result, VE estimates and corresponding confidence intervals can extend far below zero, making interpretation difficult and potentially obscuring whether the apparent effect reflects true harm or simply statistical uncertainty. The proposed symmetric vaccine efficacy (SVE) is a bounded and interpretable alternative to VE that maintains desirable statistical properties while resolving these asymmetries. SVE is defined as a symmetric transformation of infection risks, with possible values within $[-1, 1]$, providing a common scale for both beneficial and harmful vaccine effects. This paper describes the relationship between SVE and traditional VE, considers inference about SVE, and illustrates the utility of the proposed measure by reanalyzing data from a randomized trial of a candidate HIV vaccine. Open-source tools for computing estimates of SVE and corresponding confidence intervals are available in R through the sve package."}
{"id": "2512.07541", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07541", "abs": "https://arxiv.org/abs/2512.07541", "authors": ["Youngwen Sun", "Katerina Papagiannouli", "Vladimir Spokoiny"], "title": "High-Dimensional Change Point Detection using Graph Spanning Ratio", "comment": null, "summary": "Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical."}
{"id": "2512.07088", "categories": ["stat.ME", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.07088", "abs": "https://arxiv.org/abs/2512.07088", "authors": ["Abdoulaye Camara", "Saliou Diouf", "Moumouni Diallo", "Gane Samb Lo"], "title": "Asymptotic theory and statistical inference for the samples problems with heavy-tailed data using the functional empirical process", "comment": "35 pages, 2 figures", "summary": "This paper introduces the Trimmed Functional Empirical Process (TFEP) as a robust framework for statistical inference when dealing with heavy-tailed or skewed distributions, where classical moments such as the mean or variance may be infinite or undefined. Standard approaches including the classical Functional Empirical Process (FEP), break down under such conditions, especially for distributions like Pareto, Cauchy, low degree of freedom Student-t, due to their reliance on finite-variance assumptions to guarantee asymptotic convergence. The TFEP approach addresses these limitations by trimming a controlled proportion of extreme order statistics, thereby stabilizing the empirical process and restoring asymptotic Gaussian behavior. We establish the weak convergence of the TFEP under mild regularity conditions and derive new asymptotic distributions for one-sample and twosample problems. These theoretical developments lead to robust confidence intervals for truncated means, variances, and their differences or ratios. The efficiency and reliability of the TFEP are supported by extensive Monte Carlo experiments and an empirical application to Senegalese income data. In all scenarios, the TFEP provides accurate inference where both Gaussian-based methods and the classical FEP break down. The methodology thus offers a powerful and flexible tool for statistical analysis in heavy-tailed and non-standard environments."}
{"id": "2512.07557", "categories": ["stat.ML", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.07557", "abs": "https://arxiv.org/abs/2512.07557", "authors": ["Jitendra K. Tugnait"], "title": "On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series", "comment": "16 pages, 3 figures, 4 tables", "summary": "Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data."}
{"id": "2512.07575", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.07575", "abs": "https://arxiv.org/abs/2512.07575", "authors": ["Niklas V. Lehmann"], "title": "Surprisingly-early bias in forecasts for unscheduled events", "comment": null, "summary": "When a dataset contains forecasts on unscheduled events, such as natural catastrophes, outcomes may be censored or ``hidden'' since some events have not yet occurred. This article finds that this can lead to a selection bias which affects the perceived accuracy and calibration of forecasts. This selection bias can be eliminated by excluding forecasts on outcomes which have been verified surprisingly early."}
{"id": "2512.07578", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.07578", "abs": "https://arxiv.org/abs/2512.07578", "authors": ["Dongseok Kim", "Hyoungsun Choi", "Mohamed Jismy Aashik Rasool", "Gisung Oh"], "title": "$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations", "comment": "15 pages", "summary": "We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference."}
{"id": "2512.07739", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.07739", "abs": "https://arxiv.org/abs/2512.07739", "authors": ["Lucy D'Agostino McGowan", "Sarah C. Lotspeich", "Michael G. Hudgens"], "title": "Symmetric Vaccine Efficacy", "comment": null, "summary": "Traditional measures of vaccine efficacy (VE) are inherently asymmetric, constrained above by $1$ but unbounded below. As a result, VE estimates and corresponding confidence intervals can extend far below zero, making interpretation difficult and potentially obscuring whether the apparent effect reflects true harm or simply statistical uncertainty. The proposed symmetric vaccine efficacy (SVE) is a bounded and interpretable alternative to VE that maintains desirable statistical properties while resolving these asymmetries. SVE is defined as a symmetric transformation of infection risks, with possible values within $[-1, 1]$, providing a common scale for both beneficial and harmful vaccine effects. This paper describes the relationship between SVE and traditional VE, considers inference about SVE, and illustrates the utility of the proposed measure by reanalyzing data from a randomized trial of a candidate HIV vaccine. Open-source tools for computing estimates of SVE and corresponding confidence intervals are available in R through the sve package."}
{"id": "2512.07755", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07755", "abs": "https://arxiv.org/abs/2512.07755", "authors": ["Brenda Anague", "Bamdad Hosseini", "Issa Karambal", "Jean Medard Ngnotchouye"], "title": "Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion", "comment": null, "summary": "Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements."}
{"id": "2512.06348", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.06348", "abs": "https://arxiv.org/abs/2512.06348", "authors": ["Xiaoyu Ma", "Likun Zhang", "Christopher K. Wikle"], "title": "Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders", "comment": null, "summary": "Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Niño/Southern Oscillation (ENSO) index."}
{"id": "2512.07770", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07770", "abs": "https://arxiv.org/abs/2512.07770", "authors": ["Dongjian Hu", "Junxi Wu", "Shu-Tao Xia", "Changliang Zou"], "title": "Distribution-informed Online Conformal Prediction", "comment": null, "summary": "Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines."}
{"id": "2512.07578", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.07578", "abs": "https://arxiv.org/abs/2512.07578", "authors": ["Dongseok Kim", "Hyoungsun Choi", "Mohamed Jismy Aashik Rasool", "Gisung Oh"], "title": "$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations", "comment": "15 pages", "summary": "We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference."}
{"id": "2512.06514", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.06514", "abs": "https://arxiv.org/abs/2512.06514", "authors": ["Jie Wu", "Bo Zhang", "Daoji Li", "Zemin Zheng"], "title": "Simultaneous Heterogeneity and Reduced-rank Learning for Multivariate Response Regression", "comment": null, "summary": "Heterogeneous data are now ubiquitous in many applications in which correctly identifying the subgroups from a heterogeneous population is critical. Although there is an increasing body of literature on subgroup detection, existing methods mainly focus on the univariate response setting. In this paper, we propose a joint heterogeneity and reduced-rank learning framework to simultaneously identify the subgroup structure and estimate the covariate effects for heterogeneous multivariate response regression. In particular, our approach uses rank-constrained pairwise fusion penalization and conducts the subgroup analysis without requiring prior knowledge regarding the individual subgroup memberships. We implement the proposed approach by an alternating direction method of multipliers (ADMM) algorithm and show its convergence. We also establish the asymptotic properties for the resulting estimators under mild and interpretable conditions. A predictive information criterion is proposed to select the rank of the coefficient matrix with theoretical support. The effectiveness of the proposed approach is demonstrated through simulation studies and a real data application."}
{"id": "2512.06522", "categories": ["stat.ME", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.06522", "abs": "https://arxiv.org/abs/2512.06522", "authors": ["Di Wu", "Jacob Bien", "Snigdha Panigrahi"], "title": "Hierarchical Clustering With Confidence", "comment": "44 Pages, 9 Figures, 2 Algorithms", "summary": "Agglomerative hierarchical clustering is one of the most widely used approaches for exploring how observations in a dataset relate to each other. However, its greedy nature makes it highly sensitive to small perturbations in the data, often producing different clustering results and making it difficult to separate genuine structure from spurious patterns. In this paper, we show how randomizing hierarchical clustering can be useful not just for measuring stability but also for designing valid hypothesis testing procedures based on the clustering results.\n  We propose a simple randomization scheme together with a method for constructing a valid p-value at each node of the hierarchical clustering dendrogram that quantifies evidence against performing the greedy merge. Our test controls the Type I error rate, works with any hierarchical linkage without case-specific derivations, and simulations show it is substantially more powerful than existing selective inference approaches. To demonstrate the practical utility of our p-values, we develop an adaptive $α$-spending procedure that estimates the number of clusters, with a probabilistic guarantee on overestimation. Experiments on simulated and real data show that this estimate yields powerful clustering and can be used, for example, to assess clustering stability across multiple runs of the randomized algorithm."}
{"id": "2512.07019", "categories": ["stat.ME", "cs.AI", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.07019", "abs": "https://arxiv.org/abs/2512.07019", "authors": ["Zhiyu Xu", "Jia Liu", "Yixin Wang", "Yuqi Gu"], "title": "Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model."}
{"id": "2512.07074", "categories": ["stat.AP", "hep-ex", "hep-ph", "physics.data-an", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.07074", "abs": "https://arxiv.org/abs/2512.07074", "authors": ["Huanbiao Zhu", "Krish Desai", "Mikael Kuusela", "Vinicius Mikuni", "Benjamin Nachman", "Larry Wasserman"], "title": "Machine Learning-based Unfolding for Cross Section Measurements in the Presence of Nuisance Parameters", "comment": null, "summary": "Statistically correcting measured cross sections for detector effects is an important step across many applications. In particle physics, this inverse problem is known as \\textit{unfolding}. In cases with complex instruments, the distortions they introduce are often known only implicitly through simulations of the detector. Modern machine learning has enabled efficient simulation-based approaches for unfolding high-dimensional data. Among these, one of the first methods successfully deployed on experimental data is the \\textsc{OmniFold} algorithm, a classifier-based Expectation-Maximization procedure. In practice, however, the forward model is only approximately specified, and the corresponding uncertainty is encoded through nuisance parameters. Building on the well-studied \\textsc{OmniFold} algorithm, we show how to extend machine learning-based unfolding to incorporate nuisance parameters. Our new algorithm, called Profile \\textsc{OmniFold}, is demonstrated using a Gaussian example as well as a particle physics case study using simulated data from the CMS Experiment at the Large Hadron Collider."}
