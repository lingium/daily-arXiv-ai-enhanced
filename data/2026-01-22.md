<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 1]
- [stat.AP](#stat.AP) [Total: 12]
- [stat.ML](#stat.ML) [Total: 13]
- [stat.ME](#stat.ME) [Total: 26]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Bayesian Inference for Partially Observed McKean-Vlasov SDEs with Full Distribution Dependence](https://arxiv.org/abs/2601.12515)
*Ning Ning,Amin Wu*

Main category: stat.CO

TL;DR: 提出贝叶斯框架用于部分观测的McKean-Vlasov随机微分方程的状态推断和参数估计，开发了单层和多层粒子马尔可夫链蒙特卡洛算法，其中多层方法在计算复杂度上有显著改进。


<details>
  <summary>Details</summary>
Motivation: McKean-Vlasov随机微分方程在神经科学、金融和流行病学中广泛应用，但在许多应用中系统只能部分观测，当漂移和扩散系数都依赖于演化经验分布时，推断变得非常困难。

Method: 结合时间离散化和基于粒子的近似构建可处理的似然估计器，设计了两种粒子马尔可夫链蒙特卡洛算法：单层PMCMC方法和多层PMCMC方法，后者耦合不同离散化水平的粒子系统。

Result: 多层构造产生相关的似然估计，在计算成本O(ε^{-6})下实现均方误差O(ε^2)，优于单层方案的O(ε^{-7})复杂度。数值实验证实了所提方法的效率和准确性。

Conclusion: 该研究为完全依赖于分布扩散设置的MVSDEs提供了贝叶斯推断框架，在标准正则性假设下提供理论保证，多层PMCMC方法在计算效率上有显著优势。

Abstract: McKean-Vlasov stochastic differential equations (MVSDEs) describe systems whose dynamics depend on both individual states and the population distribution, and they arise widely in neuroscience, finance, and epidemiology. In many applications the system is only partially observed, making inference very challenging when both drift and diffusion coefficients depend on the evolving empirical law. This paper develops a Bayesian framework for latent state inference and parameter estimation in such partially observed MVSDEs. We combine time-discretization with particle-based approximations to construct tractable likelihood estimators, and we design two particle Markov chain Monte Carlo (PMCMC) algorithms: a single-level PMCMC method and a multilevel PMCMC (MLPMCMC) method that couples particle systems across discretization levels. The multilevel construction yields correlated likelihood estimates and achieves mean square error $(O(\varepsilon^2))$ at computational cost $(O(\varepsilon^{-6}))$, improving on the $(O(\varepsilon^{-7}))$ complexity of single-level schemes. We address the fully law-dependent diffusion setting which is the most general formulation of MVSDEs, and provide theoretical guarantees under standard regularity assumptions. Numerical experiments confirm the efficiency and accuracy of the proposed methodology.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [2] [Adversarial Drift-Aware Predictive Transfer: Toward Durable Clinical AI](https://arxiv.org/abs/2601.11860)
*Xin Xiong,Zijian Guo,Haobo Zhu,Chuan Hong,Jordan W Smoller,Tianxi Cai,Molei Liu*

Main category: stat.AP

TL;DR: ADAPT框架通过构建未来模型不确定性集，优化最坏情况性能，实现临床AI系统对时间漂移的鲁棒性，无需频繁重训练


<details>
  <summary>Details</summary>
Motivation: 临床AI系统部署后常因时间数据漂移（如人群演变、诊断编码更新、COVID-19冲击）导致性能衰减，而频繁重训练因计算成本和隐私限制不切实际

Method: ADAPT框架结合历史源模型和有限当前数据构建未来模型不确定性集，通过优化最坏情况性能平衡当前准确性和未来漂移鲁棒性，仅需历史时期的模型级摘要统计

Result: 在麻省总院布里格姆（2005-2021）和杜克大学健康系统的纵向自杀风险预测中，ADAPT在编码转换和疫情冲击下表现出优越的稳定性，最小化年度性能衰减

Conclusion: ADAPT为高风险医疗环境中维持可靠AI提供了可扩展路径，无需未来数据标注或重训练，通过最小化性能衰减实现临床AI系统的持久性

Abstract: Clinical AI systems frequently suffer performance decay post-deployment due to temporal data shifts, such as evolving populations, diagnostic coding updates (e.g., ICD-9 to ICD-10), and systemic shocks like the COVID-19 pandemic. Addressing this ``aging'' effect via frequent retraining is often impractical due to computational costs and privacy constraints. To overcome these hurdles, we introduce Adversarial Drift-Aware Predictive Transfer (ADAPT), a novel framework designed to confer durability against temporal drift with minimal retraining. ADAPT innovatively constructs an uncertainty set of plausible future models by combining historical source models and limited current data. By optimizing worst-case performance over this set, it balances current accuracy with robustness against degradation due to future drifts. Crucially, ADAPT requires only summary-level model estimators from historical periods, preserving data privacy and ensuring operational simplicity. Validated on longitudinal suicide risk prediction using electronic health records from Mass General Brigham (2005--2021) and Duke University Health Systems, ADAPT demonstrated superior stability across coding transitions and pandemic-induced shifts. By minimizing annual performance decay without labeling or retraining future data, ADAPT offers a scalable pathway for sustaining reliable AI in high-stakes healthcare environments.

</details>


### [3] [A Deep Learning-Copula Framework for Climate-Related Home Insurance Risk](https://arxiv.org/abs/2601.11949)
*Asim K. Dey*

Main category: stat.AP

TL;DR: 提出结合深度神经网络与copula多元分析的两步法，研究降水对房屋保险索赔的影响，以加拿大草原地区为案例验证


<details>
  <summary>Details</summary>
Motivation: 极端天气事件日益频繁，对保险业构成直接威胁，保险公司需要可靠工具评估未来风险，以设定保费、维持偿付能力并支持灾害准备和恢复力建设

Method: 采用两步法：结合深度神经网络的预测能力和copula多元分析的灵活性，研究降水模式与索赔动态的关系

Result: 通过加拿大草原地区2002-2011年数据的案例研究验证了该方法

Conclusion: 该方法能够更详细地理解降水模式与索赔动态的关系，为保险公司提供可靠的风险评估工具

Abstract: Extreme weather events are becoming more common, with severe storms, floods, and prolonged precipitation affecting communities worldwide. These shifts in climate patterns pose a direct threat to the insurance industry, which faces growing exposure to weather-related damages. As claims linked to extreme weather rise, insurance companies need reliable tools to assess future risks. This is not only essential for setting premiums and maintaining solvency but also for supporting broader disaster preparedness and resilience efforts. In this study, we propose a two-step method to examine the impact of precipitation on home insurance claims. Our approach combines the predictive power of deep neural networks with the flexibility of copula-based multivariate analysis, enabling a more detailed understanding of how precipitation patterns relate to claim dynamics. We demonstrate this methodology through a case study of the Canadian Prairies, using data from 2002 to 2011.

</details>


### [4] [A warping function-based control chart for detecting distributional changes in damage-sensitive features for structural condition assessment](https://arxiv.org/abs/2601.12221)
*Zhicheng Chen,Wenyu Chen,Xinyi Lei*

Main category: stat.AP

TL;DR: 提出一种基于概率密度函数和变形函数的控制图方法，用于检测结构健康监测中损伤敏感特征数据的分布变化，能同时检测均值/方差偏移和复杂形状变形，具有优异的在线检测性能和抗数据污染鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统控制图主要检测均值或方差偏移，难以识别复杂的分布形状变化，导致损伤检测灵敏度不足，且对数据污染的鲁棒性较差。需要开发能同时检测分布偏移和形状变形的更灵敏、更鲁棒的损伤检测方法。

Method: 使用分组DSF数据的概率密度函数作为监控对象，通过变形函数表征形状变化。在函数数据分析框架下，构建专门用于监控变形函数的非参数控制图。

Result: 新方法能够同时检测分布偏移和复杂形状变形，具有优异的在线检测性能和对数据污染的鲁棒性。大量仿真研究证明其优于现有方法，并在大跨斜拉桥缆索状态评估的实际应用中验证了其工程实用性。

Conclusion: 提出的基于PDF和变形函数的控制图方法有效解决了传统方法在检测复杂分布形状变化方面的局限性，为结构健康监测中的自动损伤检测提供了更灵敏、更鲁棒的工具。

Abstract: Data-driven damage detection methods achieve damage identification by analyzing changes in damage-sensitive features (DSFs) derived from structural health monitoring (SHM) data. The core reason for their effectiveness lies in the fact that damage or structural state transition can be manifested as changes in the distribution of DSF data. This enables us to reframe the problem of damage detection as one of identifying these distributional changes. Hence, developing automated tools for detecting such changes is pivotal for automated structural health diagnosis. Control charts are extensively utilized in SHM for DSF change detection, owing to their excellent online detection and early warning capabilities. However, conventional methods are primarily designed to detect mean or variance shifts, making it challenging to identify complex shape changes in distributions. This limitation results in insufficient damage detection sensitivity. Moreover, they typically exhibit poor robustness against data contamination. This paper proposes a novel control chart to address these limitations. It employs the probability density functions (PDFs) of subgrouped DSF data as monitoring objects, with shape deformations characterized by warping functions. Furthermore, a nonparametric control chart is specifically constructed for warping function monitoring in the functional data analysis framework. Key advantages of the new method include the ability to detect both shifts and complex shape deformations in distributions, excellent online detection performance, and robustness against data contamination. Extensive simulation studies demonstrate its superiority over competing approaches. Finally, the method is applied to detecting distributional changes in DSF data for cable condition assessment in a long-span cable-stayed bridge, demonstrating its practical utility in engineering.

</details>


### [5] [A Machine Learning--Based Surrogate EKMA Framework for Diagnosing Urban Ozone Formation Regimes: Evidence from Los Angeles](https://arxiv.org/abs/2601.12321)
*Sijie Zheng*

Main category: stat.AP

TL;DR: 该研究开发了一个基于机器学习的替代框架，受经验动力学建模方法启发，用于诊断洛杉矶的臭氧形成机制，发现该地区在2024-2025年期间主要受VOC限制。


<details>
  <summary>Details</summary>
Motivation: 地表臭氧污染仍然是全球许多大都市地区面临的持续挑战，因为臭氧形成对氮氧化物和挥发性有机化合物的非线性依赖使得设计有效的排放控制策略变得复杂。虽然化学传输模型提供了机理洞察，但它们依赖于详细的排放清单且计算成本高昂。

Method: 研究开发了一个基于机器学习的替代框架，受经验动力学建模方法启发。使用洛杉矶2024-2025年的每小时空气质量观测数据，训练随机森林模型来预测地表臭氧浓度，基于前体物测量和时空特征（包括站点位置和循环时间编码）。

Result: 模型实现了强大的预测性能，排列重要性分析显示昼夜时间特征和二氧化氮起主导作用，一氧化碳也有额外贡献。基于训练好的替代模型，通过扰动前体物浓度进行EKMA式敏感性实验，结果表明研究期间洛杉矶的臭氧形成主要是VOC限制的。

Conclusion: 提出的框架为数据丰富的城市环境中的臭氧机制诊断提供了一种高效且可解释的方法。

Abstract: Surface ozone pollution remains a persistent challenge in many metropolitan regions worldwide, as the nonlinear dependence of ozone formation on nitrogen oxides and volatile organic compounds (VOCs) complicates the design of effective emission control strategies. While chemical transport models provide mechanistic insights, they rely on detailed emission inventories and are computationally expensive.
  This study develops a machine learning--based surrogate framework inspired by the Empirical Kinetic Modeling Approach (EKMA). Using hourly air quality observations from Los Angeles during 2024--2025, a random forest model is trained to predict surface ozone concentrations based on precursor measurements and spatiotemporal features, including site location and cyclic time encodings. The model achieves strong predictive performance, with permutation importance highlighting the dominant roles of diurnal temporal features and nitrogen dioxide, along with additional contributions from carbon monoxide.
  Building on the trained surrogate, EKMA-style sensitivity experiments are conducted by perturbing precursor concentrations while holding other covariates fixed. The results indicate that ozone formation in Los Angeles during the study period is predominantly VOC-limited. Overall, the proposed framework offers an efficient and interpretable approach for ozone regime diagnosis in data-rich urban environments.

</details>


### [6] [Assessing Interactive Causes of an Occurred Outcome Due to Two Binary Exposures](https://arxiv.org/abs/2601.12478)
*Shanshan Luo,Wei Li,Xueli Wang,Shaojie Wei,Zhi Geng*

Main category: stat.AP

TL;DR: 该论文提出了一种因果归因分析方法，用于评估二元暴露变量对二元结果变量的交互作用贡献，通过定义后验概率并使用次要结果变量实现可识别性，应用于吸烟与石棉暴露案例。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断主要关注治疗效果评估，而因果归因分析需要识别导致观察结果的关键因素。对于二元暴露变量和二元结果变量，研究者不仅需要评估特定暴露导致结果的可能性，还需要评估两个暴露交互作用导致结果的可能性。即使在随机对照试验中，识别和评估两个暴露之间的回顾性因果交互作用仍然具有挑战性。

Method: 定义后验概率来表征观察结果的交互原因，通过使用可能在主要结果之后出现的次要结果变量来建立后验概率的可识别性。

Result: 将提出的方法应用于经典的吸烟和石棉暴露案例，结果显示对于吸烟且暴露于石棉的肺癌患者，疾病主要归因于吸烟和石棉暴露之间的协同效应。

Conclusion: 该研究提供了一种评估二元暴露变量交互作用贡献的因果归因分析方法，通过后验概率和次要结果变量解决了回顾性因果交互作用的识别挑战，在吸烟与石棉暴露案例中验证了方法的有效性。

Abstract: In contrast to evaluating treatment effects, causal attribution analysis focuses on identifying the key factors responsible for an observed outcome. For two binary exposure variables and a binary outcome variable, researchers need to assess not only the likelihood that an observed outcome was caused by a particular exposure, but also the likelihood that it resulted from the interaction between the two exposures. For example, in the case of a male worker who smoked, was exposed to asbestos, and developed lung cancer, researchers aim to explore whether the cancer resulted from smoking, asbestos exposure, or their interaction. Even in randomized controlled trials, widely regarded as the gold standard for causal inference, identifying and evaluating retrospective causal interactions between two exposures remains challenging. In this paper, we define posterior probabilities to characterize the interactive causes of an observed outcome. We establish the identifiability of posterior probabilities by using a secondary outcome variable that may appear after the primary outcome. We apply the proposed method to the classic case of smoking and asbestos exposure. Our results indicate that for lung cancer patients who smoked and were exposed to asbestos, the disease is primarily attributable to the synergistic effect between smoking and asbestos exposure.

</details>


### [7] [Stop using limiting stimuli as a measure of sensitivities of energetic materials](https://arxiv.org/abs/2601.12552)
*Dennis Christensen,Geir Petter Novik*

Main category: stat.AP

TL;DR: 该论文指出联合国"1-In-6"测试中使用的"极限刺激"概念存在根本缺陷，不是衡量爆炸物敏感度的可靠指标，并提出了三种替代方法。


<details>
  <summary>Details</summary>
Motivation: 准确评估爆炸物敏感度对安全至关重要，但当前广泛使用的联合国"1-In-6"测试中的"极限刺激"概念存在理论缺陷，无法提供可靠信息且不能构建置信区间，导致研究混乱。

Method: 论文首先分析"极限刺激"概念的理论缺陷，然后提出三种基于统计原理的替代敏感度测试方法，通过广泛的模拟研究比较性能，并将最佳方法应用于PETN摩擦敏感度的实际数据。

Result: 研究表明"极限刺激"不是明确定义的敏感度概念，无法量化估计不确定性；通过模拟比较找到了性能最佳的替代方法，并成功应用于PETN的实际敏感度评估。

Conclusion: 应停止使用基于"极限刺激"的1-In-6测试，采用统计上更可靠的替代方法来评估爆炸物敏感度，以提高安全性和研究一致性。

Abstract: Accurately estimating the sensitivity of explosive materials is a potentially life-saving task which requires standardised protocols across nations. One of the most widely applied procedures worldwide is the so-called '1-In-6' test from the United Nations (UN) Manual of Tests in Criteria, which estimates a 'limiting stimulus' for a material. In this paper we demonstrate that, despite their popularity, limiting stimuli are not a well-defined notion of sensitivity and do not provide reliable information about a material's susceptibility to ignition. In particular, they do not permit construction of confidence intervals to quantify estimation uncertainty. We show that continued reliance on limiting stimuli through the 1-In-6 test has caused needless confusion in energetic materials research, both in theoretical studies and practical safety applications. To remedy this problem, we consider three well-founded alternative approaches to sensitivity testing to replace limiting stimulus estimation. We compare their performance in an extensive simulation study and apply the best-performing approach to real data, estimating the friction sensitivity of pentaerythritol tetranitrate (PETN).

</details>


### [8] [The impact of abnormal temperatures on crop yields in Italy: a functional quantile regression approach](https://arxiv.org/abs/2601.12864)
*Giovanni Bocchi,Alessandra Micheletti,Paolo Nota,Alessandro Olper*

Main category: stat.AP

TL;DR: 该研究应用函数回归分析识别温度与降水异常影响作物产量的关键季节时段，使用意大利1952-2023年省级数据，分析玉米和软小麦，发现温度影响具有季节特异性时间模式。


<details>
  <summary>Details</summary>
Motivation: 传统统计产量模型假设温度效应在季节内可加，无法捕捉天气影响的时序和函数形状。需要更精确的方法来识别作物生长周期中天气异常影响产量的具体时段，为气候变化适应策略提供依据。

Method: 采用函数回归分析方法，使用意大利省级数据（1952-2023年），分析玉米和软小麦两种主要谷物。该方法能够捕捉天气影响的时序和函数形状，而非传统模型的加性温度效应假设。

Result: 研究发现：1）高于平均温度在6-8月降低玉米产量，但在4月和10月有轻微正面效应；2）异常高温在3月底至4月初对软小麦产量产生负面影响；3）降水效应也具季节依赖性，早期改善小麦产量，后期则降低产量。

Conclusion: 考虑季节内天气模式对准确评估气候影响至关重要，这些发现为调整关键作物管理投入的时机提供了见解，有助于制定气候变化适应策略。

Abstract: In this study, we apply functional regression analysis to identify the specific within-season periods during which temperature and precipitation anomalies most affect crop yields. Using provincial data for Italy from 1952 to 2023, we analyze two major cereals, maize and soft wheat, and quantify how abnormal weather conditions influence yields across the growing cycle. Unlike traditional statistical yield models, which assume additive temperature effects over the season, our approach is capable of capturing the timing and functional shape of weather impacts. In particular, the results show that above-average temperatures reduce maize yields primarily between June and August, while exerting a mild positive effect in April and October. For soft wheat, unusually high temperatures negatively affect yields from late March to early April. Precipitation also exerts season-dependent effects, improving wheat yields early in the season but reducing them later on. These findings highlight the importance of accounting for intra-seasonal weather patterns to provide insights for climate change adaptation strategies, including the timely adjustment of key crop management inputs.

</details>


### [9] [Improving Geopolitical Forecasts with Bayesian Networks](https://arxiv.org/abs/2601.13362)
*Matthew Martin*

Main category: stat.AP

TL;DR: 贝叶斯网络在预测准确性上优于逻辑回归但不如重新校准聚合方法，主要受离散化信息损失和线性假设限制


<details>
  <summary>Details</summary>
Motivation: 探索贝叶斯网络相比逻辑回归和重新校准聚合方法在预测准确性上的改进潜力，使用Good Judgment Project数据

Method: 比较正则化逻辑回归模型、基线重新校准聚合与两种贝叶斯网络（结构学习型和朴素型），使用四个预测变量：与聚合值的绝对差异、预测值、距离问题截止天数、平均标准化Brier分数

Result: 重新校准聚合方法准确率最高（AUC=0.985），其次是两种贝叶斯网络，最后是逻辑回归模型。贝叶斯网络性能受离散化过程信息减少影响，逻辑回归受线性假设违反影响

Conclusion: 未来研究应探索贝叶斯网络与逻辑回归的混合方法，考察更多预测变量，并考虑层次数据依赖性

Abstract: This study explores how Bayesian networks (BNs) can improve forecast accuracy compared to logistic regression and recalibration and aggregation methods, using data from the Good Judgment Project. Regularized logistic regression models and a baseline recalibrated aggregate were compared to two types of BNs: structure-learned BNs with arcs between predictors, and naive BNs. Four predictor variables were examined: absolute difference from the aggregate, forecast value, days prior to question close, and mean standardized Brier score. Results indicated the recalibrated aggregate achieved the highest accuracy (AUC = 0.985), followed by both types of BNs, then the logistic regression models. Performance of the BNs was likely harmed by reduced information from the discretization process and violation of the assumption of linearity likely harmed the logistic regression models. Future research should explore hybrid approaches combining BNs with logistic regression, examine additional predictor variables, and account for hierarchical data dependencies.

</details>


### [10] [A Two-Stage Bayesian Framework for Multi-Fidelity Online Updating of Spatial Fragility Fields](https://arxiv.org/abs/2601.13396)
*Abdullah M. Braik,Maria Koliou*

Main category: stat.AP

TL;DR: 提出贝叶斯框架统一物理脆弱性函数与实时灾后观测，通过两阶段方法持续更新区域脆弱性估计，应用于2011年乔普林龙卷风案例。


<details>
  <summary>Details</summary>
Motivation: 解决自然灾害建模中长期存在的空白，即如何将基于物理的脆弱性函数与实时灾后观测数据统一起来，实现区域脆弱性估计的持续优化。

Method: 采用两阶段贝叶斯框架：第一阶段将物理脆弱性估计转化为Probit-Normal表示，通过Beta代理进行局部贝叶斯更新；第二阶段通过probit变换的高斯过程，将高保真站点的信息传播到低保真和未观测区域。

Result: 应用于2011年乔普林龙卷风案例，方法能够纠正有偏先验、空间传播信息，并产生考虑不确定性的超越概率，支持实时态势感知。

Conclusion: 该框架成功统一了物理模型与实时观测，实现了脆弱性估计的持续优化，为自然灾害风险评估提供了不确定性感知的实时决策支持工具。

Abstract: This paper addresses a long-standing gap in natural hazard modeling by unifying physics-based fragility functions with real-time post-disaster observations. It introduces a Bayesian framework that continuously refines regional vulnerability estimates as new data emerges. The framework reformulates physics-informed fragility estimates into a Probit-Normal (PN) representation that captures aleatory variability and epistemic uncertainty in an analytically tractable form. Stage 1 performs local Bayesian updating by moment-matching PN marginals to Beta surrogates that preserve their probability shapes, enabling conjugate Beta-Bernoulli updates with soft, multi-fidelity observations. Fidelity weights encode source reliability, and the resulting Beta posteriors are re-projected into PN form, producing heteroscedastic fragility estimates whose variances reflect data quality and coverage. Stage 2 assimilates these heteroscedastic observations within a probit-warped Gaussian Process (GP), which propagates information from high-fidelity sites to low-fidelity and unobserved regions through a composite kernel that links space, archetypes, and correlated damage states. The framework is applied to the 2011 Joplin tornado, where wind-field priors and computer-vision damage assessments are fused under varying assumptions about tornado width, sampling strategy, and observation completeness. Results show that the method corrects biased priors, propagates information spatially, and produces uncertainty-aware exceedance probabilities that support real-time situational awareness.

</details>


### [11] [Are Large Language Models able to Predict Highly Cited Papers? Evidence from Statistical Publications](https://arxiv.org/abs/2601.13627)
*Zhanshuo Ye,Yiming Hou,Rui Pan,Tianchen Gao,Hansheng Wang*

Main category: stat.AP

TL;DR: 本文提出一个基于大语言模型的文本中心框架，利用论文发表时的文本信息（标题、摘要、关键词等）来预测统计学领域的高被引论文，并开发了微信小程序便于实际应用。


<details>
  <summary>Details</summary>
Motivation: 预测高被引论文是一个长期挑战，涉及研究内容、学术社区和时间动态的复杂交互。随着大语言模型的发展，需要探索早期文本信息是否能提供长期科学影响力的有用信号。

Method: 提出一个灵活的文本中心框架，利用大语言模型和结构化提示设计，仅使用发表时可获得的信息（标题、摘要、关键词和有限元数据）来预测高被引论文。使用大量统计学论文语料库进行评估。

Result: 该方法在多个发表时期和不同高被引定义下都实现了稳定且具有竞争力的性能，表现出良好的时间泛化能力。文本分析显示预测为高被引的论文集中在因果推断和深度学习等重复出现的主题上。

Conclusion: 大语言模型能够捕捉长期引用影响力的早期信号，但作为研究影响力评估工具仍存在局限性。开发了微信小程序"Stat Highly Cited Papers"来促进实际应用。

Abstract: Predicting highly-cited papers is a long-standing challenge due to the complex interactions of research content, scholarly communities, and temporal dynamics. Recent advances in large language models (LLMs) raise the question of whether early-stage textual information can provide useful signals of long-term scientific impact. Focusing on statistical publications, we propose a flexible, text-centered framework that leverages LLMs and structured prompt design to predict highly cited papers. Specifically, we utilize information available at the time of publication, including titles, abstracts, keywords, and limited bibliographic metadata. Using a large corpus of statistical papers, we evaluate predictive performance across multiple publication periods and alternative definitions of highly cited papers. The proposed approach achieves stable and competitive performance relative to existing methods and demonstrates strong generalization over time. Textual analysis further reveals that papers predicted as highly cited concentrate on recurring topics such as causal inference and deep learning. To facilitate practical use of the proposed approach, we further develop a WeChat mini program, \textit{Stat Highly Cited Papers}, which provides an accessible interface for early-stage citation impact assessment. Overall, our results provide empirical evidence that LLMs can capture meaningful early signals of long-term citation impact, while also highlighting their limitations as tools for research impact assessment.

</details>


### [12] [Correction of Pooling Matrix Mis-specifications in Compressed Sensing Based Group Testing](https://arxiv.org/abs/2601.13641)
*Shuvayan Banerjee,Radhendushka Srivastava,James Saunderson,Ajit Rajwade*

Main category: stat.AP

TL;DR: 提出一种算法，用于校正群体检测中的模型失配误差，直接从检测结果和不准确的混合矩阵中恢复健康状态


<details>
  <summary>Details</summary>
Motivation: 在公共健康领域的群体检测中，技术人员可能在混合样本时出现少量错误，导致混合矩阵不准确（模型失配误差），这给从少量混合测试中确定个体健康状态带来困难

Method: 提出一种算法，首先从混合测试结果和不准确的混合矩阵中直接校正模型失配误差，然后从校正后的混合矩阵重建信号向量来确定个体健康状态

Result: 提供了校正模型失配误差的理论保证和从校正后混合矩阵重建信号的理论误差界限，并通过数值实验验证了算法的有效性

Conclusion: 该算法能够有效处理群体检测中的模型失配误差，为资源受限或时间紧迫的疫情场景提供了更可靠的解决方案

Abstract: Compressed sensing, which involves the reconstruction of sparse signals from an under-determined linear system, has been recently used to solve problems in group testing. In a public health context, group testing aims to determine the health status values of p subjects from n<<p pooled tests, where a pool is defined as a mixture of small, equal-volume portions of the samples of a subset of subjects. This approach saves on the number of tests administered in pandemics or other resource-constrained scenarios. In practical group testing in time-constrained situations, a technician can inadvertently make a small number of errors during pool preparation, which leads to errors in the pooling matrix, which we term `model mismatch errors' (MMEs). This poses difficulties while determining health status values of the participating subjects from the results on n<<p pooled tests. In this paper, we present an algorithm to correct the MMEs in the pooled tests directly from the pooled results and the available (inaccurate) pooling matrix. Our approach then reconstructs the signal vector from the corrected pooling matrix, in order to determine the health status of the subjects. We further provide theoretical guarantees for the correction of the MMEs and the reconstruction error from the corrected pooling matrix. We also provide several supporting numerical results.

</details>


### [13] [On the Anchoring Effect of Monetary Policy on the Labor Share of Income and the Rationality of Its Setting Mechanism](https://arxiv.org/abs/2601.13675)
*Li Tuobang*

Main category: stat.AP

TL;DR: 该论文总结现代宏观经济货币理论中劳动收入份额作为核心参数通过公开市场操作锚定的争议，分析非顶层政策制定者的市场参与者对其影响范围，探讨设置机制的合理性


<details>
  <summary>Details</summary>
Motivation: 现代宏观经济货币理论认为劳动收入份额已成为通过公开市场操作锚定的核心宏观经济参数，但这一参数的设定仍是激烈经济辩论的主题，需要系统梳理争议并分析其合理性

Method: 提供争议的详细总结，分析非顶层政策制定者的市场参与者对劳动收入份额的影响范围，探索其设置机制的理性基础

Result: 论文系统梳理了劳动收入份额作为核心参数的理论争议，识别了市场参与者的影响范围，评估了当前设置机制的合理性

Conclusion: 劳动收入份额作为锚定参数的理论框架存在重要争议，需要更全面地考虑市场参与者的影响，并重新审视其设置机制的理性基础

Abstract: Modern macroeconomic monetary theory suggests that the labor share of income has effectively become a core macroe-conomic parameter anchored by top policymakers through Open Market Operations (OMO). However, the setting of this parameter remains a subject of intense economic debate. This paper provides a detailed summary of these controversies, analyzes the scope of influence exerted by market agents other than the top policymakers on the labor share, and explores the rationality of its setting mechanism.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [14] [Gradient-based Active Learning with Gaussian Processes for Global Sensitivity Analysis](https://arxiv.org/abs/2601.11790)
*Guerlain Lambert,Céline Helbert,Claire Lauvernet*

Main category: stat.ML

TL;DR: 提出一种基于高斯过程代理模型的主动学习方法，通过利用梯度信息改进敏感性分析精度，针对计算受限的复杂数值模拟器


<details>
  <summary>Details</summary>
Motivation: 复杂数值模拟器的全局敏感性分析通常受限于可负担的少量模型评估次数。在有限模拟次数下，需要高效丰富计算机实验设计来构建代理模型，减少计算负担

Method: 基于高斯过程代理模型的主动学习方法，利用GP梯度的联合后验分布开发获取函数，更好地考虑偏导数之间的相关性及其对响应面的影响，比现有DGSM导向准则更全面稳健

Result: 首先在标准基准函数上与最先进方法进行比较，然后将该方法应用于农药迁移的真实环境模型

Conclusion: 该方法通过主动学习策略，在固定评估预算下，针对输入空间中最具信息量的区域，提高了敏感性分析的准确性

Abstract: Global sensitivity analysis of complex numerical simulators is often limited by the small number of model evaluations that can be afforded. In such settings, surrogate models built from a limited set of simulations can substantially reduce the computational burden, provided that the design of computer experiments is enriched efficiently. In this context, we propose an active learning approach that, for a fixed evaluation budget, targets the most informative regions of the input space to improve sensitivity analysis accuracy. More specifically, our method builds on recent advances in active learning for sensitivity analysis (Sobol' indices and derivative-based global sensitivity measures, DGSM) that exploit derivatives obtained from a Gaussian process (GP) surrogate. By leveraging the joint posterior distribution of the GP gradient, we develop acquisition functions that better account for correlations between partial derivatives and their impact on the response surface, leading to a more comprehensive and robust methodology than existing DGSM-oriented criteria. The proposed approach is first compared to state-of-the-art methods on standard benchmark functions, and is then applied to a real environmental model of pesticide transfers.

</details>


### [15] [Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via Activation-Level Gaussian Processes](https://arxiv.org/abs/2502.20966)
*Richard Bergna,Stefan Depeweg,Sergio Calvo Ordonez,Jonathan Plenk,Alvaro Cartea,Jose Miguel Hernandez-Lobato*

Main category: stat.ML

TL;DR: 提出GAPA方法，通过高斯过程在激活层量化不确定性，避免传统方法欠拟合或计算复杂的问题，保持预训练网络预测均值不变。


<details>
  <summary>Details</summary>
Motivation: 传统不确定性量化方法（如Dropout、贝叶斯神经网络、Laplace近似）存在欠拟合或计算复杂的问题，难以应用于大规模数据集。需要一种既能保持预测准确性又能高效量化不确定性的方法。

Method: 提出Gaussian Process Activation (GAPA)方法，将不确定性量化从权重空间转移到激活层。采用后处理方式，保持预训练网络的均值预测不变。提出两种实现：GAPA-Free（从训练数据经验学习核超参数）和GAPA-Variational（通过梯度下降学习核超参数）。

Result: GAPA-Variational在大多数数据集上，在至少一个不确定性量化指标上优于Laplace近似方法。GAPA-Free在训练时效率很高。

Conclusion: GAPA方法通过激活层的高斯过程有效量化神经网络不确定性，避免了传统方法的欠拟合问题，计算效率高，为大规模数据集的不确定性量化提供了实用解决方案。

Abstract: Uncertainty quantification in neural networks through methods such as Dropout, Bayesian neural networks and Laplace approximations is either prone to underfitting or computationally demanding, rendering these approaches impractical for large-scale datasets. In this work, we address these shortcomings by shifting the focus from uncertainty in the weight space to uncertainty at the activation level, via Gaussian processes. More specifically, we introduce the Gaussian Process Activation function (GAPA) to capture neuron-level uncertainties. Our approach operates in a post-hoc manner, preserving the original mean predictions of the pre-trained neural network and thereby avoiding the underfitting issues commonly encountered in previous methods. We propose two methods. The first, GAPA-Free, employs empirical kernel learning from the training data for the hyperparameters and is highly efficient during training. The second, GAPA-Variational, learns the hyperparameters via gradient descent on the kernels, thus affording greater flexibility. Empirical results demonstrate that GAPA-Variational outperforms the Laplace approximation on most datasets in at least one of the uncertainty quantification metrics.

</details>


### [16] [A Kernel Approach for Semi-implicit Variational Inference](https://arxiv.org/abs/2601.12023)
*Longlin Yu,Ziheng Cheng,Shiyue Zhang,Cheng Zhang*

Main category: stat.ML

TL;DR: 提出KSIVI方法，通过核方法消除SIVI中的下层优化问题，将目标简化为核斯坦差异，实现高效优化并保持理论保证


<details>
  <summary>Details</summary>
Motivation: 半隐式变分推断(SIVI)通过分层半隐式分布增强变分族的表达能力，但其密度不可计算性导致标准ELBO优化存在偏差。现有的SIVI-SM方法通过极小极大公式解决此问题，但引入了额外的下层优化问题，增加了计算复杂度

Method: 提出核半隐式变分推断(KSIVI)，利用核方法消除下层优化。在再生核希尔伯特空间中优化时，下层问题有显式解，目标简化为核斯坦差异(KSD)。利用半隐式分布的分层结构，KSD目标可通过随机梯度方法高效优化。进一步提出多层分层扩展以增强表达能力

Result: 建立了蒙特卡洛梯度估计器的方差界优化保证，推导出统计泛化界为$\tilde{\mathcal{O}}(1/\sqrt{n})$。在合成和真实世界贝叶斯推断任务上的实证结果表明KSIVI的有效性

Conclusion: KSIVI为SIVI提供了一个原则性且可处理的替代方案，通过核方法消除下层优化问题，同时保持理论保证和实际有效性，为变分推断提供了更高效的框架

Abstract: Semi-implicit variational inference (SIVI) enhances the expressiveness of variational families through hierarchical semi-implicit distributions, but the intractability of their densities makes standard ELBO-based optimization biased. Recent score-matching approaches to SIVI (SIVI-SM) address this issue via a minimax formulation, at the expense of an additional lower-level optimization problem. In this paper, we propose kernel semi-implicit variational inference (KSIVI), a principled and tractable alternative that eliminates the lower-level optimization by leveraging kernel methods. We show that when optimizing over a reproducing kernel Hilbert space, the lower-level problem admits an explicit solution, reducing the objective to the kernel Stein discrepancy (KSD). Exploiting the hierarchical structure of semi-implicit distributions, the resulting KSD objective can be efficiently optimized using stochastic gradient methods. We establish optimization guarantees via variance bounds on Monte Carlo gradient estimators and derive statistical generalization bounds of order $\tilde{\mathcal{O}}(1/\sqrt{n})$. We further introduce a multi-layer hierarchical extension that improves expressiveness while preserving tractability. Empirical results on synthetic and real-world Bayesian inference tasks demonstrate the effectiveness of KSIVI.

</details>


### [17] [On the Provable Suboptimality of Momentum SGD in Nonstationary Stochastic Optimization](https://arxiv.org/abs/2601.12238)
*Sharan Sahu,Cameron J. Hogan,Martin T. Wells*

Main category: stat.ML

TL;DR: 动量方法在非平稳环境中存在权衡：虽然能抑制梯度噪声，但会放大漂移导致的跟踪误差，在快速变化环境中性能下降。


<details>
  <summary>Details</summary>
Motivation: 动量加速在确定性优化中研究广泛，但在非平稳环境（数据分布和最优参数随时间漂移）中的行为尚未充分探索。需要分析SGD及其动量变体在动态环境中的跟踪性能。

Method: 在均匀强凸性和平滑性假设下，分析SGD、Polyak heavy-ball和Nesterov动量方法在不同步长机制下的跟踪性能。推导有限时间期望和高概率跟踪误差界，建立误差分解为三个分量：初始化瞬态项、噪声方差项和漂移跟踪滞后项。

Result: 动量方法在抑制梯度噪声的同时，会显著放大漂移导致的跟踪误差，当动量参数接近1时放大效应无界。建立了梯度变化约束下动态遗憾的极小极大下界，证明惯性惩罚是信息理论障碍而非分析假象。

Conclusion: 动量在动态环境中存在不可避免的"惯性窗口"，会从根本上降低性能。这些结果为动量在动态环境中的经验不稳定性提供了理论依据，并划定了SGD优于加速方法的精确边界。

Abstract: While momentum-based acceleration has been studied extensively in deterministic optimization problems, its behavior in nonstationary environments -- where the data distribution and optimal parameters drift over time -- remains underexplored. We analyze the tracking performance of Stochastic Gradient Descent (SGD) and its momentum variants (Polyak heavy-ball and Nesterov) under uniform strong convexity and smoothness in varying stepsize regimes. We derive finite-time bounds in expectation and with high probability for the tracking error, establishing a sharp decomposition into three components: a transient initialization term, a noise-induced variance term, and a drift-induced tracking lag. Crucially, our analysis uncovers a fundamental trade-off: while momentum can suppress gradient noise, it incurs an explicit penalty on the tracking capability. We show that momentum can substantially amplify drift-induced tracking error, with amplification that becomes unbounded as the momentum parameter approaches one, formalizing the intuition that using 'stale' gradients hinders adaptation to rapid regime shifts. Complementing these upper bounds, we establish minimax lower bounds for dynamic regret under gradient-variation constraints. These lower bounds prove that the inertia-induced penalty is not an artifact of analysis but an information-theoretic barrier: in drift-dominated regimes, momentum creates an unavoidable 'inertia window' that fundamentally degrades performance. Collectively, these results provide a definitive theoretical grounding for the empirical instability of momentum in dynamic environments and delineate the precise regime boundaries where SGD provably outperforms its accelerated counterparts.

</details>


### [18] [A Theory of Diversity for Random Matrices with Applications to In-Context Learning of Schrödinger Equations](https://arxiv.org/abs/2601.12587)
*Frank Cole,Yulong Lu,Shaurya Sehgal*

Main category: stat.ML

TL;DR: 该论文研究了随机矩阵集合具有平凡中心化的概率，并将其应用于证明transformer网络在薛定谔方程上下文学习中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究随机矩阵集合中心化性质的概率问题，为transformer神经网络在薛定谔方程上下文学习中的泛化理论提供数学基础。

Method: 针对从随机势能薛定谔算子离散化得到的随机矩阵族，分析其中心化性质的概率下界，结合样本大小N和维度d建立数学理论。

Result: 为多个随机矩阵族提供了中心化平凡性的概率下界，这些结果与机器学习理论结合，为transformer网络在薛定谔方程学习中的泛化能力提供了理论保证。

Conclusion: 随机矩阵中心化性质的概率分析为transformer神经网络在科学计算任务中的理论理解提供了重要数学工具，特别是在薛定谔方程上下文学习方面。

Abstract: We address the following question: given a collection $\{\mathbf{A}^{(1)}, \dots, \mathbf{A}^{(N)}\}$ of independent $d \times d$ random matrices drawn from a common distribution $\mathbb{P}$, what is the probability that the centralizer of $\{\mathbf{A}^{(1)}, \dots, \mathbf{A}^{(N)}\}$ is trivial? We provide lower bounds on this probability in terms of the sample size $N$ and the dimension $d$ for several families of random matrices which arise from the discretization of linear Schrödinger operators with random potentials. When combined with recent work on machine learning theory, our results provide guarantees on the generalization ability of transformer-based neural networks for in-context learning of Schrödinger equations.

</details>


### [19] [Approximate full conformal prediction in RKHS](https://arxiv.org/abs/2601.13102)
*Davidson Lova Razafindrakoto,Alain Celisse,Jérôme Lacaille*

Main category: stat.ML

TL;DR: 提出一种高效计算全保形预测置信区域紧逼近的通用策略，引入厚度概念量化逼近误差


<details>
  <summary>Details</summary>
Motivation: 全保形预测框架虽然能构建分布无关的置信预测区域，但传统方法需要训练无限多个估计器，计算上不可行，需要设计高效逼近方法

Method: 开发通用策略设计全保形预测区域的紧逼近，引入厚度概念量化逼近误差，分析依赖于损失函数和评分函数平滑性假设的理论量化

Result: 提出了可高效计算的近似置信区域，并建立了该逼近紧度的理论量化框架

Conclusion: 通过引入厚度概念和通用逼近策略，解决了全保形预测计算不可行的问题，为实际应用提供了理论保证的高效计算方法

Abstract: Full conformal prediction is a framework that implicitly formulates distribution-free confidence prediction regions for a wide range of estimators. However, a classical limitation of the full conformal framework is the computation of the confidence prediction regions, which is usually impossible since it requires training infinitely many estimators (for real-valued prediction for instance). The main purpose of the present work is to describe a generic strategy for designing a tight approximation to the full conformal prediction region that can be efficiently computed. Along with this approximate confidence region, a theoretical quantification of the tightness of this approximation is developed, depending on the smoothness assumptions on the loss and score functions. The new notion of thickness is introduced for quantifying the discrepancy between the approximate confidence region and the full conformal one.

</details>


### [20] [Empirical Risk Minimization with $f$-Divergence Regularization](https://arxiv.org/abs/2601.13191)
*Francisco Daunas,Iñaki Esnaola,Samir M. Perlaza,H. Vincent Poor*

Main category: stat.ML

TL;DR: 本文提出了f-散度正则化经验风险最小化(ERM-fDR)的解决方案，建立了该解同时满足f-散度约束下期望经验风险最小化的条件，并引入了归一化函数的概念及其数值计算方法。


<details>
  <summary>Details</summary>
Motivation: 扩展f-散度正则化经验风险最小化问题的适用范围，建立更一般的理论框架，解决现有方法对f-散度类别限制较多的问题，并提供实用的数值计算工具。

Method: 提出归一化函数作为关键数学对象，通过非线性常微分方程隐式表征，建立其性质并构造数值算法；分析不同f-散度ERM-fDR问题的结构等价性；通过变换经验风险建立理论联系。

Result: 扩展了f-散度正则化的适用范围，恢复了先前已知结果；建立了ERM-fDR解与参考测度期望经验风险的差异特征；提出了归一化函数的数值计算方法；展示了不同f函数选择对训练和测试风险的实际影响。

Conclusion: 本文为f-散度正则化经验风险最小化问题提供了完整的理论框架和实用计算工具，揭示了不同f-散度选择的理论联系和实际影响，扩展了该方法的适用范围和实用性。

Abstract: In this paper, the solution to the empirical risk minimization problem with $f$-divergence regularization (ERM-$f$DR) is presented and conditions under which the solution also serves as the solution to the minimization of the expected empirical risk subject to an $f$-divergence constraint are established. The proposed approach extends applicability to a broader class of $f$-divergences than previously reported and yields theoretical results that recover previously known results. Additionally, the difference between the expected empirical risk of the ERM-$f$DR solution and that of its reference measure is characterized, providing insights into previously studied cases of $f$-divergences. A central contribution is the introduction of the normalization function, a mathematical object that is critical in both the dual formulation and practical computation of the ERM-$f$DR solution. This work presents an implicit characterization of the normalization function as a nonlinear ordinary differential equation (ODE), establishes its key properties, and subsequently leverages them to construct a numerical algorithm for approximating the normalization factor under mild assumptions. Further analysis demonstrates structural equivalences between ERM-$f$DR problems with different $f$-divergences via transformations of the empirical risk. Finally, the proposed algorithm is used to compute the training and test risks of ERM-$f$DR solutions under different $f$-divergence regularizers. This numerical example highlights the practical implications of choosing different functions $f$ in ERM-$f$DR problems.

</details>


### [21] [Distribution-Free Confidence Ellipsoids for Ridge Regression with PAC Bounds](https://arxiv.org/abs/2601.13436)
*Szabolcs Szentpéteri,Balázs Csanád Csáji*

Main category: stat.ML

TL;DR: 本文扩展了SPS EOA算法到岭回归，推导了置信区域大小的PAC上界，并分析了正则化参数对区域大小的影响。


<details>
  <summary>Details</summary>
Motivation: 在线性参数化模型中，当输入激励不足时，最小二乘估计可能无法求解或不稳定。虽然正则化（如岭回归）可以减少方差误差，但仍需量化估计不确定性。现有的SPS EOA算法可以构建非渐近置信椭球，但需要扩展到岭回归场景。

Method: 扩展SPS EOA算法到岭回归，推导了所得置信区域大小的概率近似正确（PAC）上界。新方法明确展示了正则化参数如何影响区域大小，并在更弱的激励假设下提供了更紧的界。

Result: 提出了岭回归的SPS EOA扩展算法，获得了比先前分析更紧的PAC上界，明确揭示了正则化参数与置信区域大小的关系。仿真实验验证了正则化的实际效果。

Conclusion: 成功将SPS EOA算法扩展到岭回归，提供了量化正则化估计不确定性的有效方法，在激励不足条件下具有更好的理论保证和实际性能。

Abstract: Linearly parametrized models are widely used in control and signal processing, with the least-squares (LS) estimate being the archetypical solution. When the input is insufficiently exciting, the LS problem may be unsolvable or numerically unstable. This issue can be resolved through regularization, typically with ridge regression. Although regularized estimators reduce the variance error, it remains important to quantify their estimation uncertainty. A possible approach for linear regression is to construct confidence ellipsoids with the Sign-Perturbed Sums (SPS) ellipsoidal outer approximation (EOA) algorithm. The SPS EOA builds non-asymptotic confidence ellipsoids under the assumption that the noises are independent and symmetric about zero. This paper introduces an extension of the SPS EOA algorithm to ridge regression, and derives probably approximately correct (PAC) upper bounds for the resulting region sizes. Compared with previous analyses, our result explicitly show how the regularization parameter affects the region sizes, and provide tighter bounds under weaker excitation assumptions. Finally, the practical effect of regularization is also demonstrated via simulation experiments.

</details>


### [22] [Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs](https://arxiv.org/abs/2601.13458)
*Zihan Dong,Ruijia Wu,Linjun Zhang*

Main category: stat.ML

TL;DR: 提出PCAL方法，通过半参数推断将预算分配问题建模为单调缺失数据框架，优化标注预算在真实标签和成对偏好之间的分配，实现统计高效估计。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成伪标签越来越依赖人类偏好反馈，需要原则性的、预算意识的数据获取策略。核心问题是如何在固定标注预算下，最优分配真实标签和成对偏好的标注资源。

Method: 基于半参数推断，将预算分配问题建模为单调缺失数据框架。提出Preference-Calibrated Active Learning (PCAL)方法，学习最优数据获取策略，并开发数据分布泛函的统计高效估计器。

Result: 理论上证明了PCAL估计器的渐近最优性，并建立了关键鲁棒性保证，即使在辅助模型估计不佳时也能保持稳健性能。模拟和真实数据分析展示了方法的实际优势和优越性能。

Conclusion: 为现代AI中的预算约束学习提供了原则性和统计高效的方法框架，直接优化估计器方差而非需要闭式解，适用于广泛问题类别。

Abstract: The increasing reliance on human preference feedback to judge AI-generated pseudo labels has created a pressing need for principled, budget-conscious data acquisition strategies. We address the crucial question of how to optimally allocate a fixed annotation budget between ground-truth labels and pairwise preferences in AI. Our solution, grounded in semi-parametric inference, casts the budget allocation problem as a monotone missing data framework. Building on this formulation, we introduce Preference-Calibrated Active Learning (PCAL), a novel method that learns the optimal data acquisition strategy and develops a statistically efficient estimator for functionals of the data distribution. Theoretically, we prove the asymptotic optimality of our PCAL estimator and establish a key robustness guarantee that ensures robust performance even with poorly estimated nuisance models. Our flexible framework applies to a general class of problems, by directly optimizing the estimator's variance instead of requiring a closed-form solution. This work provides a principled and statistically efficient approach for budget-constrained learning in modern AI. Simulations and real-data analysis demonstrate the practical benefits and superior performance of our proposed method.

</details>


### [23] [Small Gradient Norm Regret for Online Convex Optimization](https://arxiv.org/abs/2601.13519)
*Wenzhi Gao,Chang He,Madeleine Udell*

Main category: stat.ML

TL;DR: 本文为在线凸优化引入了一种新的问题依赖后悔度量$G^\star$，它基于累积平方梯度范数，比现有的$L^\star$（小损失）后悔更精细，在损失函数在事后决策点附近曲率消失时可能更尖锐。


<details>
  <summary>Details</summary>
Motivation: 现有在线凸优化中的$L^\star$后悔度量虽然能提供问题依赖的保证，但在损失函数在事后最优决策点附近曲率消失时可能不够精确。需要一种更精细的后悔度量来更好地反映问题的内在难度。

Method: 提出$G^\star$后悔度量：$\sum_{t=1}^T \|\nabla \ell(x^\star)\|^2$，基于累积平方梯度范数。建立了该度量的上下界，并扩展到动态后悔和赌博机设置。还改进了插值机制下随机优化算法的收敛分析。

Result: $G^\star$后悔严格细化了$L^\star$后悔，当损失函数在事后决策点附近曲率消失时，$G^\star$后悔可以任意更尖锐。实验验证了理论发现。

Conclusion: $G^\star$后悔为在线凸优化提供了一个更精细的问题依赖后悔度量，能更好地反映问题的内在难度，特别是在损失函数曲率消失的情况下，并改进了现有分析框架。

Abstract: This paper introduces a new problem-dependent regret measure for online convex optimization with smooth losses. The notion, which we call the $G^\star$ regret, depends on the cumulative squared gradient norm evaluated at the decision in hindsight $\sum_{t=1}^T \|\nabla \ell(x^\star)\|^2$. We show that the $G^\star$ regret strictly refines the existing $L^\star$ (small loss) regret, and that it can be arbitrarily sharper when the losses have vanishing curvature around the hindsight decision. We establish upper and lower bounds on the $G^\star$ regret and extend our results to dynamic regret and bandit settings. As a byproduct, we refine the existing convergence analysis of stochastic optimization algorithms in the interpolation regime. Some experiments validate our theoretical findings.

</details>


### [24] [Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning](https://arxiv.org/abs/2601.13642)
*Yuchen Jiao,Jiin Woo,Gen Li,Gauri Joshi,Yuejie Chi*

Main category: stat.ML

TL;DR: 本文研究了平均奖励MDPs的Q学习算法，在单智能体和联邦学习场景下提供了理论保证，显著改进了样本复杂度并首次建立了联邦Q学习算法。


<details>
  <summary>Details</summary>
Motivation: 虽然Q学习在折扣和有限时域MDPs中已有成熟的样本复杂度理论，但在平均奖励设置下的理论保证仍然有限。平均奖励强化学习为长期决策提供了原则性框架，但缺乏理论分析。

Method: 研究了一个简单但有效的Q学习算法，用于具有有限状态和动作空间的平均奖励MDPs，在弱通信假设下工作。考虑了单智能体和联邦学习两种场景，通过精心选择参数来优化算法性能。

Result: 单智能体情况下，Q学习实现了$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$的样本复杂度，比先前结果至少改进$\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$倍。联邦设置中，M个智能体的协作将每个智能体的样本复杂度降低到$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$，仅需$\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$轮通信。

Conclusion: 本文首次建立了平均奖励MDPs的联邦Q学习算法，在样本复杂度和通信复杂度方面都提供了可证明的效率保证，填补了平均奖励强化学习理论分析的空白。

Abstract: Average-reward reinforcement learning offers a principled framework for long-term decision-making by maximizing the mean reward per time step. Although Q-learning is a widely used model-free algorithm with established sample complexity in discounted and finite-horizon Markov decision processes (MDPs), its theoretical guarantees for average-reward settings remain limited. This work studies a simple but effective Q-learning algorithm for average-reward MDPs with finite state and action spaces under the weakly communicating assumption, covering both single-agent and federated scenarios. For the single-agent case, we show that Q-learning with carefully chosen parameters achieves sample complexity $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$, where $\|h^{\star}\|_{\mathsf{sp}}$ is the span norm of the bias function, improving previous results by at least a factor of $\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$. In the federated setting with $M$ agents, we prove that collaboration reduces the per-agent sample complexity to $\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$, with only $\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$ communication rounds required. These results establish the first federated Q-learning algorithm for average-reward MDPs, with provable efficiency in both sample and communication complexity.

</details>


### [25] [Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses](https://arxiv.org/abs/2601.13874)
*Shijie Zhong,Jiangfeng Fu,Yikun Yang*

Main category: stat.ML

TL;DR: 提出了一种基于U统计量和Hoeffding分解的最大均值差异(MMD)方差统一有限样本表征方法，并在拉普拉斯核下单变量情况下实现了从O(n²)到O(n log n)的精确加速计算。


<details>
  <summary>Details</summary>
Motivation: MMD作为基于核的非参数两样本检验统计量，其推断准确性严重依赖于方差表征。现有方法在不同假设（零假设与备择假设）和不同样本配置（平衡或不平衡采样）下提供了各种有限样本方差估计器，缺乏统一框架。

Method: 通过MMD统计量的U统计量表示和Hoeffding分解来研究其方差，建立了覆盖不同假设和样本配置的统一有限样本表征。基于此分析，针对拉普拉斯核下的单变量情况，提出了一种精确加速方法。

Result: 建立了MMD方差的统一有限样本表征框架，并在单变量拉普拉斯核情况下实现了计算复杂度从O(n²)到O(n log n)的显著降低。

Conclusion: 该研究为MMD方差提供了统一的理论框架，并展示了在特定核函数下实现计算加速的可行性，为高效的两样本检验提供了理论基础和实用工具。

Abstract: The maximum mean discrepancy (MMD) is a kernel-based nonparametric statistic for two-sample testing, whose inferential accuracy depends critically on variance characterization. Existing work provides various finite-sample estimators of the MMD variance, often differing under the null and alternative hypotheses and across balanced or imbalanced sampling schemes. In this paper, we study the variance of the MMD statistic through its U-statistic representation and Hoeffding decomposition, and establish a unified finite-sample characterization covering different hypotheses and sample configurations. Building on this analysis, we propose an exact acceleration method for the univariate case under the Laplacian kernel, which reduces the overall computational complexity from $\mathcal O(n^2)$ to $\mathcal O(n \log n)$.

</details>


### [26] [Intermittent time series forecasting: local vs global models](https://arxiv.org/abs/2601.14031)
*Stefano Damato,Nicolò Rubattu,Dario Azzimonti,Giorgio Corani*

Main category: stat.ML

TL;DR: 该研究首次系统比较了局部模型(iETS、TweedieGP)和全局模型(D-Linear、DeepAR、Transformers)在间歇性时间序列预测上的表现，发现D-Linear在神经网络模型中表现最佳，且优于局部模型。


<details>
  <summary>Details</summary>
Motivation: 间歇性时间序列（包含大量零值）在供应链库存中占比很大，需要概率预测来规划库存水平。虽然全局模型在时间序列预测中越来越流行，但尚未在间歇性时间序列上得到充分测试。

Method: 研究比较了局部模型(iETS、TweedieGP)和基于神经网络的全局模型(D-Linear、DeepAR、Transformers)。对神经网络模型使用了三种适合间歇性时间序列的分布头：负二项分布、障碍移位负二项分布和Tweedie分布。在包含超过40,000个真实世界时间序列的五个大数据集上进行实验。

Result: 在神经网络模型中，D-Linear提供最佳准确性，且始终优于局部模型，同时计算需求较低。基于Transformer的架构计算需求更高且准确性较差。在分布头中，Tweedie对最高分位数估计最好，而负二项分布整体表现最佳。

Conclusion: D-Linear是间歇性时间序列预测的有效选择，既准确又高效。Tweedie分布头适合估计极端分位数，而负二项分布提供最佳整体性能。全局模型在间歇性时间序列预测中具有潜力。

Abstract: Intermittent time series, characterised by the presence of a significant amount of zeros, constitute a large percentage of inventory items in supply chain. Probabilistic forecasts are needed to plan the inventory levels; the predictive distribution should cover non-negative values, have a mass in zero and a long upper tail. Intermittent time series are commonly forecast using local models, which are trained individually on each time series. In the last years global models, which are trained on a large collection of time series, have become popular for time series forecasting. Global models are often based on neural networks. However, they have not yet been exhaustively tested on intermittent time series. We carry out the first study comparing state-of-the-art local (iETS, TweedieGP) and global models (D-Linear, DeepAR, Transformers) on intermittent time series. For neural networks models we consider three different distribution heads suitable for intermittent time series: negative binomial, hurdle-shifted negative binomial and Tweedie. We use, for the first time, the last two distribution heads with neural networks. We perform experiments on five large datasets comprising more than 40'000 real-world time series. Among neural networks D-Linear provides best accuracy; it also consistently outperforms the local models. Moreover, it has also low computational requirements. Transformers-based architectures are instead much more computationally demanding and less accurate. Among the distribution heads, the Tweedie provides the best estimates of the highest quantiles, while the negative binomial offers overall the best performance.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [27] [Identifying Conditions Favouring Multiplicative Heterogeneity Models in Network Meta-Analysis](https://arxiv.org/abs/2601.11735)
*Xinlei Xu,Caitlin H Daly,Audrey Béliveau*

Main category: stat.ME

TL;DR: 该论文比较了网络荟萃分析中的两种异质性建模方法：传统的加性随机效应模型和较少探索的乘性效应模型，发现乘性效应模型在模型拟合和稳健性方面表现相当或更好。


<details>
  <summary>Details</summary>
Motivation: 网络荟萃分析中，明确建模研究间异质性对于确保有效推断和避免过度精确至关重要。虽然加性随机效应模型是传统方法，但乘性效应模型仍然研究不足，需要比较这两种方法的实际表现。

Method: 使用加权最小二乘法估计一个共同因子来膨胀研究内方差，通过加权最小二乘估计一个共同因子来膨胀研究内方差，产生与固定效应模型相同的点估计但膨胀置信区间。从nma数据库中选择具有显著异质性的双臂研究网络荟萃分析，使用赤池信息准则评估模型拟合。

Result: 乘性效应模型通常提供与随机效应模型相当或更好的拟合。案例研究进一步显示，随机效应模型对极端和不精确的观察结果敏感，而乘性效应模型对此类观察结果赋予较小权重，因此对发表偏倚表现出更大的稳健性。

Conclusion: 乘性效应模型在网络荟萃分析实践中值得与传统的随机效应模型一起考虑，特别是在存在极端或不精确观察结果的情况下。

Abstract: Explicit modelling of between-study heterogeneity is essential in network meta-analysis (NMA) to ensure valid inference and avoid overstating precision. While the additive random-effects (RE) model is the conventional approach, the multiplicative-effect (ME) model remains underexplored. The ME model inflates within-study variances by a common factor estimated via weighted least squares, yielding identical point estimates to a fixed-effect model while inflating confidence intervals. We empirically compared RE and ME models across NMAs of two-arm studies with significant heterogeneity from the nmadb database, assessing model fit using the Akaike Information Criterion. The ME model often provided comparable or better fit to the RE model. Case studies further revealed that RE models are sensitive to extreme and imprecise observations, whereas ME models assign less weight to such observations and hence exhibit greater robustness to publication bias. Our results suggest that the ME model warrant consideration alongside conventional RE model in NMA practice.

</details>


### [28] [On Nonasymptotic Confidence Intervals for Treatment Effects in Randomized Experiments](https://arxiv.org/abs/2601.11744)
*Ricardo J. Sandoval,Sivaraman Balakrishnan,Avi Feller,Michael I. Jordan,Ian Waudby-Smith*

Main category: stat.ME

TL;DR: 提出了具有与渐近置信区间相同有效样本量的非渐近置信区间，通过利用负依赖性或方差自适应性来缩小现有非渐近置信区间的性能差距


<details>
  <summary>Details</summary>
Motivation: 现有文献中，非渐近置信区间的有效样本量通常比基于中心极限定理的置信区间宽松一个与倾向得分平方根相关的因子，这种性能差距需要被消除

Method: 通过系统性地利用负依赖性（negative dependence）或方差自适应性（variance adaptivity），或两者结合，设计新的非渐近置信区间

Result: 设计了具有与渐近置信区间相同有效样本量的非渐近置信区间，并且证明所达到的非渐近速率在信息论意义上是不可改进的

Conclusion: 非渐近置信区间的性能差距可以被消除，通过利用负依赖性和方差自适应性可以实现与渐近置信区间相同的有效样本量，且这种速率是最优的

Abstract: We study nonasymptotic (finite-sample) confidence intervals for treatment effects in randomized experiments. In the existing literature, the effective sample sizes of nonasymptotic confidence intervals tend to be looser than the corresponding central-limit-theorem-based confidence intervals by a factor depending on the square root of the propensity score. We show that this performance gap can be closed, designing nonasymptotic confidence intervals that have the same effective sample size as their asymptotic counterparts. Our approach involves systematic exploitation of negative dependence or variance adaptivity (or both). We also show that the nonasymptotic rates that we achieve are unimprovable in an information-theoretic sense.

</details>


### [29] [Estimations of Extreme CoVaR and CoES under Asymptotic Independence](https://arxiv.org/abs/2601.12031)
*Qingzhao Zhong*

Main category: stat.ME

TL;DR: 该论文研究极端条件下CoVaR和CoES的估计方法，针对渐近独立但正相关的随机变量，提出两种外推方法，并通过模拟和实证分析验证其有效性。


<details>
  <summary>Details</summary>
Motivation: CoVaR和CoES作为系统性风险度量在经济学和金融学中应用日益广泛，但现有方法在极端条件下的估计存在挑战，特别是当变量渐近独立但正相关时。

Method: 提出两种外推方法：1) 通过调整因子将中间VaR外推到极端CoVaR/CoES；2) 直接将估计的中间CoVaR/CoES外推到极端尾部。所有估计量都证明具有渐近正态性。

Result: 通过蒙特卡洛模拟和实证分析（S&P500指数及12只成分股数据）验证了方法的有效性，所有估计量都表现出良好的统计性质。

Conclusion: 提出的两种外推方法能够有效估计极端条件下的CoVaR和CoES，为系统性风险度量提供了新的工具，特别是在变量渐近独立但正相关的复杂情况下。

Abstract: The two popular systemic risk measures CoVaR (Conditional Value-at-Risk) and CoES (Conditional Expected Shortfall) have recently been receiving growing attention on applications in economics and finance. In this paper, we study the estimations of extreme CoVaR and CoES when the two random variables are asymptotic independent but positively associated. We propose two types of extrapolative approaches: the first relies on intermediate VaR and extrapolates it to extreme CoVaR/CoES via an adjustment factor; the second directly extrapolates the estimated intermediate CoVaR/CoES to the extreme tails. All estimators, including both intermediate and extreme ones, are shown to be asymptotically normal. Finally, we explore the empirical performances of our methods through conducting a series of Monte Carlo simulations and a real data analysis on S&P500 Index with 12 constituent stock data.

</details>


### [30] [Lost in Aggregation: The Causal Interpretation of the IV Estimand](https://arxiv.org/abs/2601.12120)
*Danielle Tsao,Krikamol Muandet,Frederick Eberhardt,Emilija Perković*

Main category: stat.ME

TL;DR: 本文指出工具变量估计中一个被忽视的问题：当处理变量是"聚合处理变量"（由多个细粒度组分构成）时，因果效应存在模糊性，因为标准工具变量估计依赖于对聚合干预在组分层面如何具体实现的假设。


<details>
  <summary>Details</summary>
Motivation: 工具变量估计已成为社会科学和流行病学中缓解混杂偏倚的标准方法，但论证工具变量的有效性常常面临挑战。本文强调了一个在工具变量有效性论证中通常被忽视的问题：存在"聚合处理变量"的情况，即处理变量（如教育、GDP、热量摄入）由多个细粒度组分构成，每个组分可能对结果有不同的影响。

Method: 通过形式化分析，引入"聚合约束的组分干预分布"概念，展示聚合处理的因果效应通常存在模糊性，因为它取决于聚合干预在组分层面如何具体实现。然后刻画了在何种干预分布和聚合设置条件下，标准工具变量估计量能够识别聚合效应。

Result: 研究发现，聚合处理的因果效应通常是模糊的，标准工具变量估计量识别聚合效应需要满足特定条件。这些条件的刻意性质意味着基于聚合处理的工具变量估计在解释上存在重大限制。

Conclusion: 基于聚合处理的工具变量估计在解释上存在根本性限制，突显了在此类设置中需要更广泛的论证基础来支持排除限制假设。研究者需要更谨慎地解释基于聚合处理的工具变量估计结果。

Abstract: Instrumental variable based estimation of a causal effect has emerged as a standard approach to mitigate confounding bias in the social sciences and epidemiology, where conducting randomized experiments can be too costly or impossible. However, justifying the validity of the instrument often poses a significant challenge. In this work, we highlight a problem generally neglected in arguments for instrumental variable validity: the presence of an ''aggregate treatment variable'', where the treatment (e.g., education, GDP, caloric intake) is composed of finer-grained components that each may have a different effect on the outcome. We show that the causal effect of an aggregate treatment is generally ambiguous, as it depends on how interventions on the aggregate are instantiated at the component level, formalized through the aggregate-constrained component intervention distribution. We then characterize conditions on the interventional distribution and the aggregate setting under which standard instrumental variable estimators identify the aggregate effect. The contrived nature of these conditions implies major limitations on the interpretation of instrumental variable estimates based on aggregate treatments and highlights the need for a broader justificatory base for the exclusion restriction in such settings.

</details>


### [31] [Using Directed Acyclic Graphs to Illustrate Common Biases in Diagnostic Test Accuracy Studies](https://arxiv.org/abs/2601.12167)
*Yang Lu,Nandini Dendukuri*

Main category: stat.ME

TL;DR: 本文系统地将因果图(DAGs)应用于诊断准确性研究(DTA)，展示了五种主要偏倚的因果结构及其与病因学研究的对应关系。


<details>
  <summary>Details</summary>
Motivation: 诊断准确性研究(DTA)存在多种偏倚，如参考标准误差偏倚、部分验证偏倚等。虽然因果图(DAGs)在病因学研究中广泛用于识别和图示偏倚结构，但尚未系统应用于DTA研究。本文旨在填补这一空白。

Method: 开发因果图(DAGs)来图示DTA研究中常见偏倚的因果结构。针对每种偏倚，展示相应的DAG结构，并与病因学研究中等效偏倚进行对比。使用真实世界案例说明每种偏倚机制。

Result: 证明了DTA研究中的五种主要偏倚可以用DAGs表示，并与病因学研究有明确的结构对应：参考标准误差偏倚对应暴露错误分类，条件独立性错误假设产生类似未测量混杂的虚假相关，谱效应对应效应修饰，混杂通过后门路径在两个场景中起作用，部分验证偏倚对应选择偏倚。这些DAG表示揭示了每种偏倚的因果机制并提出了适当的校正策略。

Conclusion: DAGs为理解DTA研究中的偏倚结构提供了有价值的框架，应作为STARD和QUADAS-2等现有质量评估工具的补充。建议在研究设计阶段纳入DAGs以前瞻性识别潜在偏倚，在报告阶段使用以增强透明度。DAG构建需要跨学科合作和在不同因果结构下的敏感性分析。

Abstract: Background: Diagnostic test accuracy (DTA) studies, like etiological studies, are susceptible to various biases including reference standard error bias, partial verification bias, spectrum effect, confounding, and bias from misassumption of conditional independence. While directed acyclic graphs (DAGs) are widely used in etiological research to identify and illustrate bias structures, they have not been systematically applied to DTA studies. Methods: We developed DAGs to illustrate the causal structures underlying common biases in DTA studies. For each bias, we present the corresponding DAG structure and demonstrate the parallel with equivalent biases in etiological studies. We use real-world examples to illustrate each bias mechanism. Results: We demonstrate that five major biases in DTA studies can be represented using DAGs with clear structural parallels to etiological studies: reference standard error bias corresponds to exposure misclassification, misassumption of conditional independence creates spurious correlations similar to unmeasured confounding, spectrum effect parallels effect modification, confounding operates through backdoor paths in both settings, and partial verification bias mirrors selection bias. These DAG representations reveal the causal mechanisms underlying each bias and suggest appropriate correction strategies. Conclusions: DAGs provide a valuable framework for understanding bias structures in DTA studies and should complement existing quality assessment tools like STARD and QUADAS-2. We recommend incorporating DAGs during study design to prospectively identify potential biases and during reporting to enhance transparency. DAG construction requires interdisciplinary collaboration and sensitivity analyses under alternative causal structures.

</details>


### [32] [Robust semi-parametric mixtures of linear experts using the contaminated Gaussian distribution](https://arxiv.org/abs/2601.12425)
*Peterson Mambondimumwe,Sphiwe B. Skhosana,Najmeh Nakhaei Rad*

Main category: stat.ME

TL;DR: 提出半参数和非参数污染高斯混合回归模型，用于在存在温和异常值时稳健估计模型参数和非参数项，同时实现基于模型的聚类和异常值检测。


<details>
  <summary>Details</summary>
Motivation: 现有的半参数和非参数混合回归模型基于高斯误差分布假设，对异常值和重尾误差分布敏感，需要更稳健的估计方法。

Method: 提出污染高斯混合回归模型，使用EM和ECM算法进行最大似然估计和局部似然核估计，同时处理参数和非参数项。

Result: 通过广泛的模拟研究验证了所提模型的稳健性，并使用真实数据展示了其实际效用。

Conclusion: 污染高斯混合回归模型能够有效处理异常值，同时实现聚类和异常检测，为半参数和非参数混合回归提供了稳健的估计框架。

Abstract: Semi- and non-parametric mixture of regressions are a very useful flexible class of mixture of regressions in which some or all of the parameters are non-parametric functions of the covariates. These models are, however, based on the Gaussian assumption of the component error distributions. Thus, their estimation is sensitive to outliers and heavy-tailed error distributions. In this paper, we propose semi- and non-parametric contaminated Gaussian mixture of regressions to robustly estimate the parametric and/or non-parametric terms of the models in the presence of mild outliers. The virtue of using a contaminated Gaussian error distribution is that we can simultaneously perform model-based clustering of observations and model-based outlier detection. We propose two algorithms, an expectation-maximization (EM)-type algorithm and an expectation-conditional-maximization (ECM)-type algorithm, to perform maximum likelihood and local-likelihood kernel estimation of the parametric and non-parametric of the proposed models, respectively. The robustness of the proposed models is examined using an extensive simulation study. The practical utility of the proposed models is demonstrated using real data.

</details>


### [33] [Single-index Semiparametric Transformation Cure Models with Interval-censored Data](https://arxiv.org/abs/2601.12370)
*Xiaoru Huang,Tonghui Yu,Xiaoyu Liu*

Main category: stat.ME

TL;DR: 提出一种灵活的单一指标半参数转换治愈模型，用于处理区间删失数据，通过核技术和样条方法逼近函数，使用EM算法进行估计。


<details>
  <summary>Details</summary>
Motivation: 传统混合治愈模型通常假设未治愈概率服从逻辑模型，易感者服从比例风险模型，但这些参数形式假设在实践中可能不成立。需要更灵活的模型来处理区间删失数据。

Method: 提出单一指标半参数转换治愈模型：使用单一指标模型处理未治愈概率，半参数转换模型处理条件生存概率。采用核技术逼近单一指标函数，样条逼近累积基线风险函数，开发基于四层伽马脆弱泊松数据增强的EM算法。

Result: 模拟研究表明，与基于样条的方法和经典逻辑混合治愈模型相比，所提方法表现良好。应用于阿尔茨海默病数据集验证了方法的实用性。

Conclusion: 提出的灵活单一指标半参数转换治愈模型能够有效处理区间删失数据，放宽了传统模型的参数假设，为实际医学研究提供了更通用的分析工具。

Abstract: Interval censored data commonly arise in medical studies when the event time of interest is only known to lie within an interval. In the presence of a cure subgroup, conventional mixture cure models typically assume a logistic model for the uncure probability and a proportional hazards model for the susceptible subjects. However, in practice, the assumptions of parametric form for the uncure probability and the proportional hazards model for the susceptible may not always be satisfied. In this paper, we propose a class of flexible single-index semiparametric transformation cure models for interval-censored data, where a single-index model and a semiparametric transformation model are utilized for the uncured and conditional survival probability, respectively, encompassing both the proportional hazards cure and proportional odds cure models as specific cases. We approximate the single-index function and cumulative baseline hazard functions via the kernel technique and splines, respectively, and develop a computationally feasible expectation-maximisation (EM) algorithm, facilitated by a four-layer gamma-frailty Poisson data augmentation. Simulation studies demonstrate the satisfactory performance of our proposed method, compared to the spline-based approach and the classical logistic-based mixture cure models. The application of the proposed methodology is illustrated using the Alzheimers dataset.

</details>


### [34] [Modeling Zero-Inflated Longitudinal Circular Data Using Bayesian Methods: Application to Ophthalmology](https://arxiv.org/abs/2601.13998)
*Prajamitra Bhuyan,Soutik Halder,Jayant Jha*

Main category: stat.ME

TL;DR: 提出基于投影正态分布的混合效应两阶段模型，用于处理纵向研究中零膨胀的圆形数据


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏处理纵向研究中零膨胀圆形观测的模型，受实际案例研究启发需要解决这一问题

Method: 基于投影正态分布的混合效应两阶段模型，采用贝叶斯方法（Gibbs采样）进行参数估计

Result: 模拟结果显示该方法在各种情况下优于竞争对手，实际术后散光数据集验证了方法的实用性

Conclusion: 所提方法能有效处理纵向零膨胀圆形数据，有助于治疗选择和随访阶段的决策制定

Abstract: This paper introduces the modeling of circular data with excess zeros under a longitudinal framework, where the response is a circular variable and the covariates can be both linear and circular in nature. In the literature, various circular-circular and circular-linear regression models have been studied and applied to different real-world problems. However, there are no models for addressing zero-inflated circular observations in the context of longitudinal studies. Motivated by a real case study, a mixed-effects two-stage model based on the projected normal distribution is proposed to handle such issues. The interpretation of the model parameters is discussed and identifiability conditions are derived. A Bayesian methodology based on Gibbs sampling technique is developed for estimating the associated model parameters. Simulation results show that the proposed method outperforms its competitors in various situations. A real dataset on post-operative astigmatism is analyzed to demonstrate the practical implementation of the proposed methodology. The use of the proposed method facilitates effective decision-making for treatment choices and in the follow-up phases.

</details>


### [35] [Rerandomization for quantile treatment effects](https://arxiv.org/abs/2601.12540)
*Tingxuan Han,Yuhao Wang*

Main category: stat.ME

TL;DR: 该论文研究了在重随机化设计下分位数处理效应的渐近性质，建立了非高斯渐近分布理论，提出了保守方差估计和置信区间构造方法，证明了重随机化相比完全随机化能提高估计效率。


<details>
  <summary>Details</summary>
Motivation: 虽然完全随机化被认为是因果推断的金标准，但在有限样本中协变量不平衡仍可能偶然发生。重随机化能改善处理组间的协变量平衡，提高因果效应估计精度。现有研究主要关注平均处理效应，而分位数处理效应能更丰富地刻画处理异质性，捕捉结果变量的分布变化，这对政策评估和公平导向研究至关重要。

Method: 在有限总体框架下，建立重随机化设计中分位数处理效应估计量的渐近性质，不施加任何协变量或结果的分布或建模假设。估计量呈现非高斯渐近分布，表现为高斯和截断高斯随机变量的线性组合。提出保守方差估计量并构建相应的置信区间。

Result: 理论分析表明，在温和的正则条件下，重随机化相比完全随机化能提高效率。模拟研究进一步支持理论发现，并展示了重随机化在分位数处理效应估计中的实际优势。

Conclusion: 重随机化为分位数处理效应估计提供了有效的设计工具，通过改善协变量平衡来提高估计精度，同时建立了相应的渐近理论和推断方法，为政策评估和公平研究提供了更可靠的分析框架。

Abstract: Although complete randomization is widely regarded as the gold standard for causal inference, covariate imbalance can still arise by chance in finite samples. Rerandomization has emerged as an effective tool to improve covariate balance across treatment groups and enhance the precision of causal effect estimation. While existing work focuses on average treatment effects, quantile treatment effects (QTEs) provide a richer characterization of treatment heterogeneity by capturing distributional shifts in outcomes, which is crucial for policy evaluation and equity-oriented research. In this article, we establish the asymptotic properties of the QTE estimator under rerandomization within a finite-population framework, without imposing any distributional or modeling assumptions on the covariates or outcomes.The estimator exhibits a non-Gaussian asymptotic distribution, represented as a linear combination of Gaussian and truncated Gaussian random variables. To facilitate inference, we propose a conservative variance estimator and construct corresponding confidence interval. Our theoretical analysis demonstrates that rerandomization improves efficiency over complete randomization under mild regularity conditions. Simulation studies further support the theoretical findings and illustrate the practical advantages of rerandomization for QTE estimation.

</details>


### [36] [Optimal estimation of generalized causal effects in cluster-randomized trials with multiple outcomes](https://arxiv.org/abs/2601.13428)
*Xinyuan Chen,Fan Li*

Main category: stat.ME

TL;DR: 提出一个统一的潜在结果框架，用于分析聚类随机试验中的多结局治疗效应，通过加权聚类U统计量进行非参数估计，并整合去偏机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 聚类随机试验中常收集多个结局指标来全面评估治疗效果，但现有方法要么关注单一结局，要么依赖模型假设且因果解释不明确，缺乏稳健的多结局分析方法。

Method: 建立统一的潜在结果框架，定义聚类对和个体对因果估计量，使用灵活的成对对比函数，通过加权聚类U统计量进行非参数估计，结合去偏机器学习构建协变量调整估计器。

Result: 提出的估计器具有一致性和渐近正态性，在温和正则条件下达到半参数效率界，方差估计器在交叉验证下保持一致性，模拟研究和慢性疼痛管理试验应用验证了实用性。

Conclusion: 该方法为聚类随机试验中的多结局分析提供了统一、稳健且高效的因果推断框架，能够处理非优先和优先结局设置，并考虑信息性聚类大小的影响。

Abstract: Cluster-randomized trials (CRTs) are widely used to evaluate group-level interventions and increasingly collect multiple outcomes capturing complementary dimensions of benefit and risk. Investigators often seek a single global summary of treatment effect, yet existing methods largely focus on single-outcome estimands or rely on model-based procedures with unclear causal interpretation or limited robustness. We develop a unified potential outcomes framework for generalized treatment effects with multiple outcomes in CRTs, accommodating both non-prioritized and prioritized outcome settings. The proposed cluster-pair and individual-pair causal estimands are defined through flexible pairwise contrast functions and explicitly account for potentially informative cluster sizes. We establish nonparametric estimation via weighted clustered U-statistics and derive efficient influence functions to construct covariate-adjusted estimators that integrate debiased machine learning with U-statistics. The resulting estimators are consistent and asymptotically normal, attain the semiparametric efficiency bounds under mild regularity conditions, and have analytically tractable variance estimators that are proven to be consistent under cross-fitting. Simulations and an application to a CRT for chronic pain management illustrate the practical utility of the proposed methods.

</details>


### [37] [Quasi-Bayesian Variable Selection: Model Selection without a Model](https://arxiv.org/abs/2601.12767)
*Beniamino Hadj-Amar,Jack Jewson*

Main category: stat.ME

TL;DR: 本文提出使用模型准后验作为变量选择的原理性工具，它不需要完整似然函数，只需指定均值和方差函数，从而对模型误设具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断通过先验信念引入稀疏性并量化参数不确定性，但准确量化不确定性需要正确指定的模型。模型误设对变量选择造成问题，现有解决方案要么需要更复杂的模型（降低可解释性），要么通过偏离严格的贝叶斯不确定性量化来获得鲁棒性。

Method: 建立模型准后验作为变量选择的原理性工具。准后验不需要完整似然函数，只需指定均值和方差函数，因此对数据其他方面具有鲁棒性。当准边际似然没有闭式解时，使用拉普拉斯近似来提供计算可行性。

Result: 证明模型准后验具有与完整贝叶斯变量选择相似的理想性质。通过大量模拟研究表明，准后验在各种数据生成场景（包括具有重尾误差的线性模型和过度分散的计数数据）中提高了变量选择的准确性。在实际数据集（社会科学和基因组学）的应用进一步说明了该方法的实际相关性。

Conclusion: 模型准后验为变量选择提供了一个原理性工具，它结合了贝叶斯推断的理想性质（如不确定性量化）和对模型误设的鲁棒性，同时保持了原始变量选择任务的可解释性。

Abstract: Bayesian inference offers a powerful framework for variable selection by incorporating sparsity through prior beliefs and quantifying uncertainty about parameters, leading to consistent procedures with good finite-sample performance. However, accurately quantifying uncertainty requires a correctly specified model, and there is increasing awareness of the problems that model misspecification causes for variable selection. Current solutions to this problem either require a more complex model, detracting from the interpretability of the original variable selection task, or gain robustness by moving outside of rigorous Bayesian uncertainty quantification. This paper establishes the model quasi-posterior as a principled tool for variable selection. We prove that the model quasi-posterior shares many of the desirable properties of full Bayesian variable selection, but no longer necessitates a full likelihood specification. Instead, the quasi-posterior only requires the specification of mean and variance functions, and as a result, is robust to other aspects of the data. Laplace approximations are used to approximate the quasi-marginal likelihood when it is not available in closed form to provide computational tractability. We demonstrate through extensive simulation studies that the quasi-posterior improves variable selection accuracy across a range of data-generating scenarios, including linear models with heavy-tailed errors and overdispersed count data. We further illustrate the practical relevance of the proposed approach through applications to real datasets from social science and genomics

</details>


### [38] [Guidance for Addressing Individual Time Effects in Cohort Stepped Wedge Cluster Randomized Trials: A Simulation Study](https://arxiv.org/abs/2601.12930)
*Jale Basten,Katja Ickstadt,Nina Timmesfeld*

Main category: stat.ME

TL;DR: 模拟研究表明，在队列设计的阶梯楔形整群随机试验中，包含固定分类时间效应和随机群组/个体效应的线性混合模型能提供无偏干预效应估计，但需要使用群组稳健方差估计器来控制标准误。


<details>
  <summary>Details</summary>
Motivation: 队列设计的阶梯楔形整群随机试验需要考虑时间效应、群组水平变化和个体水平变化（如年龄增长）对干预效应估计的影响，但目前缺乏对这些个体水平时间变化的系统评估。

Method: 通过蒙特卡洛模拟分析不同时间效应对队列SW-CRTs干预效应估计的影响，比较四种具有不同调整策略的线性混合模型，所有模型都包含群组和重复测量的随机截距，记录固定干预效应估计值及其模型标准误。

Result: 包含固定分类时间效应、固定干预效应和两个随机截距的模型在封闭和开放队列SW-CRTs中都能提供无偏的干预效应估计。固定分类时间效应捕捉时间队列变化，随机个体效应解释基线差异，但群组稳健方差估计器对于控制I类错误率是必要的。

Conclusion: 这是首个评估队列SW-CRTs中个体水平时间变化的研究，包含固定分类时间效应和随机群组/个体效应的线性混合模型能提供无偏干预效应估计，但必须使用群组稳健方差估计器，特别是当时间变化变量呈现非线性效应时。

Abstract: Background: Stepped wedge cluster randomized trials (SW-CRTs) involve sequential measurements within clusters over time. Initially, all clusters start in the control condition before crossing over to the intervention on a staggered schedule. In cohort designs, secular trends, cluster-level changes, and individual-level changes (e.g., aging) must be considered. Methods: We performed a Monte Carlo simulation to analyze the influence of different time effects on the estimation of the intervention effect in cohort SW-CRTs. We compared four linear mixed models with different adjustment strategies, all including random intercepts for clustering and repeated measurements. We recorded the estimated fixed intervention effects and their corresponding model-based standard errors, derived from models both without and with cluster-robust variance estimators (CRVEs). Results: Models incorporating fixed categorical time effects, a fixed intervention effect, and two random intercepts provided unbiased estimates of the intervention effect in both closed and open cohort SW-CRTs. Fixed categorical time effects captured temporal cohort changes, while random individual effects accounted for baseline differences. However, these differences can cause large, non-normally distributed random individual effects. CRVEs provide reliable standard errors for the intervention effect, controlling the Type I error rate. Conclusions: Our simulation study is the first to assess individual-level changes over time in cohort SW-CRTs. Linear mixed models incorporating fixed categorical time effects and random cluster and individual effects yield unbiased intervention effect estimates. However, cluster-robust variance estimation is necessary when time-varying independent variables exhibit nonlinear effects. We recommend always using CRVEs.

</details>


### [39] [Propensity Score Propagation: A General Framework for Design-Based Inference with Unknown Propensity Scores](https://arxiv.org/abs/2601.13150)
*Siyu Heng,Yanxin Shen,Zijian Guo*

Main category: stat.ME

TL;DR: 提出倾向得分传播框架，解决设计推断中倾向得分未知时的有效推断问题，避免传统方法导致的系统性覆盖不足。


<details>
  <summary>Details</summary>
Motivation: 设计推断框架在倾向得分未知时（如观察性研究、真实世界调查、缺失数据场景）难以应用，现有方法要么忽略估计倾向得分的不确定性，要么依赖通常被违反的均匀倾向条件，导致系统性覆盖不足。

Method: 提出倾向得分传播框架，通过再生与联合过程自动将倾向得分估计的不确定性传播到下游设计推断中，支持参数和非参数倾向得分模型，与已知倾向得分的标准设计推断工具无缝集成。

Result: 模拟研究表明，所提框架在传统方法存在严重覆盖不足的场景中恢复了名义覆盖水平。

Conclusion: 倾向得分传播框架为倾向得分未知的设计推断问题提供了通用解决方案，适用于观察性研究、真实世界调查、缺失数据分析等多种重要场景。

Abstract: Design-based inference, also known as randomization-based or finite-population inference, provides a principled framework for causal and descriptive analyses that attribute randomness solely to the design mechanism (e.g., treatment assignment, sampling, or missingness) without imposing distributional or modeling assumptions on the outcome data of study units. Despite its conceptual appeal and long history, this framework becomes challenging to apply when the underlying design probabilities (i.e., propensity scores) are unknown, as is common in observational studies, real-world surveys, and missing-data settings. Existing plug-in or matching-based approaches either ignore the uncertainty stemming from estimated propensity scores or rely on the post-matching uniform-propensity condition (an assumption typically violated when there are multiple or continuous covariates), leading to systematic under-coverage. Finite-population M-estimation partially mitigates these issues but remains limited to parametric propensity score models. In this work, we introduce propensity score propagation, a general framework for valid design-based inference with unknown propensity scores. The framework introduces a regeneration-and-union procedure that automatically propagates uncertainty in propensity score estimation into downstream design-based inference. It accommodates both parametric and nonparametric propensity score models, integrates seamlessly with standard tools in design-based inference with known propensity scores, and is universally applicable to various important design-based inference problems, such as observational studies, real-world surveys, and missing-data analyses, among many others. Simulation studies demonstrate that the proposed framework restores nominal coverage levels in settings where conventional methods suffer from severe under-coverage.

</details>


### [40] [Resampling-free Inference for Time Series via RKHS Embedding](https://arxiv.org/abs/2601.13468)
*Deep Ghoshal,Xiaofeng Shao*

Main category: stat.ME

TL;DR: 提出一种基于核方法的非参数检验框架，用于多元或函数时间序列分析，通过样本分割、投影和自归一化技术构建检验统计量，具有枢轴极限分布和计算高效性。


<details>
  <summary>Details</summary>
Motivation: 现有文献中大多数方法使用带宽依赖的自举或子抽样方法，这些方法计算成本高且对带宽选择敏感。本文旨在克服这些限制。

Method: 将数据嵌入再生核希尔伯特空间，采用样本分割、投影和自归一化技术构建核基检验统计量，并通过新的条件化技术证明其极限分布。

Result: 在强混合和温和矩假设下，检验统计量具有枢轴极限零分布；分析了局部备择下的极限功效；方法在尺寸精度和计算效率上优于现有方法。

Conclusion: 提出的核基检验框架为多元和函数时间序列的非参数推断提供了计算高效、对带宽选择不敏感的有效方法。

Abstract: In this article, we study nonparametric inference problems in the context of multivariate or functional time series, including testing for goodness-of-fit, the presence of a change point in the marginal distribution, and the independence of two time series, among others. Most methodologies available in the existing literature address these problems by employing a bandwidth-dependent bootstrap or subsampling approach, which can be computationally expensive and/or sensitive to the choice of bandwidth. To address these limitations, we propose a novel class of kernel-based tests by embedding the data into a reproducing kernel Hilbert space, and construct test statistics using sample splitting, projection, and self-normalization (SN) techniques. Through a new conditioning technique, we demonstrate that our test statistics have pivotal limiting null distributions under strong mixing and mild moment assumptions. We also analyze the limiting power of our tests under local alternatives. Finally, we showcase the superior size accuracy and computational efficiency of our methods as compared to some existing ones.

</details>


### [41] [Associating High-Dimensional Longitudinal Datasets through an Efficient Cross-Covariance Decomposition](https://arxiv.org/abs/2601.13405)
*Jianbin Tan,Pixu Shi*

Main category: stat.ME

TL;DR: FACD是一个用于分析成对高维纵向数据集关联的新框架，通过功能聚合交叉协方差分解方法，自适应学习时间结构并支持特征选择。


<details>
  <summary>Details</summary>
Motivation: 高维纵向数据集（如纵向多组学研究）的关联分析面临挑战，因为复杂的时间变化交叉协方差结构和高维度使得模型构建和统计估计都很困难。现有方法通常局限于低维数据或依赖显式参数化时间动态建模。

Method: 提出FACD（功能聚合交叉协方差分解）框架，通过统计高效且理论可靠的程序，自适应学习时间结构，通过跨特征聚合信号，并自然支持变量选择以识别数据集间最相关的特征。

Result: 建立了FACD的统计保证，通过广泛的模拟研究证明了其相对于现有方法的优势，并成功应用于纵向多组学人类研究，揭示了急性运动期间跨组学层具有时间变化关联的血液分子。

Conclusion: FACD为成对高维纵向数据集的规范交叉协方差分析提供了一个有效框架，能够自适应学习时间结构并识别相关特征，在纵向多组学研究中具有实际应用价值。

Abstract: Understanding associations between paired high-dimensional longitudinal datasets is a fundamental yet challenging problem that arises across scientific domains, including longitudinal multi-omic studies. The difficulty stems from the complex, time-varying cross-covariance structure coupled with high dimensionality, which complicates both model formulation and statistical estimation. To address these challenges, we propose a new framework, termed Functional-Aggregated Cross-covariance Decomposition (FACD), tailored for canonical cross-covariance analysis between paired high-dimensional longitudinal datasets through a statistically efficient and theoretically grounded procedure. Unlike existing methods that are often limited to low-dimensional data or rely on explicit parametric modeling of temporal dynamics, FACD adaptively learns temporal structure by aggregating signals across features and naturally accommodates variable selection to identify the most relevant features associated across datasets. We establish statistical guarantees for FACD and demonstrate its advantages over existing approaches through extensive simulation studies. Finally, we apply FACD to a longitudinal multi-omic human study, revealing blood molecules with time-varying associations across omic layers during acute exercise.

</details>


### [42] [Pathway-based Bayesian factor models for gene expression data](https://arxiv.org/abs/2601.13419)
*Lorenzo Mauri,Federica Stolf,Amy H. Herring,Cameron Miller,David B. Dunson*

Main category: stat.ME

TL;DR: BASIL：一种可扩展的贝叶斯因子建模框架，将基因通路注释整合到RNA测序数据的潜在变量分析中，通过结构化先验提高生物可解释性，无需MCMC采样即可提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 传统方法如主成分分析和因子模型虽然能降维，但其潜在组分可能缺乏清晰的生物学意义。现有整合通路注释的方法存在限制性假设、需要大量超参数调优、缺乏原则性的不确定性量化等问题，影响了结果的稳健性和可重复性。

Method: 开发BASIL框架，在因子载荷上施加结构化先验，使其向注释基因集的组合收缩，增强生物学可解释性和稳定性，同时学习新的非结构化组分。采用自动经验贝叶斯程序消除手动超参数调优需求，无需计算昂贵的MCMC采样即可提供准确协方差估计和不确定性量化。

Result: 在模拟和大规模人类转录组数据集中，BASIL持续优于最先进方法，能准确重建基因-基因协方差、选择正确的潜在维度，并识别生物学上一致的模块。

Conclusion: BASIL提供了一个可扩展、稳健且可解释的框架，通过整合基因通路注释来增强RNA测序数据的潜在变量分析，解决了现有方法在可解释性、超参数调优和不确定性量化方面的局限性。

Abstract: Interpreting gene expression data requires methods that can uncover coordinated patterns corresponding to biological pathways. Traditional approaches such as principal component analysis and factor models reduce dimensionality, but latent components may have unclear biological meaning. Current approaches to incorporate pathway annotations impose restrictive assumptions, require extensive hyperparameter tuning, and do not provide principled uncertainty quantification, hindering the robustness and reproducibility of results. Here, we develop Bayesian Analysis with gene-Sets Informed Latent space (BASIL), a scalable Bayesian factor modeling framework that incorporates gene pathway annotations into latent variable analysis for RNA-sequencing data. BASIL places structured priors on factor loadings, shrinking them toward combinations of annotated gene sets, enhancing biological interpretability and stability, while simultaneously learning new unstructured components. BASIL provides accurate covariance estimates and uncertainty quantification, without resorting to computationally expensive Markov chain Monte Carlo sampling. An automatic empirical Bayes procedure eliminates the need for manual hyperparameter tuning, promoting reproducibility and usability in practice. In simulations and large-scale human transcriptomic datasets, BASIL consistently outperforms state-of-the-art approaches, accurately reconstructing gene-gene covariance, selecting the correct latent dimension, and identifying biologically coherent modules.

</details>


### [43] [Identifying Causes of Test Unfairness: Manipulability and Separability](https://arxiv.org/abs/2601.13449)
*Youmi Suk,Weicong Lyu*

Main category: stat.ME

TL;DR: 该研究提出了一种干预主义的因果框架来分析考试中的差异项目功能(DIF)，将不可操纵的群体特征分解为可干预的中间变量，从而更好地识别和理解DIF的因果来源。


<details>
  <summary>Details</summary>
Motivation: 传统DIF分析将群体成员身份作为处理变量，但这对性别、种族、英语学习者状态等不可操纵特征存在因果解释争议。需要更好的方法来识别和理解DIF的因果来源。

Method: 采用Robins和Richardson(2010)提出的干预主义方法，将不可操纵的处理分解为可干预的中间变量。定义了与这些分解成分相关的可分离DIF效应，提供了因果识别策略，并使用因果机器学习方法（因果森林和贝叶斯加性回归树）进行检测。

Result: 将该框架应用于SAT和Regents考试中的偏倚测试项目，并通过模拟研究展示了因果机器学习方法的性能。证明了干预主义方法能够更好地识别和理解DIF的因果机制。

Conclusion: 采用干预主义方法能够为教育测试实践提供更深入的洞见，有助于更准确地识别和理解考试项目对不同群体的不公平影响，从而改进测试的公平性。

Abstract: Differential item functioning (DIF) is a widely used statistical notion for identifying items that may disadvantage specific groups of test-takers. These groups are often defined by non-manipulable characteristics, e.g., gender, race/ethnicity, or English-language learner (ELL) status. While DIF can be framed as a causal fairness problem by treating group membership as the treatment variable, this invokes the long-standing controversy over the interpretation of causal effects for non-manipulable treatments. To better identify and interpret causal sources of DIF, this study leverages an interventionist approach using treatment decomposition proposed by Robins and Richardson (2010). Under this framework, we can decompose a non-manipulable treatment into intervening variables. For example, ELL status can be decomposed into English vocabulary unfamiliarity and classroom learning barriers, each of which influences the outcome through different causal pathways. We formally define separable DIF effects associated with these decomposed components, depending on the absence or presence of item impact, and provide causal identification strategies for each effect. We then apply the framework to biased test items in the SAT and Regents exams. We also provide formal detection methods using causal machine learning methods, namely causal forests and Bayesian additive regression trees, and demonstrate their performance through a simulation study. Finally, we discuss the implications of adopting interventionist approaches in educational testing practices.

</details>


### [44] [Categorical distance correlation under general encodings and its application to high-dimensional feature screening](https://arxiv.org/abs/2601.13454)
*Qingyang Zhang*

Main category: stat.ME

TL;DR: 将距离相关性扩展到具有一般编码的分类数据，利用类别间距信息提升性能，提供两种估计量及其极限分布，建立高维分类数据的确定筛选性质。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理分类数据时未能充分利用类别之间的间距信息，特别是对于有序变量，传统的one-hot编码忽略了类别间的顺序关系，限制了距离相关性的性能。

Method: 扩展距离相关性到分类数据，采用一般编码方案：名义变量用one-hot编码，有序变量用半圆编码。提供最大似然估计和偏差校正估计两种估计量，推导零假设和备择假设下的极限分布，建立高维分类数据的确定筛选性质。

Result: 提出的方法能有效利用类别间距信息提升距离相关性性能，模拟研究表明不同编码方案的效果差异，2018年综合社会调查数据的实际应用验证了方法的实用性。

Conclusion: 通过引入考虑类别间距的一般编码方案，成功扩展了距离相关性到分类数据，提供了理论保证和实用工具，为高维分类数据分析提供了有效方法。

Abstract: In this paper, we extend distance correlation to categorical data with general encodings, such as one-hot encoding for nominal variables and semicircle encoding for ordinal variables. Unlike existing methods, our approach leverages the spacing information between categories, which enhances the performance of distance correlation. Two estimates including the maximum likelihood estimate and a bias-corrected estimate are given, together with their limiting distributions under the null and alternative hypotheses. Furthermore, we establish the sure screening property for high-dimensional categorical data under mild conditions. We conduct a simulation study to compare the performance of different encodings, and illustrate their practical utility using the 2018 General Social Survey data.

</details>


### [45] [Two-stage least squares with clustered data](https://arxiv.org/abs/2601.13507)
*Anqi Zhao,Peng Ding,Fan Li*

Main category: stat.ME

TL;DR: 本文比较了处理聚类数据时两种2SLS方法：经典2SLS（仅聚类稳健标准误）与2SFE（包含聚类固定效应），在LATE框架下分析其权衡、效率差异及异质性处理。


<details>
  <summary>Details</summary>
Motivation: 商业研究中聚类数据普遍存在（如用户重复测量、企业面板数据），当估计内生处理效应时，研究者面临选择：经典2SLS（仅聚类稳健推断）还是包含聚类固定效应的2SFE。需要澄清这两种方法的权衡，为实证选择提供指导。

Method: 在局部平均处理效应（LATE）框架下，建立两种方法的理论性质：1）同质聚类下验证两种方法的有效性并比较相对效率；2）异质聚类下分析两种方法估计量的性质差异；3）发展联合渐近理论并提出检验聚类异质性的Wald型检验。

Result: 1）同质聚类下，两种方法都适用于LATE的Wald型推断，2SFE仅在聚类特定效应变异主导单位级误差变异时更有效；2）异质聚类下，2SFE恢复聚类特定LATE的加权平均，而经典2SLS通常不能；3）提出了检测聚类异质性的检验方法。

Conclusion: 为聚类数据中的因果推断提供了方法选择指导：同质聚类时可根据效率条件选择方法，异质聚类时2SFE能估计有意义的加权平均LATE，并提供了检验异质性的实用工具。

Abstract: Clustered data -- where units of observation are nested within higher-level groups, such as repeated measurements on users, or panel data of firms, industries, or geographic regions -- are ubiquitous in business research. When the objective is to estimate the causal effect of a potentially endogenous treatment, a common approach -- which we call the canonical two-stage least squares (2sls) -- is to fit a 2sls regression of the outcome on treatment status with instrumental variables (IVs) for point estimation, and apply cluster-robust standard errors to account for clustering in inference. When both the treatment and IVs vary within clusters, a natural alternative -- which we call the two-stage least squares with fixed effects (2sfe) -- is to include cluster indicators in the 2sls specification, thereby incorporating cluster information in point estimation as well. This paper clarifies the trade-off between these two approaches within the local average treatment effect (LATE) framework, and makes three contributions. First, we establish the validity of both approaches for Wald-type inference of the LATE when clusters are homogeneous, and characterize their relative efficiency. We show that, when the true outcome model includes cluster-specific effects, 2sfe is more efficient than the canonical 2sls only when the variation in cluster-specific effects dominates that in unit-level errors. Second, we show that with heterogeneous clusters, 2sfe recovers a weighted average of cluster-specific LATEs, whereas the canonical 2sls generally does not. Third, to guide empirical choice between the two procedures, we develop a joint asymptotic theory for the two estimators under homogeneous clusters, and propose a Wald-type test for detecting cluster heterogeneity.

</details>


### [46] [Post-selection inference for penalized M-estimators via score thinning](https://arxiv.org/abs/2601.13514)
*Ronan Perry,Snigdha Panigrahi,Daniela Witten*

Main category: stat.ME

TL;DR: 提出一种基于噪声注入的模型选择后推断方法，通过在数据中添加精心构造的噪声，使得在噪声数据上进行惩罚M估计模型选择后，使用标准推断方法即可获得有效的推断结果，无需定制化方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型选择后推断方法需要专门的定制化程序，过程复杂。本文旨在简化这一过程，让研究者能够在进行惩罚M估计（如glmnet）模型选择后，直接使用标准推断方法（如glm）获得有效的推断结果。

Method: 核心思想是在数据中添加精心构造的噪声，利用两个关键洞察：(1) 对具有未知均值和已知方差的高斯随机变量进行加减特定噪声，可以得到两个独立的高斯随机变量；(2) 惩罚M估计的选择事件和标准置信区间覆盖事件都可以用近似正态的"得分变量"来表征。通过选择适当的噪声，使得噪声数据上的模型选择与噪声数据上的标准推断之间具有渐近独立性。

Result: 理论证明表明，在噪声数据上进行惩罚M估计模型选择后，使用标准推断方法可以获得有效的推断结果。该方法仅要求观测独立性和较弱的分布假设。在社交网络数据中成功应用于性别与吸烟关联性的推断。

Conclusion: 本文提出了一种简单有效的模型选择后推断方法，通过在数据中添加精心构造的噪声，研究者可以直接使用标准统计软件（如R中的glmnet和glm）进行模型选择和推断，无需开发专门的定制化方法，大大简化了实际应用。

Abstract: We consider inference for M-estimators after model selection using a sparsity-inducing penalty. While existing methods for this task require bespoke inference procedures, we propose a simpler approach, which relies on two insights: (i) adding and subtracting carefully-constructed noise to a Gaussian random variable with unknown mean and known variance leads to two \emph{independent} Gaussian random variables; and (ii) both the selection event resulting from penalized M-estimation, and the event that a standard (non-selective) confidence interval for an M-estimator covers its target, can be characterized in terms of an approximately normal ``score variable". We combine these insights to show that -- when the noise is chosen carefully -- there is asymptotic independence between the model selected using a noisy penalized M-estimator, and the event that a standard (non-selective) confidence interval on noisy data covers the selected parameter. Therefore, selecting a model via penalized M-estimation (e.g. \verb=glmnet= in \verb=R=) on noisy data, and then conducting \emph{standard} inference on the selected model (e.g. \verb=glm= in \verb=R=) using noisy data, yields valid inference: \emph{no bespoke methods are required}. Our results require independence of the observations, but only weak distributional requirements. We apply the proposed approach to conduct inference on the association between sex and smoking in a social network.

</details>


### [47] [What is Overlap Weighting, How Has it Evolved, and When to Use It for Causal Inference?](https://arxiv.org/abs/2601.13535)
*Haidong Lu,Fan Li,Laine E. Thomas,Fan Li*

Main category: stat.ME

TL;DR: 本文综述了重叠加权方法在观察性研究中的应用，该方法通过强调处于经验平衡状态的个体来估计重叠人群的平均处理效应，实现协变量平衡并最小化渐近方差。


<details>
  <summary>Details</summary>
Motivation: 随着大型健康数据库的可用性增加，观察性研究在比较效果研究中的应用日益广泛。但观察性研究必须调整治疗组间的系统性差异，而倾向评分方法中的重叠加权方法因其强调处于经验平衡状态的个体而受到关注。

Method: 重叠加权是一种倾向评分方法，通过分配与接受相反治疗概率成比例的权重，针对重叠人群的平均处理效应。该方法在逻辑倾向评分模型下实现精确的协变量均值平衡，并最小化渐近方差。

Result: 过去十年中，重叠加权方法已被统计、流行病学和临床研究社区认可为有价值的混杂调整工具，并越来越多地应用于临床和健康研究中。该方法能够优先考虑处于临床平衡状态的人群，同时实现协变量平衡。

Conclusion: 本文提供了重叠加权方法最新方法学发展的简明概述，并给出了何时适合选择该方法进行因果推断的实用指导，特别是在使用观察性数据模拟随机试验的情况下。

Abstract: The growing availability of large health databases has expanded the use of observational studies for comparative effectiveness research. Unlike randomized trials, observational studies must adjust for systematic differences in patient characteristics between treatment groups. Propensity score methods, including matching, weighting, stratification, and regression adjustment, address this issue by creating groups that are comparable with respect to measured covariates. Among these approaches, overlap weighting (OW) has emerged as a principled and efficient method that emphasizes individuals at empirical equipoise, those who could plausibly receive either treatment. By assigning weights proportional to the probability of receiving the opposite treatment, OW targets the Average Treatment Effect in the Overlap population (ATO), achieves exact mean covariate balance under logistic propensity score models, and minimizes asymptotic variance. Over the last decade, the OW method has been recognized as a valuable confounding adjustment tool across the statistical, epidemiologic, and clinical research communities, and is increasingly applied in clinical and health studies. Given the growing interest in using observational data to emulate randomized trials and the capacity of OW to prioritize populations at clinical equipoise while achieving covariate balance (fundamental attributes of randomized studies), this article provides a concise overview of recent methodological developments in OW and practical guidance on when it represents a suitable choice for causal inference.

</details>


### [48] [Building a Standardised Statistical Reporting Toolbox in an Academic Oncology Clinical Trials Unit: The grstat R Package](https://arxiv.org/abs/2601.13755)
*Dan Chaltiel,Alexis Cochard,Nusaibah Ibrahimi,Charlotte Bargain,Ikram Benchara,Anne Lourdessamy,Aldéric Fraslin,Matthieu Texier,Livia Pierotti*

Main category: stat.ME

TL;DR: 开发了grstat R包，为肿瘤临床试验单元提供标准化统计工具，结合组织框架确保代码质量和可维护性


<details>
  <summary>Details</summary>
Motivation: 学术临床试验单元面临统计工作流程碎片化问题，导致重复劳动、协作有限和分析实践不一致

Method: 开发grstat R包，采用结构化组织框架：正式请求跟踪、同行评审开发、自动化测试和分阶段验证新功能

Result: 创建了开源R包，支持透明工作流程、集体代码所有权和可追溯决策，提高了效率、可重复性和长期可维护性

Conclusion: grstat展示了在学术环境中组织、验证和维护共享分析工具箱的可转移方法，可为面临类似组织挑战的其他临床试验单元提供参考

Abstract: Academic Clinical Trial Units frequently face fragmented statistical workflows, leading to duplicated effort, limited collaboration, and inconsistent analytical practices. To address these challenges within an oncology Clinical Trial Unit, we developed grstat, an R package providing a standardised set of tools for routine statistical analyses. Beyond the software itself, the development of grstat is embedded in a structured organisational framework combining formal request tracking, peer-reviewed development, automated testing, and staged validation of new functionalities. The package is intentionally opinionated, reflecting shared practices agreed upon within the unit, and evolves through iterative use in real-world projects. Its development as an open-source project on GitHub supports transparent workflows, collective code ownership, and traceable decision-making.  While primarily designed for internal use, this work illustrates a transferable approach to organising, validating, and maintaining a shared analytical toolbox in an academic setting. By coupling technical implementation with governance and validation principles, grstat supports efficiency, reproducibility, and long-term maintainability of biostatistical workflows, and may serve as a source of inspiration for other Clinical Trial Units facing similar organisational challenges.

</details>


### [49] [ChauBoxplot and AdaptiveBoxplot: two R packages for boxplot-based outlier detection](https://arxiv.org/abs/2601.13759)
*Tiejun Tong,Hongmei Lin,Bowen Gang,Riquan Zhang*

Main category: stat.ME

TL;DR: 介绍了两个新的R包ChauBoxplot和AdaptiveBoxplot，用于改进传统箱线图在样本量增大时标记过多异常值的问题，并提供选择箱线图方法的实用指导。


<details>
  <summary>Details</summary>
Motivation: 传统Tukey箱线图的固定围栏规则在样本量增大时倾向于标记过多的异常值，这限制了其在实际应用中的可靠性。

Method: 开发了两个新的R包：ChauBoxplot和AdaptiveBoxplot，实现了更稳健的异常值检测方法。通过模拟研究提供了选择合适箱线图方法的实用指导。

Result: 提出了改进的箱线图方法，能够更准确地检测异常值，特别是在大样本情况下。提供了平衡可解释性和统计可靠性的方法选择指南。

Conclusion: 新的R包和实用指导帮助实践者选择更合适的箱线图方法，解决了传统方法在大样本中标记过多异常值的问题，提高了异常值检测的可靠性。

Abstract: Tukey's boxplot is widely used for outlier detection; however, its classic fixed-fence rule tends to flag an excessive number of outliers as the sample size grows. To address this limitation, we introduce two new R packages, ChauBoxplot and AdaptiveBoxplot, which implement more robust methods for outlier detection. We also provide practical guidance, drawn from simulation results, to help practitioners choose suitable boxplot methods and balance interpretability with statistical reliability.

</details>


### [50] [An Adaptive Phase II Trial Design for Dose Selection and Addition in Microfilarial Infections](https://arxiv.org/abs/2601.13784)
*Sonja Zehetmayer,Marta Bofill Roig,Fabrice Lotola Mougeni,Sabine Specht,Marc P. Hübner,Martin Posch*

Main category: stat.ME

TL;DR: 提出一种频率主义自适应二期试验设计，用于评估三种治疗方案（剂量）与安慰剂相比对四种蠕虫感染的安全性和有效性。设计包含基于安全性和早期疗效的中期分析，可调整剂量选择。


<details>
  <summary>Details</summary>
Motivation: 该研究需要解决几个关键挑战：1) 最高剂量安全性尚未确定，试验必须从两个较低剂量开始；2) 招募速度快，适应决策需依赖早期替代终点；3) 主要结果是具有0点原子的混合分布计数变量。需要在自适应设计中控制多重比较的强族错误率。

Method: 提出频率主义自适应二期试验设计，开始时使用两个较低剂量和安慰剂。基于安全性和早期疗效的中期分析结果，决定继续使用较低剂量或引入最高剂量。设计跨篮子借用信息进行安全性评估，但每个篮子单独评估疗效。扩展部分条件错误方法以容纳中期分析后新假设的纳入。

Result: 通过全面的模拟研究评估各种设计选项和分析策略，评估设计在不同假设和参数值下的稳健性。识别出自适应设计能够提高识别最佳剂量能力的场景。

Conclusion: 自适应剂量选择能够将资源分配到最有希望的治疗组，增加选择最佳剂量的可能性，同时减少所需的总样本量和试验持续时间。

Abstract: We propose a frequentist adaptive phase 2 trial design to evaluate the safety and efficacy of three treatment regimens (doses) compared to placebo for four types of helminth (worm) infections. This trial will be carried out in four Subsaharan African countries from spring 2025. Since the safety of the highest dose is not yet established, the study begins with the two lower doses and placebo. Based on safety and early efficacy results from an interim analysis, a decision will be made to either continue with the two lower doses or drop one or both and introduce the highest dose instead. This design borrows information across baskets for safety assessment, while efficacy is assessed separately for each basket. The proposed adaptive design addresses several key challenges: (1) The trial must begin with only the two lower doses because reassuring safety data from these doses is required before escalating to a higher dose. (2) Due to the expected speed of recruitment, adaptation decisions must rely on an earlier, surrogate endpoint. (3) The primary outcome is a count variable that follows a mixture distribution with an atom at 0. To control the familywise error rate in the strong sense when comparing multiple doses to the control in the adaptive design, we extend the partial conditional error approach to accommodate the inclusion of new hypotheses after the interim analysis. In a comprehensive simulation study we evaluate various design options and analysis strategies, assessing the robustness of the design under different design assumptions and parameter values. We identify scenarios where the adaptive design improves the trial's ability to identify an optimal dose. Adaptive dose selection enables resource allocation to the most promising treatment arms, increasing the likelihood of selecting the optimal dose while reducing the required overall sample size and trial duration.

</details>


### [51] [Tail-Aware Density Forecasting of Locally Explosive Time Series: A Neural Network Approach](https://arxiv.org/abs/2601.14049)
*Elena Dumitrescu,Julien Peignon,Arthur Thomas*

Main category: stat.ME

TL;DR: 提出一种基于混合密度网络的时间序列预测方法，专门针对具有局部爆炸性行为的金融泡沫时间序列，使用偏斜t分布作为混合分量，并采用自适应加权方案提高极端区域预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法难以捕捉金融泡沫等具有局部爆炸性、偏斜、厚尾和潜在多峰特征的预测密度分布，需要更灵活的模型来准确估计极端区域的风险。

Method: 采用混合密度网络框架，使用偏斜t分布作为混合分量来建模预测密度，实现自适应加权方案在训练中强调尾部观测值，专门针对混合因果-非因果ARMA过程建模的泡沫动态。

Result: 通过蒙特卡洛模拟和天然气价格实证应用表明，该方法在预测性能上优于现有方法，能够产生近乎即时的密度预测，在极端区域提供更准确的密度估计。

Conclusion: 提出的混合密度网络框架为具有局部爆炸性行为的时间序列提供了有效的预测工具，特别适用于金融应用中需要准确估计极端风险的情况。

Abstract: This paper proposes a Mixture Density Network for forecasting time series that exhibit locally explosive behavior. By incorporating skewed t-distributions as mixture components, our approach offers enhanced flexibility in capturing the skewed, heavy-tailed, and potentially multimodal nature of predictive densities associated with bubble dynamics modeled by mixed causal-noncausal ARMA processes. In addition, we implement an adaptive weighting scheme that emphasizes tail observations during training and hence leads to accurate density estimation in the extreme regions most relevant for financial applications. Equally important, once trained, the MDN produces near-instantaneous density forecasts. Through extensive Monte Carlo simulations and an empirical application on the natural gas price, we show that the proposed MDN-based framework delivers superior forecasting performance relative to existing approaches.

</details>


### [52] [Factor Analysis of Multivariate Stochastic Volatility Model](https://arxiv.org/abs/2601.14199)
*Taehee Lee,Jun S. Liu*

Main category: stat.ME

TL;DR: 提出贝叶斯因子建模框架，用于同时推断高维时间序列的协方差结构及其时变动态，解决了现有方法在建模灵活性或推断效率方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在显著局限性：变点建模无法捕捉协方差结构的连续时变特性，而GARCH和随机波动率模型存在过度参数化和过拟合风险。需要一种既能建模高维变量时变协方差结构，又具有推断效率的方法。

Method: 提出贝叶斯因子建模框架，通过期望最大化（EM）算法进行推断。该算法具有精确的闭式M步更新，且易于推广到更复杂场景（如时空多元因子分析）。

Result: 通过模拟研究和真实数据实验（气候和金融数据集）验证了方法的有效性。

Conclusion: 该方法能够同时推断高维时间序列的协方差结构及其时变动态，解决了现有方法的局限性，具有建模灵活性和推断效率的优势。

Abstract: Modeling the time-varying covariance structures of high-dimensional variables is critical across diverse scientific and industrial applications; however, existing approaches exhibit notable limitations in either modeling flexibility or inferential efficiency. For instance, change-point modeling fails to account for the continuous time-varying nature of covariance structures, while GARCH and stochastic volatility models suffer from over-parameterization and the risk of overfitting. To address these challenges, we propose a Bayesian factor modeling framework designed to enable simultaneous inference of both the covariance structure of a high-dimensional time series and its time-varying dynamics. The associated Expectation-Maximization (EM) algorithm not only features an exact, closed-form update for the M-step but also is easily generalizable to more complex settings, such as spatiotemporal multivariate factor analysis. We validate our method through simulation studies and real-data experiments using climate and financial datasets.

</details>
