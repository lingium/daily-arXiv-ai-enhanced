<div id=toc></div>

# Table of Contents

- [stat.CO](#stat.CO) [Total: 3]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.ME](#stat.ME) [Total: 14]
- [stat.AP](#stat.AP) [Total: 2]


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1] [Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both](https://arxiv.org/abs/2512.03225)
*Christophe Andrieu,Nicolas Chopin,Ettore Fincato,Mathieu Gerber*

Main category: stat.CO

TL;DR: 研究一类迭代算法在非光滑、有噪声目标函数下的收敛性，该算法使用目标函数平滑近似的梯度进行广义梯度下降


<details>
  <summary>Details</summary>
Motivation: 研究零阶优化问题中，当目标函数可能非光滑、有噪声且只能通过黑箱获取时，如何分析基于平滑近似的梯度下降算法的收敛性

Method: 提出一个广义梯度下降递归框架，使用目标函数平滑近似的梯度，包括模型化和平滑化两种经典零阶优化方法作为特例

Result: 在目标函数正则性非常弱的假设下获得收敛结果，涉及平滑程度与参数更新步长之间的权衡，随机情况需要额外假设

Conclusion: 该框架为分析基于平滑近似的零阶优化算法提供了理论基础，并通过机器学习中的分类问题验证了算法的相关性和收敛结果

Abstract: We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning.

</details>


### [2] [Numerical optimization for the compatibility constant of the lasso](https://arxiv.org/abs/2512.03321)
*Kei Hirose*

Main category: stat.CO

TL;DR: 提出计算LASSO兼容性常数的数值方法，将复杂非线性优化问题转化为二次规划或混合整数二次规划问题，并验证其有限样本行为。


<details>
  <summary>Details</summary>
Motivation: 当变量数超过观测数时，兼容性条件及其常数常用于评估LASSO的预测误差，但计算兼容性常数通常很困难，因为它是一个复杂的非线性优化问题。

Method: 1. 在已知真实回归系数零/非零模式的情况下，将优化问题转化为二次规划问题；2. 通过求解所有可能符号组合的QP来获得兼容性常数；3. 当真实非零系数数量适中时，提出混合整数二次规划方法。

Result: 1. 成功将兼容性常数计算问题转化为可求解的优化问题；2. 通过模拟数据在各种参数设置下研究了兼容性常数的有限样本行为；3. 将均方误差与基于兼容性常数的理论误差界进行比较；4. 通过真实数据分析验证了兼容性常数在有限样本中的行为。

Conclusion: 本文提出了计算LASSO兼容性常数的有效数值方法，将原本复杂的非线性优化问题转化为可计算的二次规划或混合整数二次规划问题，为实际应用中评估LASSO预测误差提供了实用工具。

Abstract: Compatibility condition and compatibility constant have been commonly used to evaluate the prediction error of the lasso when the number of variables exceeds the number of observations. However, the computation of the compatibility constant is generally difficult because it is a complicated nonlinear optimization problem. In this study, we present a numerical approach to compute the compatibility constant when the zero/nonzero pattern of true regression coefficients is given. We show that the optimization problem reduces to a quadratic program (QP) once the signs of the nonzero coefficients are specified. In this case, the compatibility constant can be obtained by solving QPs for all possible sign combinations. We also formulate a mixed-integer quadratic programming (MIQP) approach that can be applied when the number of true nonzero coefficients is moderately large. We investigate the finite-sample behavior of the compatibility constant for simulated data under a wide variety of parameter settings and compare the mean squared error with its theoretical error bound based on the compatibility constant. The behavior of the compatibility constant in finite samples is also investigated through a real data analysis.

</details>


### [3] [SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models](https://arxiv.org/abs/2512.03322)
*Geoffrey J. McLachlan,Jinran Wu*

Main category: stat.CO

TL;DR: SSLfmm R包通过有限混合模型估计贝叶斯分类器，结合MCAR和MAR缺失机制，利用标签缺失信息提高分类效率。


<details>
  <summary>Details</summary>
Motivation: 半监督学习中标签缺失通常被忽略或假设为完全随机缺失，但实际上缺失过程可能包含信息（如特征模糊性影响标注概率）。利用这些缺失指示信息可以提升估计效率。

Method: 使用有限混合模型估计贝叶斯分类器，每个类别对应多元正态分布。采用混合缺失机制：MCAR成分 + 基于特征熵的逻辑函数MAR成分。通过期望-条件最大化算法估计参数。

Result: 在两类高斯设置中，部分标注数据训练的分类器有时能达到比全标注监督版本更低的误分类率。包提供实用工具并通过模拟示例展示性能。

Conclusion: SSLfmm包通过建模信息性缺失机制，在半监督学习中有效利用标签缺失信息，提升分类器性能，特别适用于标签缺失与特征模糊性相关的情况。

Abstract: Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The \textbf{SSLfmm} package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [4] [A note on the impossibility of conditional PAC-efficient reasoning in large language models](https://arxiv.org/abs/2512.03057)
*Hao Zeng*

Main category: stat.ML

TL;DR: 证明在非原子输入空间中，条件PAC效率推理是不可能的，任何算法都必须以至少1-α的概率依赖专家模型


<details>
  <summary>Details</summary>
Motivation: 最近的研究建立了组合模型（在昂贵专家模型和廉价快速模型之间切换）的边际PAC效率保证，但需要探究条件（逐点）保证是否可能

Method: 通过理论证明，在分布无关设置下，对非原子输入空间进行条件PAC效率分析

Result: 证明条件PAC效率保证是不可能的，任何实现条件PAC效率的算法都必须是平凡的，即对于几乎所有输入，必须以至少1-α的概率依赖专家模型

Conclusion: 在分布无关设置中，条件PAC效率推理是不可能的，这为大型语言模型的高效推理提供了重要的理论限制

Abstract: We prove an impossibility result for conditional Probably Approximately Correct (PAC)-efficient reasoning in large language models. While recent work has established marginal PAC efficiency guarantees for composite models that switch between expensive expert models and cheaper fast models, we show that conditional (pointwise) guarantees are impossible in the distribution-free setting. Specifically, for non-atomic input spaces, any algorithm achieving conditional PAC efficiency must be trivial in the sense that it defers to the expert model with probability at least $1-α$ for almost every input.

</details>


### [5] [Novelty detection on path space](https://arxiv.org/abs/2512.03243)
*Ioannis Gasteratos,Antoine Jacquier,Maud Lemercier,Terry Lyons,Cristopher Salvi*

Main category: stat.ML

TL;DR: 本文提出了一种基于路径空间签名的异常检测方法，通过假设检验框架和运输成本不等式获得错误率边界，推导出条件风险价值（CVaR）的精确公式，并开发了新的单类SVM算法，最后通过数值实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在路径空间上进行异常检测是一个重要问题，需要开发能够处理复杂路径数据的统计方法。现有的方法在处理非高斯分布和计算风险度量方面存在局限性，需要更通用的理论框架和算法。

Method: 1. 将路径空间异常检测建模为假设检验问题，使用基于签名的检验统计量
2. 利用Gasteratos和Jacquier（2023）的运输成本不等式，获得超越高斯分布的尾概率边界
3. 利用洗牌积推导条件风险价值（CVaR）的精确公式，开发新的单类SVM算法
4. 建立第二类错误的下界，给出一般功效边界
5. 使用合成异常扩散数据和真实分子生物学数据进行数值评估

Result: 1. 获得了RDE解定律的尾概率边界，能够估计分位数和p值
2. 推导出平滑CVaR替代函数的精确公式，基于期望签名
3. 开发了优化平滑CVaR目标的新单类SVM算法
4. 建立了第二类错误的下界，提供了参考测度与备择测度绝对连续时的功效边界
5. 数值实验验证了基于签名的检验统计量的第一类错误和统计功效

Conclusion: 本文提出了一个基于签名的路径空间异常检测框架，结合运输成本不等式和风险度量理论，提供了严格的统计保证和实用的算法实现。该方法能够处理非高斯分布，推导出CVaR的精确公式，并在合成和真实数据上展示了良好的性能。

Abstract: We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type-$\mathrm{II}$ error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type-$\mathrm{I}$ error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data.

</details>


### [6] [Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback](https://arxiv.org/abs/2512.03208)
*Pangpang Liu,Junwei Lu,Will Wei Sun*

Main category: stat.ML

TL;DR: 该论文研究用于对齐大语言模型的奖励模型的估计与统计推断，提出处理人类反馈异质性的方法，建立理论保证，并应用于不确定性量化和最佳策略选择。


<details>
  <summary>Details</summary>
Motivation: 人类反馈具有内在异质性，这给可靠的奖励学习带来了重大挑战。在LLM对齐中，人类偏好数据的不一致性使得准确估计奖励模型变得困难，需要处理这种异质性并量化不确定性。

Method: 采用异质性偏好框架，联合建模答案的潜在奖励和人类理性，形成双凸优化问题，通过交替梯度下降算法求解。建立估计器的理论保证，包括收敛性和渐近分布，从而构建置信区间。

Result: 建立了估计器的理论保证，能够构建奖励估计的置信区间。利用不确定性量化结果进行有效的统计比较，并将不确定性纳入最佳N策略框架。模拟实验验证了方法的有效性，实际LLM数据应用显示了考虑不确定性的实用价值。

Conclusion: 提出的异质性偏好框架和统计推断方法能够有效处理人类反馈的异质性，为LLM对齐中的奖励建模提供了可靠的不确定性量化工具，提高了奖励估计的可靠性和策略选择的稳健性。

Abstract: We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of-$N$ (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.

</details>


### [7] [Iterative Tilting for Diffusion Fine-Tuning](https://arxiv.org/abs/2512.03234)
*Jean Pachebat,Giovanni Conforti,Alain Durmus,Yazid Janati*

Main category: stat.ML

TL;DR: 提出迭代倾斜法，一种无需梯度的扩散模型微调方法，可将大奖励倾斜分解为多个小倾斜，仅需前向评估奖励函数。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要反向传播通过采样链，计算成本高。需要一种无需梯度的扩散模型微调方法，能够处理大奖励倾斜。

Method: 将大奖励倾斜exp(λr)分解为N个顺序小倾斜，每个小倾斜通过一阶泰勒展开进行可处理的分数更新。仅需前向评估奖励函数，避免反向传播。

Result: 在具有线性奖励的二维高斯混合模型上验证，该模型的精确倾斜分布有闭式解，验证了方法的有效性。

Conclusion: 迭代倾斜法提供了一种无需梯度的扩散模型微调方法，能够有效处理大奖励倾斜，避免了反向传播的计算负担。

Abstract: We introduce iterative tilting, a gradient-free method for fine-tuning diffusion models toward reward-tilted distributions. The method decomposes a large reward tilt $\exp(λr)$ into $N$ sequential smaller tilts, each admitting a tractable score update via first-order Taylor expansion. This requires only forward evaluations of the reward function and avoids backpropagating through sampling chains. We validate on a two-dimensional Gaussian mixture with linear reward, where the exact tilted distribution is available in closed form.

</details>


### [8] [Colored Markov Random Fields for Probabilistic Topological Modeling](https://arxiv.org/abs/2512.03727)
*Lorenzo Marinucci,Leonardo Di Nino,Gabriele D'Acunto,Mario Edoardo Pandolfo,Paolo Di Lorenzo,Sergio Barbarossa*

Main category: stat.ML

TL;DR: 提出Colored Markov Random Fields (CMRFs)，通过引入链接着色同时建模拓扑空间上高斯边缘变量的条件依赖和边际依赖，扩展经典高斯马尔可夫随机场


<details>
  <summary>Details</summary>
Motivation: 传统概率图模型在处理定义在拓扑空间上的变量时表达能力有限，因为底层拓扑结构会影响统计关系，需要新的模型来同时捕捉条件依赖和边际依赖

Method: 引入Colored Markov Random Fields (CMRFs)，基于Hodge理论，通过链接着色机制：连接性编码条件独立性，颜色编码边际独立性，扩展经典高斯马尔可夫随机场

Result: 通过物理网络上的分布式估计案例研究量化CMRFs的优势，与具有不同拓扑先验水平的基线方法进行比较

Conclusion: CMRFs能够更好地建模拓扑空间上变量的统计关系，为复杂系统分析和决策支持提供更强大的工具

Abstract: Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior.

</details>


### [9] [Comparison of neural network training strategies for the simulation of dynamical systems](https://arxiv.org/abs/2512.03851)
*Paul Strasser,Andreas Pfeffer,Jakob Weber,Markus Gurtner,Andreas Körner*

Main category: stat.ML

TL;DR: 并行训练比串行-并行训练在神经网络动态系统建模中能提供更好的长期预测精度，应作为默认训练策略


<details>
  <summary>Details</summary>
Motivation: 神经网络已成为非线性动态系统建模的常用工具，但训练策略选择仍是关键设计决策。当前实践中串行-并行训练占主导地位，但其与并行训练的性能比较缺乏系统研究。

Method: 对五种神经网络架构在两个实际示例（气动阀门测试台和工业机器人基准）上进行实证分析，比较并行训练和串行-并行训练两种策略，并澄清文献中不一致的术语。

Result: 尽管串行-并行训练在当前实践中占主导地位，但并行训练在长期预测精度方面始终表现更好。研究还澄清了文献中的术语不一致问题。

Conclusion: 并行训练应被视为基于神经网络的动态系统仿真的默认训练策略，这挑战了当前实践中串行-并行训练的主导地位。

Abstract: Neural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [10] [Invited Discussion of "Model Uncertainty and Missing Data: An Objective Bayesian Perspective" by Gonzalo García-Donato , María Eugenia Castellanos , Stefano Cabras Alicia Quirós , and Anabel Forte](https://arxiv.org/abs/2512.03266)
*Merlise A Clyde*

Main category: stat.ME

TL;DR: 本文从客观贝叶斯角度，针对高斯回归框架中的模型不确定性和缺失数据问题，提出了一种基于插补g先验的解决方案，在MAR/MCAR缺失机制下保持计算可行性。


<details>
  <summary>Details</summary>
Motivation: 高斯回归中同时处理模型不确定性和缺失数据是一个挑战。传统方法难以在随机设计矩阵X存在缺失时，保持贝叶斯框架的一致性和计算可行性。

Method: 提出插补g先验，用Σ_{X_γ}替代传统g先验中的X_γ^TX_γ，从而在MAR/MCAR缺失机制下，为随机X建立一致的贝叶斯模型选择框架。

Result: 该方法能够同时处理缺失数据和模型不确定性，保持计算可行性，并与基于插补X的g先验以及图模型选择方法建立了理论联系。

Conclusion: 插补g先验为高斯回归中同时处理模型不确定性和缺失数据问题提供了客观贝叶斯解决方案，在理论和计算上都具有优势。

Abstract: The article by Garc{í}a-Donato and co-authors addresses the dual challenges of accounting for model uncertainty and missing data within the Gaussian regression frameworks from an objective Bayesian perspective. Thru the use of an imputation $g$-prior that replaces $X_γ^TX_γ$ for model $γ$ in the covariance of $β_γ$ with $Σ_{X_γ}$, the authors develop a coherent approach to addressing the missing data problem and model uncertainty simultaneously with random $X_γ$ in the missing at random (MAR) or missing completely at random (MCAR) settings, while still being computationally tractable. I discuss the connection of the imputation $g$-prior to the $g$-prior with imputed $X$, and to model selection for graphical models that provide an alternative justification for the $g$-prior for random $X$s.

</details>


### [11] [Inference for location and height of peaks of a standardized field after selection](https://arxiv.org/abs/2512.04059)
*Alden Green,Jonathan Taylor*

Main category: stat.ME

TL;DR: 提出一种峰值推断方法，先对观测到的局部最大值进行显著性检验，然后利用被声明为显著的峰值构建后选择有效的置信区域，用于定位附近真实峰值的位置和高度。


<details>
  <summary>Details</summary>
Motivation: 在噪声随机场中，使用局部最大值（"峰值"）来检测和定位潜在信号区域的需求。需要一种既能控制错误发现率，又能为检测到的峰值提供有效置信区域的方法。

Method: 1. 对观测到的峰值进行显著性检验，检验零假设（无信号存在）；2. 利用被声明为显著的峰值构建后选择有效的置信区域，用于估计附近真实峰值的位置和高度；3. 在高曲率渐近假设下分析该方法性能，使用Kac-Rice公式推导局部最大值点过程的强度函数近似。

Result: 该方法在平滑信号加常数方差噪声模型中，在高曲率渐近假设下，能够渐近控制：1. 相对于进行推断的点数，错误发现的数量；2. 不包含真实峰值的置信区域数量。推导出了在备择假设下（附近存在高曲率真实峰值）二阶精确的局部最大值点过程强度函数近似。

Conclusion: 提出的峰值推断方法在理论上具有严格的统计保证，能够同时控制错误发现率和置信区域的有效性，为噪声随机场中的峰值检测和定位提供了可靠的统计推断框架。

Abstract: Peak inference concerns the use of local maxima ("peaks") of a noisy random field to detect and localize regions where underlying signal is present. We propose a peak inference method that first subjects observed peaks to a significance test of the null hypothesis that no signal is present, and then uses the peaks that are declared significant to construct post-selectively valid confidence regions for the location and height of nearby true peaks. We analyze the performance of this method in a smooth signal plus constant variance noise model under a high-curvature asymptotic assumption, and prove that it asymptotically controls both the number of false discoveries, and the number of confidence regions that do not contain a true peak, relative to the number of points at which inference is conducted. An important intermediate theoretical result uses the Kac-Rice formula to derive a novel approximation to the intensity function of a point process that counts local maxima, which is second-order accurate under the alternative, nearby high-curvature true peaks.

</details>


### [12] [Assessing Extrapolation of Peaks Over Thresholds with Martingale Testing](https://arxiv.org/abs/2512.03116)
*Joseph de Vilmarest,Olivier Wintenberger*

Main category: stat.ME

TL;DR: 本文提出了EVA2025数据挑战赛的获胜策略，通过极值理论对极端降水事件概率进行估计，采用单变量模型和POT方法，使用鞅检验评估外推能力并选择高分位数阈值。


<details>
  <summary>Details</summary>
Motivation: 极端降水事件在数据集中最多只出现一次，这使得挑战本质上是对极端值的外推问题。由于极端事件稀缺，需要一种简单、稳健的建模方法。

Method: 采用单变量模型而非多变量模型，使用极值理论中的POT（超阈值）方法。具体来说，在季节性调整后，对目标变量超过高分位数的部分拟合指数分布。创新点在于使用鞅检验来评估程序的外推能力，并以不可知论的方式选择高分位数的水平。

Result: 该方法在EVA2025数据挑战赛中获胜，成功估计了极端降水事件的概率。

Conclusion: 虽然该方法存在一些局限性，但将外推问题框架化为博弈过程，为极值分析中其他不可知论方法打开了大门。

Abstract: We present the winning strategy for the EVA2025 Data Challenge, which aimed to estimate the probability of extreme precipitation events. These events occurred at most once in the dataset making the challenge fundamentally one of extrapolating extreme values. Given the scarcity of extreme events, we argue that a simple, robust modeling approach is essential. We adopt univariate models instead of multivariate ones and model Peaks Over Thresholds using Extreme Value Theory. Specifically, we fit an exponential distribution to model exceedances of the target variable above a high quantile (after seasonal adjustment). The novelty of our approach lies in using martingale testing to evaluate the extrapolation power of the procedure and to agnostically select the level of the high quantile. While this method has several limitations, we believe that framing extrapolation as a game opens the door to other agnostic approaches in Extreme Value Analysis.

</details>


### [13] [Estimation of Semiparametric Factor Models with Missing Data](https://arxiv.org/abs/2512.03235)
*Sijie Zheng*

Main category: stat.ME

TL;DR: 论文提出了一种投影主成分分析（PPCA）方法，用于处理高维面板数据中带有缺失值的半参数因子模型，通过逆概率加权处理随机缺失机制，即使在时间维度固定时也能获得一致估计。


<details>
  <summary>Details</summary>
Motivation: 在实证应用中，高维面板数据的半参数因子模型面临两个主要挑战：因子载荷包含由观测协变量解释的非参数成分和捕捉未观测异质性的特异成分；更重要的是，缺失观测的存在会扭曲因子恢复和载荷估计。现有方法如经典PCA在时间维度固定时无法获得一致估计，且难以处理缺失数据问题。

Method: 开发了投影主成分分析（PPCA）程序，通过逆概率加权处理一般随机缺失机制。该方法允许筛维数发散，时间维度可以是固定或增长的。与经典PCA不同，PPCA即使在T固定时也能实现一致的因子估计。

Result: 建立了估计因子和载荷函数的一致性和渐近分布理论。在缺失数据下，极限分布呈现混合正态性，且渐近方差增大。模拟研究和实证应用支持了理论结果，表明PPCA在存在缺失数据时提供了有效且稳健的半参数因子模型估计框架。

Conclusion: PPCA为处理缺失数据的半参数因子模型提供了一个有效且稳健的估计框架，即使在时间维度固定时也能获得一致估计，填补了现有方法的空白，具有重要的理论和应用价值。

Abstract: We study semiparametric factor models in high-dimensional panels where the factor loadings consist of a nonparametric component explained by observed covariates and an idiosyncratic component capturing unobserved heterogeneity. A key challenge in empirical applications is the presence of missing observations, which can distort both factor recovery and loading estimation. To address this issue, we develop a projected principal component analysis (PPCA) procedure that accommodates general missing-at-random mechanisms through inverse-probability weighting. We establish consistency and derive the asymptotic distributions of the estimated factors and loading functions, allowing the sieve dimension to diverge and permitting the time dimension to be either fixed or growing. Unlike classical PCA, PPCA achieves consistent factor estimation even when T is fixed, and the limiting distributions under missing data exhibit mixture normality with enlarged asymptotic variances. Theoretical results are supported by simulations and an empirical application. Our findings demonstrate that PPCA provides an effective and robust framework for estimating semiparametric factor models in the presence of missing data.

</details>


### [14] [Assumption-Lean Differential Variance Inference for Heterogeneous Treatment Effect Detection](https://arxiv.org/abs/2512.03254)
*Philippe A. Boileau,Hani Zaki,Gabriele Lileikyte,Niklas Nielsen,Patrick R. Lawler,Mireille E. Schnitzer*

Main category: stat.ME

TL;DR: 提出基于潜在结果方差对比的新方法，用于检测异质性治疗效果，即使效应修饰变量缺失或测量有误也能有效工作


<details>
  <summary>Details</summary>
Motivation: 传统条件平均处理效应(CATE)方法在效应修饰变量缺失或测量有误时会失效，无法检测异质性治疗效果，需要一种更稳健的方法

Method: 通过潜在结果方差的对比来检验同质性治疗效果假设，推导因果机器学习估计器，证明其双重稳健性和渐近线性性质

Result: 估计器在实验和观察数据中都能近似达到渐近保证，成功应用于心脏骤停患者靶向体温管理的随机对照试验再分析

Conclusion: 提出的方差对比方法为检测异质性治疗效果提供了更稳健的工具，即使在效应修饰变量缺失或测量有误的情况下也能进行正式假设检验

Abstract: The conditional average treatment effect (CATE) is frequently estimated to refute the homogeneous treatment effect assumption. Under this assumption, all units making up the population under study experience identical benefit from a given treatment. Uncovering heterogeneous treatment effects through inference about the CATE, however, requires that covariates truly modifying the treatment effect be reliably collected at baseline. CATE-based techniques will necessarily fail to detect violations when effect modifiers are omitted from the data due to, for example, resource constraints. Severe measurement error has a similar impact. To address these limitations, we prove that the homogeneous treatment effect assumption can be gauged through inference about contrasts of the potential outcomes' variances. We derive causal machine learning estimators of these contrasts and study their asymptotic properties. We establish that these estimators are doubly robust and asymptotically linear under mild conditions, permitting formal hypothesis testing about the homogeneous treatment effect assumption even when effect modifiers are missing or mismeasured. Numerical experiments demonstrate that these estimators' asymptotic guarantees are approximately achieved in experimental and observational data alike. These inference procedures are then used to detect heterogeneous treatment effects in the re-analysis of randomized controlled trials investigating targeted temperature management in cardiac arrest patients.

</details>


### [15] [Weighted Conformal Prediction for Survival Analysis under Covariate Shift](https://arxiv.org/abs/2512.03738)
*Jaeyoung Shin,Chi Hyun Lee,Sangwook Kang*

Main category: stat.ME

TL;DR: 提出一种改进的生存预测共形预测方法，解决现有方法在删失数据下对启发式调参敏感、无法处理协变量偏移的问题，提供理论保证并扩展至协变量偏移场景。


<details>
  <summary>Details</summary>
Motivation: 临床生存预测需要可靠的不确定性量化，现有删失数据共形预测方法要么依赖限制性删失假设，要么计算量大，且对启发式调参敏感、无法处理协变量偏移。

Method: 建立删失调整p值的理论依据，提供预测集边界的原理性定义，并将方法扩展至协变量偏移场景，通过模拟研究和真实数据应用验证。

Result: 所提方法在不同删失水平和协变量偏移场景下均能实现稳健的覆盖率，并保持一致的区间结构。

Conclusion: 该方法为删失生存数据的共形预测提供了理论保证，解决了现有方法的局限性，在临床应用中具有实用价值。

Abstract: Reliable uncertainty quantification is essential in survival prediction, particularly in clinical settings where erroneous decisions carry high risk. Conformal prediction has attracted substantial attention as it offers a model-agnostic framework with finite-sample coverage guarantees. Extending it to right-censored outcomes poses nontrivial challenges. Several adaptations of conformal approaches for survival outcomes have been developed, but they either rely on restrictive censoring settings or substantial computation. A recent conformal approach for right-censored data constructs censoring-adjusted p-values and enables prediction intervals in general survival settings. However, the empirical coverage depends sensitively on heuristic tuning choices and its validity is limited to scenarios without covariate shift. In this paper, we establish theoretical justification for its prediction-set construction, providing a principled basis for defining prediction-set bounds, and extend the approach to covariate-shift settings. Simulation studies and a real data application demonstrate that the proposed method achieves robust coverage and coherent interval structure across varying censoring levels and covariate-shift settings.

</details>


### [16] [Change Point Detection for Functional Autoregressive Processes on the Sphere](https://arxiv.org/abs/2512.03255)
*Federica Spoto,Alessia Caponera,Pierpaolo Brutti*

Main category: stat.ME

TL;DR: 提出了一种用于球面函数自回归过程的变点检测新框架，能够在球面时空随机场中识别结构突变点。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理球面时空数据的非平稳性，特别是在气候科学、宇宙学等领域需要检测球面随机场的结构变化。

Method: 基于谐波域的LASSO正则化估计器，采用惩罚动态规划方法，无需预先知道变点的数量和位置。

Result: 该方法提供了非渐近理论保证，能够有效检测球面函数自回归过程中的结构突变点。

Conclusion: 该框架为分析球面上的非平稳现象提供了新工具，在气候科学、宇宙学等领域具有重要应用价值。

Abstract: We introduce a novel framework for change point detection in spherical functional autoregressive (SPHAR) processes, enabling the identification of structural breaks in spatio-temporal random fields on the sphere. Our LASSO-regularized estimator, based on penalized dynamic programming in the harmonic domain, operates without knowledge of the number or locations of change points and offers non-asymptotic theoretical guarantees. This approach provides a new tool for analyzing nonstationary phenomena on the sphere, relevant to climate science, cosmology, and beyond.

</details>


### [17] [Using functional information for binary classifications](https://arxiv.org/abs/2512.03761)
*Pablo Martinez-Camblor*

Main category: stat.ME

TL;DR: 提出一种基于概率准则（PBC）的函数数据二元分类方法，通过计算轨迹与正负群体相似度的距离来进行分类，并提供了非参数估计器


<details>
  <summary>Details</summary>
Motivation: 函数数据（随时间连续测量的信息）在二元分类中存在挑战，传统方法难以直接对函数值排序。当标记物与研究特征相关时，阳性个体的轨迹应该与阳性群体更相似，基于此提出新的分类方法

Method: 提出基于距离的概率准则（PBC）分类方法：计算测试函数与正负群体轨迹的相似度，通过距离比较进行分类。开发了完全非参数的PBC估计器，并研究了其渐近性质

Result: 蒙特卡洛模拟显示，当训练和测试队列样本量足够时，该方法在各种情况下表现良好，通常优于竞争对手。通过真实数据集验证了方法的实用性

Conclusion: 提出的基于概率准则的函数数据分类方法有效解决了函数数据排序问题，在样本量充足时表现优异，提供了R函数实现便于实际应用

Abstract: The adequate use of information measured in a continuous manner along a period of time represents a methodological challenge. In the last decades, most of traditional statistical procedures have been extended for accommodating these functional data. The binary classification problem, which aims to correctly identify units as positive or negative based on marker values, is not aside of this scenario. The crucial point for making binary classifications based on a marker is to establish an order in the marker values, which is not immediate when these values are presented as functions. Here, we argue that if the marker is related to the characteristic under study, a trajectory from a positive participant should be more similar to trajectories from the positive population than to those drawn from the negative. With this criterion, a classification procedure based on the distance between the involved functions is proposed. Besides, we propose a fully non-parametric estimator for this so-called probability-based criterion, PBC. We explore its asymptotic properties, and its finite-sample behavior from an extensive Monte Carlo study. The observed results suggest that the proposed methodology works adequately, and frequently better than its competitors, for a wide variety of situations when the sample size in both the training and the testing cohorts is adequate. The practical use of the proposal is illustrated from real-world dataset. As online supplementary material, the manuscript includes a document with further simulations and additional comments. An R function which wraps up the implemented routines is also provided.

</details>


### [18] [A comparison between initialization strategies for the infinite hidden Markov model](https://arxiv.org/abs/2512.03777)
*Federico P. Cortese,Luca Rossini*

Main category: stat.ME

TL;DR: 该研究系统评估了无限隐马尔可夫模型中初始化策略的影响，发现基于距离的聚类初始化方法优于现有文献中最常用的均匀初始化方法。


<details>
  <summary>Details</summary>
Motivation: 无限隐马尔可夫模型为时间序列建模提供了灵活框架，但现有文献对初始化策略的关注有限。本研究旨在填补这一空白，评估有限隐马尔可夫模型中常用初始化策略在无限设置中的适用性。

Method: 系统评估了有限隐马尔可夫模型中常用的初始化策略在无限隐马尔可夫模型框架下的表现，包括基于距离的聚类初始化、基于模型的初始化和均匀初始化。使用模拟数据集和真实数据集进行对比分析。

Result: 实验结果显示，基于距离的聚类初始化方法在无限隐马尔可夫模型中表现最佳，一致优于基于模型的初始化和均匀初始化方法，而均匀初始化是目前文献中最广泛采用的方法。

Conclusion: 初始化策略对无限隐马尔可夫模型的性能有重要影响，基于距离的聚类初始化方法应成为该框架的首选初始化策略，这为改进现有实践提供了实证依据。

Abstract: Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature.

</details>


### [19] [Parsimonious Factor Models for Asymmetric Dependence in Multivariate Extremes](https://arxiv.org/abs/2512.03543)
*Pavel Krupskii,Boris Béranger*

Main category: stat.ME

TL;DR: 提出一种新的加性因子模型用于多元极值建模，能够捕捉非对称尾部依赖和非平稳行为，相比流行的Hüsler-Reiss模型具有更好的拟合效果。


<details>
  <summary>Details</summary>
Motivation: 现有多元极值模型难以同时满足灵活性（特别是捕捉非对称依赖结构）、简约性和计算可扩展性。流行的Hüsler-Reiss模型仅限于对称依赖结构，无法处理现实世界中的复杂极值现象。

Method: 引入一类加性因子模型并推导其极值极限，得到参数可控的广泛且易处理的模型族。从分量最大值和超阈值两个角度（分别通过多元极值分布和多元广义帕累托分布）呈现极限模型。

Result: 新模型能够自然容纳非对称尾部依赖并允许非平稳行为。模拟研究展示了基于现有推断方法的可识别性。在墨尔本夏季最高温度和英国四大银行周负收益率的应用中，相比Hüsler-Reiss模型显示出改进的拟合效果。

Conclusion: 提出的加性因子模型为多元极值建模提供了一个灵活、简约且计算可扩展的框架，特别适合处理现实世界中具有非对称依赖结构的极端事件。

Abstract: Modelling multivariate extreme events is essential when extrapolating beyond the range of observed data. Parametric models that are suitable for real-world extremes must be flexible -- particularly in their ability to capture asymmetric dependence structures -- while also remaining parsimonious for interpretability and computationally scalable in high dimensions. Although many models have been proposed, it is rare for any single construction to satisfy all of these requirements. For instance, the popular Hüsler-Reiss model is limited to symmetric dependence structures. In this manuscript, we introduce a class of additive factor models and derive their extreme-value limits. This leads to a broad and tractable family of models characterised by a manageable number of parameters. These models naturally accommodate asymmetric tail dependence and allow for non-stationary behaviour. We present the limiting models from both the componentwise-maxima and Peaks-over-Thresholds perspectives, via the multivariate extreme value and multivariate generalized Pareto distributions, respectively. Simulation studies illustrate identifiability properties based on existing inference methodologies. Finally, applications to summer temperature maxima in Melbourne, Australia, and to weekly negative returns from four major UK banks demonstrate improved fit compared with the Hüsler-Reiss model.

</details>


### [20] [SUP: An Inferable Private Multiple Testing Framework with Super Uniformity](https://arxiv.org/abs/2512.03859)
*Kehan Wang,Wenxuan Song,Wangli Xu,Linglong Kong*

Main category: stat.ME

TL;DR: 提出SUP框架解决私有化多重检验问题，通过p值变换、反向剥离算法和多样化拒绝阈值，在保护隐私的同时控制I类错误并提升检验功效。


<details>
  <summary>Details</summary>
Motivation: 在基因组学和健康数据分析等需要保护敏感个人信息的领域，私有化多重检验算法对于超均匀p值仍是一个开放问题。隐私机制会引入复杂的依赖关系并破坏p值的超均匀性，使选择后推断变得困难。

Method: 提出Super Uniform Private (SUP)多重检验框架，包含三个核心组件：1) 兼容多种隐私机制并保持超均匀性的p值变换；2) 减少隐私预算并促进推断的反向剥离算法；3) 针对不同I类错误（FWER和FDR）的隐私参数无关拒绝阈值。进一步开发自适应技术确定剥离数量和提升阈值。

Result: 理论分析克服了选择后I类错误控制的障碍，量化了SUP相对于非私有方法的功效损失，并证明SUP在功效上优于现有私有方法。大量模拟和真实数据应用验证了理论结果。

Conclusion: SUP框架成功解决了私有化多重检验的关键挑战，在保护隐私的同时有效控制I类错误并保持较高的检验功效，为敏感数据分析提供了实用的解决方案。

Abstract: Multiple testing is widely applied across scientific fields, particularly in genomic and health data analysis, where protecting sensitive personal information is imperative. However, developing private multiple testing algorithms for super uniform $p$-values remains an open question, as privacy mechanisms introduce intricate dependence among the peeled $p$-values and disrupt their super uniformity, complicating post-selection inference. To address this, we introduce a general Super Uniform Private (SUP) multiple testing framework with three key components. First, we develop a novel \( p \)-value transformation that is compatible with diverse privacy regimes while retaining the super uniformity. Next, a reversed peeling algorithm is designed to reduce privacy budgets while facilitating inference. Then, we provide diverse rejection thresholds that are privacy-parameter-free and tailored for different Type-I errors, including the family-wise error rate (FWER) and the false discovery rate (FDR). Building upon these, we advance adaptive techniques to determine the peeling number and boost thresholds. Theoretically, we propose a technique overcoming the post-selection obstacle to Type-I error control, quantify the privacy-induced power loss of SUP relative to its non-private counterpart, and demonstrate that SUP surpasses existing private methods in terms of power. The results of extensive simulations and a real data application validate our theories.

</details>


### [21] [Parsimonious Clustering of Covariance Matrices](https://arxiv.org/abs/2512.03912)
*Yixi Xu,Yi Zhao*

Main category: stat.ME

TL;DR: 提出一种基于混合专家模型和协方差回归的简约聚类框架，用于根据静息态fMRI功能连接数据对个体进行聚类，并在阿尔茨海默病数据中识别亚组及其与人口学和认知功能的关联。


<details>
  <summary>Details</summary>
Motivation: 功能连接（FC）为理解脑功能和神经精神疾病提供重要信息，需要无监督聚类方法基于共享特征对个体进行分组，以辅助临床诊断。

Method: 提出整合混合专家模型（MoE）和协方差回归框架的简约聚类模型，假设协方差矩阵间存在共同线性投影，并使用带协变量的广义线性模型，实现灵活且可解释的投影特异性聚类。

Result: 通过广泛的模拟研究评估聚类准确性和鲁棒性，并将方法应用于ADNI静息态fMRI数据，基于脑一致性识别亚组，同时揭示其与人口学因素和认知功能的关联。

Conclusion: 该框架能够有效聚类功能连接数据，识别临床相关的亚组，并为理解脑功能与临床特征的关系提供新视角。

Abstract: Functional connectivity (FC) derived from functional magnetic resonance imaging (fMRI) data offers vital insights for understanding brain function and neurological and psychiatric disorders. Unsupervised clustering methods are desired to group individuals based on shared features, facilitating clinical diagnosis. In this study, a parsimonious clustering model is proposed, which integrates the Mixture-of-Experts (MoE) and covariance regression framework, to cluster individuals based on FC captured by data covariance matrices in resting-state fMRI studies. The model assumes common linear projections across covariance matrices and a generalized linear model with covariates, allowing for flexible yet interpretable projection-specific clustering solutions. To evaluate the performance of the proposed framework, extensive simulation studies are conducted to assess clustering accuracy and robustness. The approach is applied to resting-state fMRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Subgroups are identified based on brain coherence and simultaneously uncover the association with demographic factors and cognitive functions.

</details>


### [22] [When are novel methods for analyzing complex chemical mixtures in epidemiology beneficial?](https://arxiv.org/abs/2512.03946)
*Nate Wiecha,Emily Griffith,Brian J. Reich,Jane A. Hoppin*

Main category: stat.ME

TL;DR: 比较分析化学混合物健康影响研究中通用统计方法与新颖方法的性能，发现不同场景下各有优劣，提供方法选择指南


<details>
  <summary>Details</summary>
Motivation: 评估化学混合物暴露健康影响的统计方法面临多重挑战：相关性、高维度、非线性交互作用。现有综述偏重新颖方法，缺乏对通用方法的系统比较。

Method: 通过模拟实验广泛比较通用方法（如广义线性模型）和专门为化学混合物设计的新颖方法（如贝叶斯核机回归），评估类型I错误率控制、统计功效、结果可解释性和预测准确性。

Result: 当混合物成分相关性中等且暴露-反应函数没有复杂交互作用，或成分效应相反时，通用方法优于新颖方法；当存在高度交互作用或高度相关暴露时，新颖方法具有重要优势。

Conclusion: 提供了不同统计方法适用场景的全面总结，帮助研究者根据具体数据特征选择合适方法，平衡通用性和专门化方法的优缺点。

Abstract: Estimating the health impacts of exposure to a mixture of chemicals poses many statistical challenges: multiple correlated exposure variables, moderate to high dimensionality, and possible nonlinear and interactive health effects of mixture components. Reviews of chemical mixture methods aim to help researchers select a statistical method suited to their goals and data, but examinations of empirical performance have emphasized novel methods purpose-built for analyzing complex chemical mixtures, or other more advanced methods, over more general methods which are widely used in many application domains. We conducted a broad experimental comparison, across simulated scenarios, of both more general methods (such as generalized linear models) and novel methods (such as Bayesian Kernel Machine Regression) designed to study chemical mixtures. We assessed methods based on their ability to control Type I error rate, maximize power, provide interpretable results, and make accurate predictions. We find that when there is moderate correlation between mixture components and the exposure-response function does not have complicated interactions, or when mixture components have opposite effects, general methods are preferred over novel ones. With highly interactive exposure-response functions or highly correlated exposures, novel methods provide important benefits. We provide a comprehensive summary of when different methods are most suitable.

</details>


### [23] [Statistical hypothesis testing for differences between layers in dynamic multiplex networks](https://arxiv.org/abs/2512.03983)
*Maximilian Baum,Francesco Sanna Passino,Axel Gandy*

Main category: stat.ME

TL;DR: 提出一个基于潜在空间网络模型的假设检验框架，用于检验动态多层网络中不同边类型层是否共享相同的潜在表示


<details>
  <summary>Details</summary>
Motivation: 随着动态多层网络的出现，需要一种统计方法来检验不同边类型层在连接性上是否存在差异，这对于理解网络结构至关重要

Method: 基于谱嵌入方法构建检验统计量，通过对图邻接矩阵展开表示进行谱嵌入，在节点数趋于无穷的渐近条件下检验层间差异

Result: 方法在渐近条件下能够有效检测层间差异，并在模拟数据和果蝇幼虫神经活动数据集上验证了有限样本性能

Conclusion: 提出的假设检验框架能够有效检验多层网络中层间差异，并可扩展到时间点差异检验，为动态多层网络分析提供了重要工具

Abstract: With the emergence of dynamic multiplex networks, corresponding to graphs where multiple types of edges evolve over time, a key inferential task is to determine whether the layers associated with different edge types differ in their connectivity. In this work, we introduce a hypothesis testing framework, under a latent space network model, for assessing whether the layers share a common latent representation. The method we propose extends previous literature related to the problem of pairwise testing for random graphs and enables global testing of differences between layers in multiplex graphs. While we introduce the method as a test for differences between layers, it can easily be adapted to test for differences between time points. We construct a test statistic based on a spectral embedding of an unfolded representation of the graph adjacency matrices and demonstrate its ability to detect differences across layers in the asymptotic regime where the number of nodes in each graph tends to infinity. The finite-sample properties of the test are empirically demonstrated by assessing its performance on both simulated data and a biological dataset describing the neural activity of larval Drosophila.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [24] [Seasonal trend assessment of US extreme precipitation via changepoint segmentation](https://arxiv.org/abs/2512.03513)
*Jaechoul Lee,Mintaek Lee,Thea Sukianto*

Main category: stat.AP

TL;DR: 该研究分析了美国季节性最大降水量的趋势，考虑了季节特异性趋势和突变点（如仪器更换等），发现考虑突变点后季节性趋势变化更大，秋季南部和东海岸呈现更多增长趋势。


<details>
  <summary>Details</summary>
Motivation: 传统气候趋势研究通常假设长期趋势在所有季节保持一致，但实际每个季节可能有不同的趋势。此外，季节性气候时间序列常存在非平稳性（如周期性和位置突变），忽略这些特征会导致趋势估计不准确。

Method: 使用惩罚似然方法估计多个突变点，结合具有周期特征的广义极值分布。采用基于遗传算法的搜索算法高效探索突变点的数量和时序空间。计算季节性极端降水的重现水平。

Result: 考虑突变点后，季节性趋势变化比忽略突变点的研究更大。发现明显的区域和季节模式：当考虑突变点时，秋季南部和东海岸地区更普遍呈现增长趋势。

Conclusion: 在分析季节性气候趋势时，必须考虑季节特异性趋势和突变点等非平稳性特征，否则可能导致趋势估计偏差。研究为美国季节性极端降水趋势提供了更准确的评估。

Abstract: Most climate trend studies analyze long-term trends as a proxy for climate dynamics. However, when examining seasonal data, it is unrealistic to assume that long-term trends remain consistent across all seasons. Instead, each season likely experiences distinct trends. Additionally, seasonal climate time series, such as seasonal maximum precipitation, often exhibit nonstationarities, including periodicities and location shifts. Failure to rigorously account for these features in modeling may lead to inaccurate trend estimates. This study quantifies seasonal trends in the contiguous United States' seasonal maximum precipitation series while addressing these nonstationarities. To ensure accurate trend estimation, we identify changepoints where the seasonal maximum precipitation shifts due to factors like measurement device changes, observer differences, or location moves. We employ a penalized likelihood method to estimate multiple changepoints, incorporating a generalized extreme value distribution with periodic features. A genetic algorithm based search algorithm efficiently explores the vast space of potential changepoints in both number and timing. Additionally, we compute seasonal return levels for extreme precipitation. Our methods are illustrated using two selected stations, and the results for the US are summarized through maps. We find that seasonal trends vary more when changepoints are considered than in studies that ignore them. Our findings also reveal distinct regional and seasonal patterns, with increasing trends more prevalent during fall in the South and along the East Coast when changepoints are accounted for.

</details>


### [25] [A decay-adjusted spatio-temporal model to account for the impact of mass drug administration on neglected tropical disease prevalence](https://arxiv.org/abs/2512.03760)
*Emanuele Giorgi,Claudio Fronterre,Peter J. Diggle*

Main category: stat.AP

TL;DR: 提出一种考虑时间衰减效应的空间时间模型（DAST），用于评估大规模药物管理对NTD流行率的影响，为稀疏调查数据提供灵活可解释的干预效果估计框架。


<details>
  <summary>Details</summary>
Motivation: 常规流行率调查用于监测NTD大规模药物管理项目效果，但现有标准地理统计模型在量化MDA影响和短期预测方面存在局限，需要更灵活且可解释的模型来处理稀疏调查数据。

Method: 提出衰减调整的空间时间模型（DAST），明确考虑MDA对NTD流行率随时间变化的影响，为稀疏调查数据提供灵活可解释的干预效果估计框架。

Result: 通过土壤传播蠕虫和淋巴丝虫病的案例研究，证明DAST在量化MDA影响和支持短期项目预测方面是标准地理统计模型的实用替代方案。

Conclusion: DAST模型为NTD控制项目提供了实用的建模工具，在数据稀疏情况下倡导数据驱动的简约性而非复杂性，并讨论了模型扩展和可识别性挑战。

Abstract: Prevalence surveys are routinely used to monitor the effectiveness of mass drug administration (MDA) programmes for controlling neglected tropical diseases (NTDs). We propose a decay-adjusted spatio-temporal (DAST) model that explicitly accounts for the time-varying impact of MDA on NTD prevalence, providing a flexible and interpretable framework for estimating intervention effects from sparse survey data. Using case studies on soil-transmitted helminths and lymphatic filariasis, we show that DAST offers a practical alternative to standard geostatistical models when the objective includes quantifying MDA impact and supporting short-term programmatic forecasting. We also discuss extensions and identifiability challenges, advocating for data-driven parsimony over complexity in settings where the available data are too sparse to support the estimation of highly parameterised models.

</details>
