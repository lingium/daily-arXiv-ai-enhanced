{"id": "2512.17127", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17127", "abs": "https://arxiv.org/abs/2512.17127", "authors": ["Benjamin S. H. Lyo", "Eero P. Simoncelli", "Cristina Savin"], "title": "Disentangled representations via score-based variational autoencoders", "comment": "34 pages, 7 figures", "summary": "We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder."}
{"id": "2512.17038", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.17038", "abs": "https://arxiv.org/abs/2512.17038", "authors": ["Brandon Marks", "Yash Dave", "Zixun Wang", "Hannah Chung", "Riya Patwa", "Simon Cha", "Michael Murphy", "Alexander Strang"], "title": "Do Generalized=Gamma Scale Mixtures of Normals Fit Large Image Data-Sets?", "comment": "25 pages main text, 21 figures, 7 tables, 6 pages appendix", "summary": "A scale mixture of normals is a distribution formed by mixing a collection of normal distributions with fixed mean but different variances. A generalized gamma scale mixture draws the variances from a generalized gamma distribution. Generalized gamma scale mixtures of normals have been proposed as an attractive class of parametric priors for Bayesian inference in inverse imaging problems. Generalized gamma scale mixtures have two shape parameters, one that controls the behavior of the distribution about its mode, and the other that controls its tail decay. In this paper, we provide the first demonstration that the prior model is realistic for multiple large imaging data sets. We draw data from remote sensing, medical imaging, and image classification applications. We study the realism of the prior when applied to Fourier and wavelet (Haar and Gabor) transformations of the images, as well as to the coefficients produced by convolving the images against the filters used in the first layer of AlexNet, a popular convolutional neural network trained for image classification. We discuss data augmentation procedures that improve the fit of the model, procedures for identifying approximately exchangeable coefficients, and characterize the parameter regions that best describe the observed data sets. These regions are significantly broader than the region of primary focus in computational work. We show that this prior family provides a substantially better fit to each data set than any of the standard priors it contains. These include Gaussian, Laplace, $\\ell_p$, and Student's $t$ priors. Finally, we identify cases where the prior is unrealistic and highlight characteristic features of images that suggest the model will fit poorly."}
{"id": "2512.17341", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17341", "abs": "https://arxiv.org/abs/2512.17341", "authors": ["Jikai Jin", "Vasilis Syrgkanis"], "title": "Sharp Structure-Agnostic Lower Bounds for General Functional Estimation", "comment": "95 pages; generalize and subsume partial results of arXiv:2402.14264 by the same authors", "summary": "The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \\citet{jin2024structure} by the same authors."}
{"id": "2512.17119", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17119", "abs": "https://arxiv.org/abs/2512.17119", "authors": ["Laura Pardo", "Juan Sosa"], "title": "Uncovering latent territorial structure in ICFES Saber 11 performance with Bayesian multilevel spatial models", "comment": "42 pages, 14 tables, 12 figures", "summary": "This article develops a Bayesian hierarchical framework to analyze academic performance in the 2022 second semester Saber 11 examination in Colombia. Our approach combines multilevel regression with municipal and departmental spatial random effects, and it incorporates Ridge and Lasso regularization priors to compare the contribution of sociodemographic covariates. Inference is implemented in a fully open source workflow using Markov chain Monte Carlo methods, and model behavior is assessed through synthetic data that mirror key features of the observed data. Simulation results indicate that Ridge provides the most balanced performance in parameter recovery, predictive accuracy, and sampling efficiency, while Lasso shows weaker fit and posterior stability, with gains in predictive accuracy under stronger multicollinearity. In the application, posterior rankings show a strong centralization of performance, with higher scores in central departments and lower scores in peripheral territories, and the strongest correlates of scores are student level living conditions, maternal education, access to educational resources, gender, and ethnic background, while spatial random effects capture residual regional disparities. A hybrid Bayesian segmentation based on K means propagates posterior uncertainty into clustering at departmental, municipal, and spatial scales, revealing multiscale territorial patterns consistent with structural inequalities and informing territorial targeting in education policy."}
{"id": "2512.17374", "categories": ["stat.ML", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.17374", "abs": "https://arxiv.org/abs/2512.17374", "authors": ["Fatima-Zahrae Akhyar", "Wei Zhang", "Gabriel Stoltz", "Christof Schütte"], "title": "Generative modeling of conditional probability distributions on the level-sets of collective variables", "comment": null, "summary": "Given a probability distribution $μ$ in $\\mathbb{R}^d$ represented by data, we study in this paper the generative modeling of its conditional probability distributions on the level-sets of a collective variable $ξ: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$, where $1 \\le k<d$. We propose a general and effcient learning approach that is able to learn generative models on different level-sets of $ξ$ simultaneously. To improve the learning quality on level-sets in low-probability regions, we also propose a strategy for data enrichment by utilizing data from enhanced sampling techniques. We demonstrate the effectiveness of our proposed learning approach through concrete numerical examples. The proposed approach is potentially useful for the generative modeling of molecular systems in biophysics, for instance."}
{"id": "2512.17758", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2512.17758", "abs": "https://arxiv.org/abs/2512.17758", "authors": ["Guillaume Koechlin", "Filippo Bovera", "Piercesare Secchi"], "title": "Day-Ahead Electricity Price Forecasting Using Merit-Order Curves Time Series", "comment": null, "summary": "We introduce a general, simple, and computationally efficient framework for predicting day-ahead supply and demand merit-order curves, from which both point and probabilistic electricity price forecasts can be derived. Specifically, we leverage functional principal component analysis to efficiently represent a pair of supply and demand curves in a low-dimensional vector space and employ regularized vector autoregressive models for their prediction. We conduct a rigorous empirical comparison of price forecasting performance between the proposed curve-based model, i.e., derived from predicted merit-order curves, and state-of-the-art price-based models that directly forecast the clearing price, using data from the Italian day-ahead market over the 2023-2024 period. Our results show that the proposed curve-based approach significantly improves both point and probabilistic price forecasting accuracy relative to price-based approaches, with average gains of approximately 5%, and improvements of up to 10% during mid-day hours, when prices occasionally drop due to high renewable generation and low demand."}
{"id": "2512.17426", "categories": ["stat.ML", "cond-mat.dis-nn", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17426", "abs": "https://arxiv.org/abs/2512.17426", "authors": ["Xiaosi Gu", "Ayaka Sakata", "Tomoyuki Obuchi"], "title": "Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing", "comment": "49 pages, 10 figures", "summary": "We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equations. A detailed comparison shows that 1RSB-AMP and 1RSB-SE agree remarkably well at the macroscopic level, even in parameter regions where replica-symmetric (RS) AMP, termed RS-AMP, diverges and where the 1RSB description itself is not expected to be thermodynamically exact. Fixed-point analysis of 1RSB-SE reveals a phase diagram consisting of success, failure, and diverging phases, as in the RS case. However, the diverging-region boundary now depends on the Parisi parameter due to the 1RSB ansatz, and we propose a new criterion -- minimizing the size of the diverging region -- rather than the conventional zero-complexity condition, to determine its value. Combining this criterion with the nonconvexity-control (NCC) protocol proposed in a previous RS study improves the algorithmic limit of perfect reconstruction compared with RS-AMP. Numerical solutions of 1RSB-SE and experiments with 1RSB-AMP confirm that this improved limit is achieved in practice, though the gain is modest and remains slightly inferior to the Bayes-optimal threshold. We also report the behavior of thermodynamic quantities -- overlaps, free entropy, complexity, and the non-self-averaging susceptibility -- that characterize the 1RSB phase in this problem."}
{"id": "2512.17340", "categories": ["stat.ME", "cs.CY", "cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.17340", "abs": "https://arxiv.org/abs/2512.17340", "authors": ["Carter H. Nakamoto", "Lucia Lushi Chen", "Agata Foryciarz", "Sherri Rose"], "title": "Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease", "comment": null, "summary": "Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit."}
{"id": "2512.17478", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17478", "abs": "https://arxiv.org/abs/2512.17478", "authors": ["Paavo Sattler", "Nils Hichert"], "title": "Inference for high dimensional repeated measure designs with the R package hdrm", "comment": null, "summary": "Repeated-measure designs allow comparisons within a group as well as between groups, and are commonly referred to as split-plot designs. While originating in agricultural experiments, they are now widely used in medical research, psychology, and the life sciences, where repeated observations on the same subject are essential.\n  Modern data collection often produces observation vectors with dimension $d$ comparable to or exceeding the sample size $N$. Although this can be advantageous in terms of cost efficiency, ethical considerations, and the study of rare diseases, it poses substantial challenges for statistical inference.\n  Parametric methods based on multivariate normality provide a flexible framework that avoids restrictive assumptions on covariance structures or on the asymptotic relationship between $d$ and $N$. Within this framework, the freely available R-package hdrm enables the analysis of a wide range of hypotheses concerning expectation vectors in high-dimensional repeated-measure designs, covering both single-group and multi-group settings with homogeneous or heterogeneous covariance matrices.\n  This paper describes the implemented tests, demonstrates their use through examples, and discusses their applicability in practical high-dimensional data scenarios. To address computational challenges arising for large $d$, the package incorporates efficient estimators and subsampling strategies that substantially reduce computation time while preserving statistical validity."}
{"id": "2512.17113", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.17113", "abs": "https://arxiv.org/abs/2512.17113", "authors": ["Alan R. Vazquez", "Kilian M. Rother", "Marco V. Charles-Gonzalez"], "title": "A systematic assessment of Large Language Models for constructing two-level fractional factorial designs", "comment": "30 pages, 11 tables", "summary": "Two-level fractional factorial designs permit the study multiple factors using a limited number of runs. Traditionally, these designs are obtained from catalogs available in standard textbooks or statistical software. However, modern Large Language Models (LLMs) can now produce two-level fractional factorial designs, but the quality of these designs has not been previously assessed. In this paper, we perform a systematic evaluation of two popular classes of LLMs, namely GPT and Gemini models, to construct two-level fractional factorial designs with 8, 16, and 32 runs, and 4 to 26 factors. To this end, we use prompting techniques to develop a high-quality set of design construction tasks for the LLMs. We compare the designs obtained by the LLMs with the best-known designs in terms of resolution and minimum aberration criteria. We show that the LLMs can effectively construct optimal 8-, 16-, and 32-run designs with up to eight factors."}
{"id": "2512.17301", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.17301", "abs": "https://arxiv.org/abs/2512.17301", "authors": ["Ratbek Dzhumashev", "Ainura Tursunalieva"], "title": "A Synthetic Instrumental Variable Method: Using the Dual Tendency Condition for Coplanar Instruments", "comment": null, "summary": "Traditional instrumental variable (IV) methods often struggle with weak or invalid instruments and rely heavily on external data. We introduce a Synthetic Instrumental Variable (SIV) approach that constructs valid instruments using only existing data. Our method leverages a data-driven dual tendency (DT) condition to identify valid instruments without requiring external variables. SIV is robust to heteroscedasticity and can determine the true sign of the correlation between endogenous regressors and errors--an assumption typically imposed in empirical work. Through simulations and real-world applications, we show that SIV improves causal inference by mitigating common IV limitations and reducing dependence on scarce instruments. This approach has broad implications for economics, epidemiology, and policy evaluation."}
{"id": "2512.17632", "categories": ["stat.ML", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.17632", "abs": "https://arxiv.org/abs/2512.17632", "authors": ["Even He"], "title": "Fast and Robust: Computationally Efficient Covariance Estimation for Sub-Weibull Vectors", "comment": null, "summary": "High-dimensional covariance estimation is notoriously sensitive to outliers. While statistically optimal estimators exist for general heavy-tailed distributions, they often rely on computationally expensive techniques like semidefinite programming or iterative M-estimation ($O(d^3)$). In this work, we target the specific regime of \\textbf{Sub-Weibull distributions} (characterized by stretched exponential tails $\\exp(-t^α)$). We investigate a computationally efficient alternative: the \\textbf{Cross-Fitted Norm-Truncated Estimator}. Unlike element-wise truncation, our approach preserves the spectral geometry while requiring $O(Nd^2)$ operations, which represents the theoretical lower bound for constructing a full covariance matrix. Although spherical truncation is geometrically suboptimal for anisotropic data, we prove that within the Sub-Weibull class, the exponential tail decay compensates for this mismatch. Leveraging weighted Hanson-Wright inequalities, we derive non-asymptotic error bounds showing that our estimator recovers the optimal sub-Gaussian rate $\\tilde{O}(\\sqrt{r(Σ)/N})$ with high probability. This provides a scalable solution for high-dimensional data that exhibits tails heavier than Gaussian but lighter than polynomial decay."}
{"id": "2512.17635", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.17635", "abs": "https://arxiv.org/abs/2512.17635", "authors": ["Yuri Taglieri Sáo", "Olivier Roustant", "Geraldo de Freitas Maciel"], "title": "Estimation and model errors in Gaussian-process-based Sensitivity Analysis of functional outputs", "comment": "Submitted to ESAIM - Probability and Statistics (ISSN: 1292-8100 - eISSN: 1262-3318)", "summary": "Global sensitivity analysis (GSA) of functional-output models is usually performed by combining statistical techniques, such as basis expansions, metamodeling and sampling based estimation of sensitivity indices. By neglecting truncation error from basis expansion, two main sources of errors propagate to the final sensitivity indices: the metamodeling related error and the sampling-based, or pick-freeze (PF), estimation error. This work provides an efficient algorithm to estimate these errors in the frame of Gaussian processes (GP), based on the approach of Le Gratiet et al. [16]. The proposed algorithm takes advantage of the fact that the number of basis coefficients of expanded model outputs is significantly smaller than output dimensions. Basis coefficients are fitted by GP models and multiple conditional GP trajectories are sampled. Then, vector-valued PF estimation is used to speed-up the estimation of Sobol indices and generalized sensitivity indices (GSI). We illustrate the methodology on an analytical test case and on an application in non-Newtonian hydraulics, modelling an idealized dam-break flow. Numerical tests show an improvement of 15 times in the computational time when compared to the application of Le Gratiet et al. [16] algorithm separately over each output dimension."}
{"id": "2512.17868", "categories": ["stat.CO", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.17868", "abs": "https://arxiv.org/abs/2512.17868", "authors": ["Kevin Bitterlich", "Daniel Rudolf", "Björn Sprungk"], "title": "Delayed Acceptance Slice Sampling", "comment": "37 pages, 4 figures", "summary": "Slice sampling is a well-established Markov chain Monte Carlo method for (approximate) sampling of target distributions which are only known up to a normalizing constant. The method is based on choosing a new state on a slice, i.e., a superlevel set of the given unnormalized target density (with respect to a reference measure). However, slice sampling algorithms usually require per step multiple evaluations of the target density, and thus can become computationally expensive. This is particularly the case for Bayesian inference with costly likelihoods. In this paper, we exploit deterministic approximations of the target density, which are relatively cheap to evaluate, and propose delayed acceptance versions of hybrid slice samplers. We show ergodicity of the resulting slice sampling methods, discuss the superiority of delayed acceptance (ideal) slice sampling over delayed acceptance Metropolis-Hastings algorithms, and illustrate the benefits of our novel approach in terms improved computational efficiency in several numerical experiments."}
{"id": "2512.17301", "categories": ["stat.ME", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.17301", "abs": "https://arxiv.org/abs/2512.17301", "authors": ["Ratbek Dzhumashev", "Ainura Tursunalieva"], "title": "A Synthetic Instrumental Variable Method: Using the Dual Tendency Condition for Coplanar Instruments", "comment": null, "summary": "Traditional instrumental variable (IV) methods often struggle with weak or invalid instruments and rely heavily on external data. We introduce a Synthetic Instrumental Variable (SIV) approach that constructs valid instruments using only existing data. Our method leverages a data-driven dual tendency (DT) condition to identify valid instruments without requiring external variables. SIV is robust to heteroscedasticity and can determine the true sign of the correlation between endogenous regressors and errors--an assumption typically imposed in empirical work. Through simulations and real-world applications, we show that SIV improves causal inference by mitigating common IV limitations and reducing dependence on scarce instruments. This approach has broad implications for economics, epidemiology, and policy evaluation."}
{"id": "2512.17341", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17341", "abs": "https://arxiv.org/abs/2512.17341", "authors": ["Jikai Jin", "Vasilis Syrgkanis"], "title": "Sharp Structure-Agnostic Lower Bounds for General Functional Estimation", "comment": "95 pages; generalize and subsume partial results of arXiv:2402.14264 by the same authors", "summary": "The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \\citet{jin2024structure} by the same authors."}
{"id": "2512.17659", "categories": ["stat.ML", "cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17659", "abs": "https://arxiv.org/abs/2512.17659", "authors": ["Madhav R. Muthyala", "Farshud Sorourifar", "Tianhong Tan", "You Peng", "Joel A. Paulson"], "title": "Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design", "comment": null, "summary": "Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular \"generate-then-optimize\" framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications."}
{"id": "2512.17340", "categories": ["stat.ME", "cs.CY", "cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.17340", "abs": "https://arxiv.org/abs/2512.17340", "authors": ["Carter H. Nakamoto", "Lucia Lushi Chen", "Agata Foryciarz", "Sherri Rose"], "title": "Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease", "comment": null, "summary": "Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit."}
{"id": "2512.17635", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.17635", "abs": "https://arxiv.org/abs/2512.17635", "authors": ["Yuri Taglieri Sáo", "Olivier Roustant", "Geraldo de Freitas Maciel"], "title": "Estimation and model errors in Gaussian-process-based Sensitivity Analysis of functional outputs", "comment": "Submitted to ESAIM - Probability and Statistics (ISSN: 1292-8100 - eISSN: 1262-3318)", "summary": "Global sensitivity analysis (GSA) of functional-output models is usually performed by combining statistical techniques, such as basis expansions, metamodeling and sampling based estimation of sensitivity indices. By neglecting truncation error from basis expansion, two main sources of errors propagate to the final sensitivity indices: the metamodeling related error and the sampling-based, or pick-freeze (PF), estimation error. This work provides an efficient algorithm to estimate these errors in the frame of Gaussian processes (GP), based on the approach of Le Gratiet et al. [16]. The proposed algorithm takes advantage of the fact that the number of basis coefficients of expanded model outputs is significantly smaller than output dimensions. Basis coefficients are fitted by GP models and multiple conditional GP trajectories are sampled. Then, vector-valued PF estimation is used to speed-up the estimation of Sobol indices and generalized sensitivity indices (GSI). We illustrate the methodology on an analytical test case and on an application in non-Newtonian hydraulics, modelling an idealized dam-break flow. Numerical tests show an improvement of 15 times in the computational time when compared to the application of Le Gratiet et al. [16] algorithm separately over each output dimension."}
{"id": "2512.17689", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17689", "abs": "https://arxiv.org/abs/2512.17689", "authors": ["Pegah Golchian", "Marvin N. Wright"], "title": "Imputation Uncertainty in Interpretable Machine Learning Methods", "comment": "19 pages, 15 Figures, accepted at conference: IJCAI 2025 Workshop on Explainable Artificial Intelligence (Montreal, Canada)", "summary": "In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage."}
{"id": "2512.17401", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17401", "abs": "https://arxiv.org/abs/2512.17401", "authors": ["Jiajun Sun", "Zhanrui Cai", "Wei Zhong"], "title": "A General Stability Approach to False Discovery Rate Control", "comment": null, "summary": "Stability and reproducibility are essential considerations in various applications of statistical methods. False Discovery Rate (FDR) control methods are able to control false signals in scientific discoveries. However, many FDR control methods, such as Model-X knockoff and data-splitting approaches, yield unstable results due to the inherent randomness of the algorithms. To enhance the stability and reproducibility of statistical outcomes, we propose a general stability approach for FDR control in feature selection and multiple testing problems, named FDR Stabilizer. Taking feature selection as an example, our method first aggregates feature importance statistics obtained by multiple runs of the base FDR control procedure into a consensus ranking. Then, we construct a stabilized relaxed e-value for each feature and apply the e-BH procedure to these stabilized e-values to obtain the final selection set. We theoretically derive the finite-sample bounds for the FDR and the power of our method, and show that our method asymptotically controls the FDR without power loss. Moreover, we establish the stability of the proposed method, showing that the stabilized selection set converges to a deterministic limit as the number of repetitions increases. Extensive numerical experiments and applications to real datasets demonstrate that the proposed method generally outperforms existing alternatives."}
{"id": "2512.17868", "categories": ["stat.CO", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.17868", "abs": "https://arxiv.org/abs/2512.17868", "authors": ["Kevin Bitterlich", "Daniel Rudolf", "Björn Sprungk"], "title": "Delayed Acceptance Slice Sampling", "comment": "37 pages, 4 figures", "summary": "Slice sampling is a well-established Markov chain Monte Carlo method for (approximate) sampling of target distributions which are only known up to a normalizing constant. The method is based on choosing a new state on a slice, i.e., a superlevel set of the given unnormalized target density (with respect to a reference measure). However, slice sampling algorithms usually require per step multiple evaluations of the target density, and thus can become computationally expensive. This is particularly the case for Bayesian inference with costly likelihoods. In this paper, we exploit deterministic approximations of the target density, which are relatively cheap to evaluate, and propose delayed acceptance versions of hybrid slice samplers. We show ergodicity of the resulting slice sampling methods, discuss the superiority of delayed acceptance (ideal) slice sampling over delayed acceptance Metropolis-Hastings algorithms, and illustrate the benefits of our novel approach in terms improved computational efficiency in several numerical experiments."}
{"id": "2512.17340", "categories": ["stat.ME", "cs.CY", "cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.17340", "abs": "https://arxiv.org/abs/2512.17340", "authors": ["Carter H. Nakamoto", "Lucia Lushi Chen", "Agata Foryciarz", "Sherri Rose"], "title": "Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease", "comment": null, "summary": "Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit."}
{"id": "2512.17471", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.17471", "abs": "https://arxiv.org/abs/2512.17471", "authors": ["Maria F. Pintado", "Matteo Iacopini", "Luca Rossini", "Alexander Y. Shestopaloff"], "title": "Bayesian Markov-Switching Partial Reduced-Rank Regression", "comment": "28 pages; Supplement available upon request to the author", "summary": "Reduced-Rank (RR) regression is a powerful dimensionality reduction technique but it overlooks any possible group configuration among the responses by assuming a low-rank structure on the entire coefficient matrix. Moreover, the temporal change of the relations between predictors and responses in time series induce a possibly time-varying grouping structure in the responses. To address these limitations, a Bayesian Markov-switching partial RR (MS-PRR) model is proposed, where the response vector is partitioned in two groups to reflect different complexity of the relationship. A \\textit{simple} group assumes a low-rank linear regression, while a \\textit{complex} group exploits nonparametric regression via a Gaussian Process. Differently from traditional approaches, group assignments and rank are treated as unknown parameters to be estimated. Then temporal persistence in the regression function is accounted for by a Markov-switching process that drives the changes in the grouping structure and model parameters over time. Full Bayesian inference is preformed via a partially collapsed Gibbs sampler, which allows uncertainty quantification without the need for trans-dimensional moves. Applications to two real-world macroeconomic and commodity data demonstrate the evidence of time-varying grouping and different degrees of complexity both across states and within each state."}
{"id": "2512.17496", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17496", "abs": "https://arxiv.org/abs/2512.17496", "authors": ["Maya N. Vienken", "Jan-Ole Koslik", "Roland Langrock"], "title": "Inference on state occupancy in covariate-driven hidden Markov models", "comment": null, "summary": "Hidden Markov models (HMMs) are popular tools for analysing animal behaviour based on movement, acceleration and other sensor data. In particular, these models allow to infer how the animal's decision-making process interacts with internal and external drivers, by relating the probabilities of switching between distinct behavioural states to covariates. A key challenge arising in the statistical analysis of behavioural data using covariate-driven HMMs is the models' interpretation, especially when there are more than two states, as then several functional relationships between state-switching probabilities and covariates need to be jointly interpreted. The model-implied probabilities of occupying the different states, as a function of a covariate of interest, constitute a much simpler summary statistic. A pragmatic approximation of the state occupancy distribution, namely the hypothetical stationary distribution of the model's underlying Markov chain for fixed covariate values, has in fact routinely been reported in HMM-based analyses of ecological data. However, for stochastically varying covariates with relatively little persistence, we show that this approximation can be severely biased, potentially invalidating ecological inference. We develop two alternative approaches for obtaining the state occupancy distribution as a function of a covariate of interest - one based on resampling of the covariate process, the other obtained by regression analysis of the empirical state probabilities. The practical application of these approaches is demonstrated in simulations and a case study on Galápagos tortoise (Chelonoidis niger) movement data. Our methods enable practitioners to conduct unbiased inference on the relationship between animal behaviour and general types of covariates, thus allowing to uncover the factors influencing behavioural decisions made by animals."}
{"id": "2512.17635", "categories": ["stat.ME", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.17635", "abs": "https://arxiv.org/abs/2512.17635", "authors": ["Yuri Taglieri Sáo", "Olivier Roustant", "Geraldo de Freitas Maciel"], "title": "Estimation and model errors in Gaussian-process-based Sensitivity Analysis of functional outputs", "comment": "Submitted to ESAIM - Probability and Statistics (ISSN: 1292-8100 - eISSN: 1262-3318)", "summary": "Global sensitivity analysis (GSA) of functional-output models is usually performed by combining statistical techniques, such as basis expansions, metamodeling and sampling based estimation of sensitivity indices. By neglecting truncation error from basis expansion, two main sources of errors propagate to the final sensitivity indices: the metamodeling related error and the sampling-based, or pick-freeze (PF), estimation error. This work provides an efficient algorithm to estimate these errors in the frame of Gaussian processes (GP), based on the approach of Le Gratiet et al. [16]. The proposed algorithm takes advantage of the fact that the number of basis coefficients of expanded model outputs is significantly smaller than output dimensions. Basis coefficients are fitted by GP models and multiple conditional GP trajectories are sampled. Then, vector-valued PF estimation is used to speed-up the estimation of Sobol indices and generalized sensitivity indices (GSI). We illustrate the methodology on an analytical test case and on an application in non-Newtonian hydraulics, modelling an idealized dam-break flow. Numerical tests show an improvement of 15 times in the computational time when compared to the application of Le Gratiet et al. [16] algorithm separately over each output dimension."}
{"id": "2512.17701", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17701", "abs": "https://arxiv.org/abs/2512.17701", "authors": ["Bernardo Flores", "Yang Ni", "Yanxun Xu", "Peter Müller"], "title": "A Dependent Feature Allocation Model Based on Random Fields", "comment": null, "summary": "We introduce a flexible framework for modeling dependent feature allocations. Our approach addresses limitations in traditional nonparametric methods by directly modeling the logit-probability surface of the feature paintbox, enabling the explicit incorporation of covariates and complex but tractable dependence structures. The core of our model is a Gaussian Markov Random Field (GMRF), which we use to robustly decompose the latent field, separating a structural component based on the baseline covariates from intrinsic, unstructured heterogeneity. This structure is not a rigid grid but a sparse k-nearest neighbors graph derived from the latent geometry in the data, ensuring high-dimensional tractability. We extend this framework to a dynamic spatio-temporal process, allowing item effects to evolve via an Ornstein-Uhlenbeck process. Feature correlations are captured using a low-rank factorization of their joint prior. We demonstrate our model's utility by applying it to a polypharmacy dataset, successfully inferring latent health conditions from patient drug profiles."}
{"id": "2512.17737", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17737", "abs": "https://arxiv.org/abs/2512.17737", "authors": ["Filip Tronarp"], "title": "Recursive state estimation via approximate modal paths", "comment": null, "summary": "In this paper, a method for recursively computing approximate modal paths is developed. A recursive formulation of the modal path can be obtained either by backward or forward dynamic programming. By combining both methods, a ``two-filter'' formula is demonstrated. Both method involves a recursion over a so-called value function, which is intractable in general. This problem is overcome by quadratic approximation of the value function in the forward dynamic programming paradigm, resulting in both a filtering and smoothing method. The merit of the approach is verified in a simulation experiments, where it is shown to be on par or better than other modern algorithms."}
{"id": "2512.17119", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17119", "abs": "https://arxiv.org/abs/2512.17119", "authors": ["Laura Pardo", "Juan Sosa"], "title": "Uncovering latent territorial structure in ICFES Saber 11 performance with Bayesian multilevel spatial models", "comment": "42 pages, 14 tables, 12 figures", "summary": "This article develops a Bayesian hierarchical framework to analyze academic performance in the 2022 second semester Saber 11 examination in Colombia. Our approach combines multilevel regression with municipal and departmental spatial random effects, and it incorporates Ridge and Lasso regularization priors to compare the contribution of sociodemographic covariates. Inference is implemented in a fully open source workflow using Markov chain Monte Carlo methods, and model behavior is assessed through synthetic data that mirror key features of the observed data. Simulation results indicate that Ridge provides the most balanced performance in parameter recovery, predictive accuracy, and sampling efficiency, while Lasso shows weaker fit and posterior stability, with gains in predictive accuracy under stronger multicollinearity. In the application, posterior rankings show a strong centralization of performance, with higher scores in central departments and lower scores in peripheral territories, and the strongest correlates of scores are student level living conditions, maternal education, access to educational resources, gender, and ethnic background, while spatial random effects capture residual regional disparities. A hybrid Bayesian segmentation based on K means propagates posterior uncertainty into clustering at departmental, municipal, and spatial scales, revealing multiscale territorial patterns consistent with structural inequalities and informing territorial targeting in education policy."}
{"id": "2512.17341", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17341", "abs": "https://arxiv.org/abs/2512.17341", "authors": ["Jikai Jin", "Vasilis Syrgkanis"], "title": "Sharp Structure-Agnostic Lower Bounds for General Functional Estimation", "comment": "95 pages; generalize and subsume partial results of arXiv:2402.14264 by the same authors", "summary": "The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \\citet{jin2024structure} by the same authors."}
{"id": "2512.17478", "categories": ["stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17478", "abs": "https://arxiv.org/abs/2512.17478", "authors": ["Paavo Sattler", "Nils Hichert"], "title": "Inference for high dimensional repeated measure designs with the R package hdrm", "comment": null, "summary": "Repeated-measure designs allow comparisons within a group as well as between groups, and are commonly referred to as split-plot designs. While originating in agricultural experiments, they are now widely used in medical research, psychology, and the life sciences, where repeated observations on the same subject are essential.\n  Modern data collection often produces observation vectors with dimension $d$ comparable to or exceeding the sample size $N$. Although this can be advantageous in terms of cost efficiency, ethical considerations, and the study of rare diseases, it poses substantial challenges for statistical inference.\n  Parametric methods based on multivariate normality provide a flexible framework that avoids restrictive assumptions on covariance structures or on the asymptotic relationship between $d$ and $N$. Within this framework, the freely available R-package hdrm enables the analysis of a wide range of hypotheses concerning expectation vectors in high-dimensional repeated-measure designs, covering both single-group and multi-group settings with homogeneous or heterogeneous covariance matrices.\n  This paper describes the implemented tests, demonstrates their use through examples, and discusses their applicability in practical high-dimensional data scenarios. To address computational challenges arising for large $d$, the package incorporates efficient estimators and subsampling strategies that substantially reduce computation time while preserving statistical validity."}
{"id": "2512.17689", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.17689", "abs": "https://arxiv.org/abs/2512.17689", "authors": ["Pegah Golchian", "Marvin N. Wright"], "title": "Imputation Uncertainty in Interpretable Machine Learning Methods", "comment": "19 pages, 15 Figures, accepted at conference: IJCAI 2025 Workshop on Explainable Artificial Intelligence (Montreal, Canada)", "summary": "In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage."}
