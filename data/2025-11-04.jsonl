{"id": "2510.27498", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.27498", "abs": "https://arxiv.org/abs/2510.27498", "authors": ["Binh Thuan Tran", "Nicolas Schreuder"], "title": "Minimax-Optimal Two-Sample Test with Sliced Wasserstein", "comment": null, "summary": "We study the problem of nonparametric two-sample testing using the sliced\nWasserstein (SW) distance. While prior theoretical and empirical work indicates\nthat the SW distance offers a promising balance between strong statistical\nguarantees and computational efficiency, its theoretical foundations for\nhypothesis testing remain limited. We address this gap by proposing a\npermutation-based SW test and analyzing its performance. The test inherits\nfinite-sample Type I error control from the permutation principle. Moreover, we\nestablish non-asymptotic power bounds and show that the procedure achieves the\nminimax separation rate $n^{-1/2}$ over multinomial and bounded-support\nalternatives, matching the optimal guarantees of kernel-based tests while\nbuilding on the geometric foundations of Wasserstein distances. Our analysis\nfurther quantifies the trade-off between the number of projections and\nstatistical power. Finally, numerical experiments demonstrate that the test\ncombines finite-sample validity with competitive power and scalability, and --\nunlike kernel-based tests, which require careful kernel tuning -- it performs\nconsistently well across all scenarios we consider."}
{"id": "2510.27056", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27056", "abs": "https://arxiv.org/abs/2510.27056", "authors": ["Arman Bolatov", "Alan Legg", "Igor Melnykov", "Amantay Nurlanuly", "Maxat Tezekbayev", "Zhenisbek Assylbekov"], "title": "Overspecified Mixture Discriminant Analysis: Exponential Convergence, Statistical Guarantees, and Remote Sensing Applications", "comment": null, "summary": "This study explores the classification error of Mixture Discriminant Analysis\n(MDA) in scenarios where the number of mixture components exceeds those present\nin the actual data distribution, a condition known as overspecification. We use\na two-component Gaussian mixture model within each class to fit data generated\nfrom a single Gaussian, analyzing both the algorithmic convergence of the\nExpectation-Maximization (EM) algorithm and the statistical classification\nerror. We demonstrate that, with suitable initialization, the EM algorithm\nconverges exponentially fast to the Bayes risk at the population level.\nFurther, we extend our results to finite samples, showing that the\nclassification error converges to Bayes risk with a rate $n^{-1/2}$ under mild\nconditions on the initial parameter estimates and sample size. This work\nprovides a rigorous theoretical framework for understanding the performance of\noverspecified MDA, which is often used empirically in complex data settings,\nsuch as image and text classification. To validate our theory, we conduct\nexperiments on remote sensing datasets."}
{"id": "2510.27562", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.27562", "abs": "https://arxiv.org/abs/2510.27562", "authors": ["Yuchen Jiao", "Yuchen Zhou", "Gen Li"], "title": "Optimal Convergence Analysis of DDPM for General Distributions", "comment": null, "summary": "Score-based diffusion models have achieved remarkable empirical success in\ngenerating high-quality samples from target data distributions. Among them, the\nDenoising Diffusion Probabilistic Model (DDPM) is one of the most widely used\nsamplers, generating samples via estimated score functions. Despite its\nempirical success, a tight theoretical understanding of DDPM -- especially its\nconvergence properties -- remains limited.\n  In this paper, we provide a refined convergence analysis of the DDPM sampler\nand establish near-optimal convergence rates under general distributional\nassumptions. Specifically, we introduce a relaxed smoothness condition\nparameterized by a constant $L$, which is small for many practical\ndistributions (e.g., Gaussian mixture models). We prove that the DDPM sampler\nwith accurate score estimates achieves a convergence rate of\n$$\\widetilde{O}\\left(\\frac{d\\min\\{d,L^2\\}}{T^2}\\right)~\\text{in\nKullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the\nnumber of iterations, and $\\widetilde{O}$ hides polylogarithmic factors in $T$.\nThis result substantially improves upon the best-known $d^2/T^2$ rate when $L <\n\\sqrt{d}$. By establishing a matching lower bound, we show that our convergence\nanalysis is tight for a wide array of target distributions. Moreover, it\nreveals that DDPM and DDIM share the same dependence on $d$, raising an\ninteresting question of why DDIM often appears empirically faster."}
{"id": "2510.27340", "categories": ["stat.ML"], "pdf": "https://arxiv.org/pdf/2510.27340", "abs": "https://arxiv.org/abs/2510.27340", "authors": ["Ferdinand Genans", "Antoine Godichon-Baggioni", "François-Xavier Vialard", "Olivier Wintenberger"], "title": "Decreasing Entropic Regularization Averaged Gradient for Semi-Discrete Optimal Transport", "comment": null, "summary": "Adding entropic regularization to Optimal Transport (OT) problems has become\na standard approach for designing efficient and scalable solvers. However,\nregularization introduces a bias from the true solution. To mitigate this bias\nwhile still benefiting from the acceleration provided by regularization, a\nnatural solver would adaptively decrease the regularization as it approaches\nthe solution. Although some algorithms heuristically implement this idea, their\ntheoretical guarantees and the extent of their acceleration compared to using a\nfixed regularization remain largely open. In the setting of semi-discrete OT,\nwhere the source measure is continuous and the target is discrete, we prove\nthat decreasing the regularization can indeed accelerate convergence. To this\nend, we introduce DRAG: Decreasing (entropic) Regularization Averaged Gradient,\na stochastic gradient descent algorithm where the regularization decreases with\nthe number of optimization steps. We provide a theoretical analysis showing\nthat DRAG benefits from decreasing regularization compared to a fixed scheme,\nachieving an unbiased $\\mathcal{O}(1/t)$ sample and iteration complexity for\nboth the OT cost and the potential estimation, and a $\\mathcal{O}(1/\\sqrt{t})$\nrate for the OT map. Our theoretical findings are supported by numerical\nexperiments that validate the effectiveness of DRAG and highlight its practical\nadvantages."}
{"id": "2510.27593", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.27593", "abs": "https://arxiv.org/abs/2510.27593", "authors": ["Derik T. Boonstra", "Rakheon Kim", "Dean M. Young"], "title": "Subspace Ordering for Maximum Response Preservation in Sufficient Dimension Reduction", "comment": null, "summary": "Sufficient dimension reduction (SDR) methods aim to identify a dimension\nreduction subspace (DRS) that preserves all the information about the\nconditional distribution of a response given its predictor. Traditional SDR\nmethods determine the DRS by solving a method-specific generalized eigenvalue\nproblem and selecting the eigenvectors corresponding to the largest\neigenvalues. In this article, we argue against the long-standing convention of\nusing eigenvalues as the measure of subspace importance and propose alternative\nordering criteria that directly assess the predictive relevance of each\nsubspace. For a binary response, we introduce a subspace ordering criterion\nbased on the absolute value of the independent Student's t-statistic.\nTheoretically, our criterion identifies subspaces that achieve the local\nminimum Bayes' error rate and yields consistent ordering of directions under\nmild regularity conditions. Additionally, we employ an F-statistic to provide a\nframework that unifies categorical and continuous responses under a single\nsubspace criterion. We evaluate our proposed criteria within multiple SDR\nmethods through extensive simulation studies and applications to real data. Our\nempirical results demonstrate the efficacy of reordering subspaces using our\nproposed criteria, which generally improves classification accuracy and\nsubspace estimation compared to ordering by eigenvalues."}
{"id": "2510.27385", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27385", "abs": "https://arxiv.org/abs/2510.27385", "authors": ["Nikita Kornilov", "Alexander Korotin"], "title": "On the Equivalence of Optimal Transport Problem and Action Matching with Optimal Vector Fields", "comment": null, "summary": "Flow Matching (FM) method in generative modeling maps arbitrary probability\ndistributions by constructing an interpolation between them and then learning\nthe vector field that defines ODE for this interpolation. Recently, it was\nshown that FM can be modified to map distributions optimally in terms of the\nquadratic cost function for any initial interpolation. To achieve this, only\nspecific optimal vector fields, which are typical for solutions of Optimal\nTransport (OT) problems, need to be considered during FM loss minimization. In\nthis note, we show that considering only optimal vector fields can lead to OT\nin another approach: Action Matching (AM). Unlike FM, which learns a vector\nfield for a manually chosen interpolation between given distributions, AM\nlearns the vector field that defines ODE for an entire given sequence of\ndistributions."}
{"id": "2510.27397", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27397", "abs": "https://arxiv.org/abs/2510.27397", "authors": ["Joshua S. Harvey", "Guanchao Feng", "Sai Anusha Meesala", "Tina Zhao", "Dhagash Mehta"], "title": "Interpretable Model-Aware Counterfactual Explanations for Random Forest", "comment": "Presented at XAI-FIN-2025: International Joint Workshop on\n  Explainable AI in Finance: Achieving Trustworthy Financial Decision-Making;\n  November 15, 2025; Singapore", "summary": "Despite their enormous predictive power, machine learning models are often\nunsuitable for applications in regulated industries such as finance, due to\ntheir limited capacity to provide explanations. While model-agnostic frameworks\nsuch as Shapley values have proved to be convenient and popular, they rarely\nalign with the kinds of causal explanations that are typically sought after.\nCounterfactual case-based explanations, where an individual is informed of\nwhich circumstances would need to be different to cause a change in outcome,\nmay be more intuitive and actionable. However, finding appropriate\ncounterfactual cases is an open challenge, as is interpreting which features\nare most critical for the change in outcome. Here, we pose the question of\ncounterfactual search and interpretation in terms of similarity learning,\nexploiting the representation learned by the random forest predictive model\nitself. Once a counterfactual is found, the feature importance of the\nexplanation is computed as a function of which random forest partitions are\ncrossed in order to reach it from the original instance. We demonstrate this\nmethod on both the MNIST hand-drawn digit dataset and the German credit\ndataset, finding that it generates explanations that are sparser and more\nuseful than Shapley values."}
{"id": "2510.26812", "categories": ["stat.ME", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26812", "abs": "https://arxiv.org/abs/2510.26812", "authors": ["Garima Jain", "Anand Bodade", "Sanghamitra Pati"], "title": "Impact of clinical decision support systems (cdss) on clinical outcomes and healthcare delivery in low- and middle-income countries: protocol for a systematic review and meta-analysis", "comment": "10 pages, 3 tables", "summary": "Clinical decision support systems (CDSS) are used to improve clinical and\nservice outcomes, yet evidence from low- and middle-income countries (LMICs) is\ndispersed. This protocol outlines methods to quantify the impact of CDSS on\npatient and healthcare delivery outcomes in LMICs. We will include comparative\nquantitative designs (randomized trials, controlled before-after, interrupted\ntime series, comparative cohorts) evaluating CDSS in World Bank-defined LMICs.\nStandalone qualitative studies are excluded; mixed-methods studies are eligible\nonly if they report comparative quantitative outcomes, for which we will\nextract the quantitative component. Searches (from inception to 30 September\n2024) will cover MEDLINE, Embase, CINAHL, CENTRAL, Web of Science, Global\nHealth, Scopus, IEEE Xplore, LILACS, African Index Medicus, and IndMED, plus\ngrey sources. Screening and extraction will be performed in duplicate. Risk of\nbias will be assessed with RoB 2 (randomized trials) and ROBINS-I\n(non-randomized). Random-effects meta-analysis will be performed where outcomes\nare conceptually or statistically comparable; otherwise, a structured narrative\nsynthesis will be presented. Heterogeneity will be explored using relative and\nabsolute metrics and a priori subgroups or meta-regression (condition area,\ncare level, CDSS type, readiness proxies, study design)."}
{"id": "2510.26982", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.26982", "abs": "https://arxiv.org/abs/2510.26982", "authors": ["Ziling Ma", "Ángel López-Oriona", "Hernando Ombao", "Ying Sun"], "title": "Robust fuzzy clustering for high-dimensional multivariate time series with outlier detection", "comment": null, "summary": "Fuzzy clustering provides a natural framework for modeling partial\nmemberships, particularly important in multivariate time series (MTS) where\nstate boundaries are often ambiguous. For example, in EEG monitoring of driver\nalertness, neural activity evolves along a continuum (from unconscious to fully\nalert, with many intermediate levels of drowsiness) so crisp labels are\nunrealistic and partial memberships are essential. However, most existing\nalgorithms are developed for static, low-dimensional data and struggle with\ntemporal dependence, unequal sequence lengths, high dimensionality, and\ncontamination by noise or artifacts. To address these challenges, we introduce\nRFCPCA, a robust fuzzy subspace-clustering method explicitly tailored to MTS\nthat, to the best of our knowledge, is the first of its kind to simultaneously:\n(i) learn membership-informed subspaces, (ii) accommodate unequal lengths and\nmoderately high dimensions, (iii) achieve robustness through trimming,\nexponential reweighting, and a dedicated noise cluster, and (iv) automatically\nselect all required hyperparameters. These components enable RFCPCA to capture\nlatent temporal structure, provide calibrated membership uncertainty, and flag\nseries-level outliers while remaining stable under contamination. On driver\ndrowsiness EEG, RFCPCA improves clustering accuracy over related methods and\nyields a more reliable characterization of uncertainty and outlier structure in\nMTS."}
{"id": "2510.26807", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26807", "abs": "https://arxiv.org/abs/2510.26807", "authors": ["Yuhan Tang"], "title": "Diabetes Lifestyle Medicine Treatment Assistance Using Reinforcement Learning", "comment": null, "summary": "Type 2 diabetes prevention and treatment can benefit from personalized\nlifestyle prescriptions. However, the delivery of personalized lifestyle\nmedicine prescriptions is limited by the shortage of trained professionals and\nthe variability in physicians' expertise. We propose an offline contextual\nbandit approach that learns individualized lifestyle prescriptions from the\naggregated NHANES profiles of 119,555 participants by minimizing the Magni\nglucose risk-reward function. The model encodes patient status and generates\nlifestyle medicine prescriptions, which are trained using a mixed-action Soft\nActor-Critic algorithm. The task is treated as a single-step contextual bandit.\nThe model is validated against lifestyle medicine prescriptions issued by three\ncertified physicians from Xiangya Hospital. These results demonstrate that\noffline mixed-action SAC can generate risk-aware lifestyle medicine\nprescriptions from cross-sectional NHANES data, warranting prospective clinical\nvalidation."}
{"id": "2510.27498", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.27498", "abs": "https://arxiv.org/abs/2510.27498", "authors": ["Binh Thuan Tran", "Nicolas Schreuder"], "title": "Minimax-Optimal Two-Sample Test with Sliced Wasserstein", "comment": null, "summary": "We study the problem of nonparametric two-sample testing using the sliced\nWasserstein (SW) distance. While prior theoretical and empirical work indicates\nthat the SW distance offers a promising balance between strong statistical\nguarantees and computational efficiency, its theoretical foundations for\nhypothesis testing remain limited. We address this gap by proposing a\npermutation-based SW test and analyzing its performance. The test inherits\nfinite-sample Type I error control from the permutation principle. Moreover, we\nestablish non-asymptotic power bounds and show that the procedure achieves the\nminimax separation rate $n^{-1/2}$ over multinomial and bounded-support\nalternatives, matching the optimal guarantees of kernel-based tests while\nbuilding on the geometric foundations of Wasserstein distances. Our analysis\nfurther quantifies the trade-off between the number of projections and\nstatistical power. Finally, numerical experiments demonstrate that the test\ncombines finite-sample validity with competitive power and scalability, and --\nunlike kernel-based tests, which require careful kernel tuning -- it performs\nconsistently well across all scenarios we consider."}
{"id": "2510.26914", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.26914", "abs": "https://arxiv.org/abs/2510.26914", "authors": ["Sam Allen", "Enrico Pescara", "Johanna Ziegel"], "title": "Residual Distribution Predictive Systems", "comment": null, "summary": "Conformal predictive systems are sets of predictive distributions with\ntheoretical out-of-sample calibration guarantees. The calibration guarantees\nare typically that the set of predictions contains a forecast distribution\nwhose prediction intervals exhibit the correct marginal coverage at all levels.\nConformal predictive systems are constructed using conformity measures that\nquantify how well possible outcomes conform with historical data. However,\nalternative methods have been proposed to construct predictive systems with\nmore appealing theoretical properties. We study an approach to construct\npredictive systems that we term Residual Distribution Predictive Systems. In\nthe split conformal setting, this approach nests conformal predictive systems\nwith a popular class of conformity measures, providing an alternative\nperspective on the classical approach. In the full conformal setting, the two\napproaches differ, and the new approach has the advantage that it does not rely\non a conformity measure satisfying fairly stringent requirements to ensure that\nthe predictive system is well-defined; it can readily be implemented alongside\nany point-valued regression method to yield predictive systems with\nout-of-sample calibration guarantees. The empirical performance of this\napproach is assessed using simulated data, where it is found to perform\ncompetitively with conformal predictive systems. However, the new approach\noffers considerable scope for implementation with alternative regression\nmethods."}
{"id": "2510.27479", "categories": ["stat.CO"], "pdf": "https://arxiv.org/pdf/2510.27479", "abs": "https://arxiv.org/abs/2510.27479", "authors": ["María del Carmen Romero", "Mariana del Fresno", "Alejandro Clausse"], "title": "Differential Set Selection via Confidence-Guided Entropy Minimization", "comment": "14 pages, 3 figures", "summary": "This paper addresses the challenge of identifying a minimal subset of\ndiscrete, independent variables that best predicts a binary class. We propose\nan efficient iterative method that sequentially selects variables based on\nwhich one provides the most statistically significant reduction in conditional\nentropy, using confidence bounds to account for finite-sample uncertainty.\nTests on simulated data demonstrate the method's ability to correctly identify\ninfluential variables while minimizing spurious selections, even with small\nsample sizes, offering a computationally tractable solution to this NP-complete\nproblem."}
{"id": "2510.26808", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26808", "abs": "https://arxiv.org/abs/2510.26808", "authors": ["Audrey Dong", "Claire Xu", "Samuel R. Guo", "Kevin Yang", "Xue-Jun Kong"], "title": "A Machine Learning-Based Framework to Shorten the Questionnaire for Assessing Autism Intervention", "comment": "10 pages, 16 figures", "summary": "Caregivers of individuals with autism spectrum disorder (ASD) often find the\n77-item Autism Treatment Evaluation Checklist (ATEC) burdensome, limiting its\nuse for routine monitoring. This study introduces a generalizable machine\nlearning framework that seeks to shorten assessments while maintaining\nevaluative accuracy. Using longitudinal ATEC data from 60 autistic children\nreceiving therapy, we applied feature selection and cross-validation techniques\nto identify the most predictive items across two assessment goals: longitudinal\ntherapy tracking and point-in-time severity estimation. For progress\nmonitoring, the framework identified 16 items (21% of the original\nquestionnaire) that retained strong correlation with total score change and\nfull subdomain coverage. We also generated smaller subsets (1-7 items) for\nefficient approximations. For point-in-time severity assessment, our model\nachieved over 80% classification accuracy using just 13 items (17% of the\noriginal set). While demonstrated on ATEC, the methodology-based on subset\noptimization, model interpretability, and statistical rigor-is broadly\napplicable to other high-dimensional psychometric tools. The resulting\nframework could potentially enable more accessible, frequent, and scalable\nassessments and offer a data-driven approach for AI-supported interventions\nacross neurodevelopmental and psychiatric contexts."}
{"id": "2510.27562", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.27562", "abs": "https://arxiv.org/abs/2510.27562", "authors": ["Yuchen Jiao", "Yuchen Zhou", "Gen Li"], "title": "Optimal Convergence Analysis of DDPM for General Distributions", "comment": null, "summary": "Score-based diffusion models have achieved remarkable empirical success in\ngenerating high-quality samples from target data distributions. Among them, the\nDenoising Diffusion Probabilistic Model (DDPM) is one of the most widely used\nsamplers, generating samples via estimated score functions. Despite its\nempirical success, a tight theoretical understanding of DDPM -- especially its\nconvergence properties -- remains limited.\n  In this paper, we provide a refined convergence analysis of the DDPM sampler\nand establish near-optimal convergence rates under general distributional\nassumptions. Specifically, we introduce a relaxed smoothness condition\nparameterized by a constant $L$, which is small for many practical\ndistributions (e.g., Gaussian mixture models). We prove that the DDPM sampler\nwith accurate score estimates achieves a convergence rate of\n$$\\widetilde{O}\\left(\\frac{d\\min\\{d,L^2\\}}{T^2}\\right)~\\text{in\nKullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the\nnumber of iterations, and $\\widetilde{O}$ hides polylogarithmic factors in $T$.\nThis result substantially improves upon the best-known $d^2/T^2$ rate when $L <\n\\sqrt{d}$. By establishing a matching lower bound, we show that our convergence\nanalysis is tight for a wide array of target distributions. Moreover, it\nreveals that DDPM and DDIM share the same dependence on $d$, raising an\ninteresting question of why DDIM often appears empirically faster."}
{"id": "2510.26917", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.26917", "abs": "https://arxiv.org/abs/2510.26917", "authors": ["Pavel Hernández-Amaro", "Maria Durban", "M. Carmen Aguilera-Morillo"], "title": "A novel generalized additive scalar-on-function regression model for partially observed multidimensional functional data: An application to air quality classification", "comment": null, "summary": "In this work we propose a generalized additive functional regression model\nfor partially observed functional data. Our approach accommodates functional\npredictors of varying dimensions without requiring imputation of missing\nobservations. Both the functional coefficients and covariates are represented\nusing basis function expansions, with B-splines used in this study, though the\nmethod is not restricted to any specific basis choice. Model coefficients are\nestimated via penalized likelihood, leveraging the mixed model representation\nof penalized splines for efficient computation and smoothing parameter\nestimation.The performance of the proposed approach is assessed through two\nsimulation studies: one involving two one-dimensional functional covariates,\nand another using a two-dimensional functional covariate. Finally, we\ndemonstrate the practical utility of our method in an application to\nair-pollution classification in Dimapur, India, where images are treated as\nobservations of a two-dimensional functional variable. This case study\nhighlights the models ability to effectively handle incomplete functional data\nand to accurately discriminate between pollution levels."}
{"id": "2510.26930", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.26930", "abs": "https://arxiv.org/abs/2510.26930", "authors": ["Nina Deliu", "Brunero Liseo"], "title": "The Interplay between Bayesian Inference and Conformal Prediction", "comment": "16 pages, 2 figures", "summary": "Conformal prediction has emerged as a cutting-edge methodology in statistics\nand machine learning, providing prediction intervals with finite-sample\nfrequentist coverage guarantees. Yet, its interplay with Bayesian statistics,\noften criticised for lacking frequentist guarantees, remains underexplored.\nRecent work has suggested that conformal prediction can serve to \"calibrate\"\nBayesian credible sets, thereby imparting frequentist validity and motivating\ndeeper investigation into frequentist-Bayesian hybrids. We further argue that\nBayesian procedures have the potential to enhance conformal prediction, not\nonly in terms of more informative intervals, but also for achieving nearly\noptimal solutions under a decision-theoretic framework. Thus, the two paradigms\ncan be jointly used for a principled balance between validity and efficiency.\nThis work provides a basis for bridging this gap. After surveying existing\nideas, we formalise the Bayesian conformal inference framework, covering\nchallenging aspects such as statistical efficiency and computational\ncomplexity."}
{"id": "2510.26809", "categories": ["stat.AP", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2510.26809", "abs": "https://arxiv.org/abs/2510.26809", "authors": ["Jochen Bröcker", "Eviatar Bach"], "title": "A generalisation of the signal-to-noise ratio using proper scoring rules", "comment": "16 pages, 2 figures, 3 tables", "summary": "A generalised concept of the signal-to-noise ratio (or equivalently the ratio\nof predictable components, or RPC) is provided, based on proper scoring rules.\nThis definition is the natural generalisation of the classical RPC, yet it\nallows one to define and analyse the signal-to-noise properties of any type of\nforecast that is amenable to scoring, thus drastically widening the\napplicability of these concepts. The methodology is illustrated for ensemble\nforecasts, scored using the continuous ranked probability score (CRPS), and for\nprobability forecasts of a binary event, scored using the logarithmic score.\nNumerical examples are demonstrated using synthetic data with prescribed\nsignal-to-noise ratios as well as seasonal ensemble hindcasts of the North\nAtlantic Oscillation (NAO) index. For the synthetic data, the RPC statistic as\nwell as the scoring rule--based ones agree regarding which data sets exhibit\nanomalous signal-to-noise ratios, but exhibit different variance, indicating\ndifferent statistical properties. For the NAO data, on the other hand, the\nresults among the different statistics are more equivocal."}
{"id": "2510.27643", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "math.OC", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.27643", "abs": "https://arxiv.org/abs/2510.27643", "authors": ["Wenwen Li", "Daniel Sanz-Alonso", "Ruiyi Yang"], "title": "Bayesian Optimization on Networks", "comment": "36 pages, 6 figures; includes appendices", "summary": "This paper studies optimization on networks modeled as metric graphs.\nMotivated by applications where the objective function is expensive to evaluate\nor only available as a black box, we develop Bayesian optimization algorithms\nthat sequentially update a Gaussian process surrogate model of the objective to\nguide the acquisition of query points. To ensure that the surrogates are\ntailored to the network's geometry, we adopt Whittle-Mat\\'ern Gaussian process\nprior models defined via stochastic partial differential equations on metric\ngraphs. In addition to establishing regret bounds for optimizing sufficiently\nsmooth objective functions, we analyze the practical case in which the\nsmoothness of the objective is unknown and the Whittle-Mat\\'ern prior is\nrepresented using finite elements. Numerical results demonstrate the\neffectiveness of our algorithms for optimizing benchmark objective functions on\na synthetic metric graph and for Bayesian inversion via maximum a posteriori\nestimation on a telecommunication network."}
{"id": "2510.26929", "categories": ["stat.ME", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.26929", "abs": "https://arxiv.org/abs/2510.26929", "authors": ["Rodrigo A. González", "Koen Classens", "Cristian R. Rojas", "Tom Oomen", "Håkan Hjalmarsson"], "title": "Finite Sample MIMO System Identification with Multisine Excitation: Nonparametric, Direct, and Two-step Parametric Estimators", "comment": "16 pages, 4 figures", "summary": "Multisine excitations are widely used for identifying multi-input\nmulti-output systems due to their periodicity, data compression properties, and\ncontrol over the input spectrum. Despite their popularity, the finite sample\nstatistical properties of frequency-domain estimators under multisine\nexcitation, for both nonparametric and parametric settings, remain\ninsufficiently understood. This paper develops a finite-sample statistical\nframework for least-squares estimation of the frequency response function (FRF)\nand its implications for parametric modeling. First, we derive exact\ndistributional and covariance properties of the FRF estimator, explicitly\naccounting for aliasing effects under slow sampling regimes, and establish\nconditions for unbiasedness, uncorrelatedness, and consistency across multiple\nexperiments. Second, we show that the FRF estimate is a sufficient statistic\nfor any parametric model under Gaussian noise, leading to an exact equivalence\nbetween optimal two stage frequency-domain methods and time-domain prediction\nerror and maximum likelihood estimation. This equivalence is shown to yield\nfinite-sample concentration bounds for parametric maximum likelihood\nestimators, enabling rigorous uncertainty quantification, and closed-form\nprediction error method estimators without iterative optimization. The\ntheoretical results are demonstrated in a representative case study."}
{"id": "2510.27643", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "math.OC", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.27643", "abs": "https://arxiv.org/abs/2510.27643", "authors": ["Wenwen Li", "Daniel Sanz-Alonso", "Ruiyi Yang"], "title": "Bayesian Optimization on Networks", "comment": "36 pages, 6 figures; includes appendices", "summary": "This paper studies optimization on networks modeled as metric graphs.\nMotivated by applications where the objective function is expensive to evaluate\nor only available as a black box, we develop Bayesian optimization algorithms\nthat sequentially update a Gaussian process surrogate model of the objective to\nguide the acquisition of query points. To ensure that the surrogates are\ntailored to the network's geometry, we adopt Whittle-Mat\\'ern Gaussian process\nprior models defined via stochastic partial differential equations on metric\ngraphs. In addition to establishing regret bounds for optimizing sufficiently\nsmooth objective functions, we analyze the practical case in which the\nsmoothness of the objective is unknown and the Whittle-Mat\\'ern prior is\nrepresented using finite elements. Numerical results demonstrate the\neffectiveness of our algorithms for optimizing benchmark objective functions on\na synthetic metric graph and for Bayesian inversion via maximum a posteriori\nestimation on a telecommunication network."}
{"id": "2510.26810", "categories": ["stat.AP", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.26810", "abs": "https://arxiv.org/abs/2510.26810", "authors": ["Tatsuru Kikuchi"], "title": "Emergent Dynamical Spatial Boundaries in Emergency Medical Services: A Navier-Stokes Framework from First Principles", "comment": "110 pages, 11 figures", "summary": "Emergency medical services (EMS) response times are critical determinants of\npatient survival, yet existing approaches to spatial coverage analysis rely on\ndiscrete distance buffers or ad-hoc geographic information system (GIS)\nisochrones without theoretical foundation. This paper derives continuous\nspatial boundaries for emergency response from first principles using fluid\ndynamics (Navier-Stokes equations), demonstrating that response effectiveness\ndecays exponentially with time: $\\tau(t) = \\tau_0 \\exp(-\\kappa t)$, where\n$\\tau_0$ is baseline effectiveness and $\\kappa$ is the temporal decay rate.\nUsing 10,000 simulated emergency incidents from the National Emergency Medical\nServices Information System (NEMSIS), I estimate decay parameters and calculate\ncritical boundaries $d^*$ where response effectiveness falls below\npolicy-relevant thresholds. The framework reveals substantial demographic\nheterogeneity: elderly populations (85+) experience 8.40-minute average\nresponse times versus 7.83 minutes for younger adults (18-44), with 33.6\\% of\npoor-access incidents affecting elderly populations despite representing 5.2\\%\nof the sample. Non-parametric kernel regression validation confirms exponential\ndecay is appropriate (mean squared error 8-12 times smaller than parametric),\nwhile traditional difference-in-differences analysis validates treatment effect\nexistence (DiD coefficient = -1.35 minutes, $p < 0.001$). The analysis\nidentifies vulnerable populations--elderly, rural, and low-income\ncommunities--facing systematically longer response times, informing optimal EMS\nstation placement and resource allocation to reduce health disparities."}
{"id": "2510.26914", "categories": ["stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.26914", "abs": "https://arxiv.org/abs/2510.26914", "authors": ["Sam Allen", "Enrico Pescara", "Johanna Ziegel"], "title": "Residual Distribution Predictive Systems", "comment": null, "summary": "Conformal predictive systems are sets of predictive distributions with\ntheoretical out-of-sample calibration guarantees. The calibration guarantees\nare typically that the set of predictions contains a forecast distribution\nwhose prediction intervals exhibit the correct marginal coverage at all levels.\nConformal predictive systems are constructed using conformity measures that\nquantify how well possible outcomes conform with historical data. However,\nalternative methods have been proposed to construct predictive systems with\nmore appealing theoretical properties. We study an approach to construct\npredictive systems that we term Residual Distribution Predictive Systems. In\nthe split conformal setting, this approach nests conformal predictive systems\nwith a popular class of conformity measures, providing an alternative\nperspective on the classical approach. In the full conformal setting, the two\napproaches differ, and the new approach has the advantage that it does not rely\non a conformity measure satisfying fairly stringent requirements to ensure that\nthe predictive system is well-defined; it can readily be implemented alongside\nany point-valued regression method to yield predictive systems with\nout-of-sample calibration guarantees. The empirical performance of this\napproach is assessed using simulated data, where it is found to perform\ncompetitively with conformal predictive systems. However, the new approach\noffers considerable scope for implementation with alternative regression\nmethods."}
{"id": "2510.26930", "categories": ["stat.ME", "stat.CO"], "pdf": "https://arxiv.org/pdf/2510.26930", "abs": "https://arxiv.org/abs/2510.26930", "authors": ["Nina Deliu", "Brunero Liseo"], "title": "The Interplay between Bayesian Inference and Conformal Prediction", "comment": "16 pages, 2 figures", "summary": "Conformal prediction has emerged as a cutting-edge methodology in statistics\nand machine learning, providing prediction intervals with finite-sample\nfrequentist coverage guarantees. Yet, its interplay with Bayesian statistics,\noften criticised for lacking frequentist guarantees, remains underexplored.\nRecent work has suggested that conformal prediction can serve to \"calibrate\"\nBayesian credible sets, thereby imparting frequentist validity and motivating\ndeeper investigation into frequentist-Bayesian hybrids. We further argue that\nBayesian procedures have the potential to enhance conformal prediction, not\nonly in terms of more informative intervals, but also for achieving nearly\noptimal solutions under a decision-theoretic framework. Thus, the two paradigms\ncan be jointly used for a principled balance between validity and efficiency.\nThis work provides a basis for bridging this gap. After surveying existing\nideas, we formalise the Bayesian conformal inference framework, covering\nchallenging aspects such as statistical efficiency and computational\ncomplexity."}
{"id": "2510.26811", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.26811", "abs": "https://arxiv.org/abs/2510.26811", "authors": ["Iman Mohamed Attia"], "title": "Proxy Variable in OECD Database: Application of Parametric Quantile Regression and Median Based Unit Rayleigh Distribution", "comment": null, "summary": "This paper presents an in-depth exploration of the innovative Median-based\nunit Rayleigh (MBUR) distribution, previously introduced by the author. This\nnew approach is specifically designed for conducting quantile regression\nanalysis, enabling researchers to gain valuable insights into real-world data\napplications. The author effectively demonstrates the feasible advantage of the\nMBUR distribution, highlighting its potential to connect advanced statistical\ntheory with meaningful results in data analysis. The author utilized OECD data\nin employing the parametric MBUR quantile regression using the response\nvariables which are distributed as MBUR."}
{"id": "2510.26982", "categories": ["stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.26982", "abs": "https://arxiv.org/abs/2510.26982", "authors": ["Ziling Ma", "Ángel López-Oriona", "Hernando Ombao", "Ying Sun"], "title": "Robust fuzzy clustering for high-dimensional multivariate time series with outlier detection", "comment": null, "summary": "Fuzzy clustering provides a natural framework for modeling partial\nmemberships, particularly important in multivariate time series (MTS) where\nstate boundaries are often ambiguous. For example, in EEG monitoring of driver\nalertness, neural activity evolves along a continuum (from unconscious to fully\nalert, with many intermediate levels of drowsiness) so crisp labels are\nunrealistic and partial memberships are essential. However, most existing\nalgorithms are developed for static, low-dimensional data and struggle with\ntemporal dependence, unequal sequence lengths, high dimensionality, and\ncontamination by noise or artifacts. To address these challenges, we introduce\nRFCPCA, a robust fuzzy subspace-clustering method explicitly tailored to MTS\nthat, to the best of our knowledge, is the first of its kind to simultaneously:\n(i) learn membership-informed subspaces, (ii) accommodate unequal lengths and\nmoderately high dimensions, (iii) achieve robustness through trimming,\nexponential reweighting, and a dedicated noise cluster, and (iv) automatically\nselect all required hyperparameters. These components enable RFCPCA to capture\nlatent temporal structure, provide calibrated membership uncertainty, and flag\nseries-level outliers while remaining stable under contamination. On driver\ndrowsiness EEG, RFCPCA improves clustering accuracy over related methods and\nyields a more reliable characterization of uncertainty and outlier structure in\nMTS."}
{"id": "2510.26995", "categories": ["stat.ME", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.26995", "abs": "https://arxiv.org/abs/2510.26995", "authors": ["Elliot L. Epstein", "John Winnicki", "Thanawat Sornwanee", "Rajat Dwaraknath"], "title": "LLMs are Overconfident: Evaluating Confidence Interval Calibration with FermiEval", "comment": "8 pages", "summary": "Large language models (LLMs) excel at numerical estimation but struggle to\ncorrectly quantify uncertainty. We study how well LLMs construct confidence\nintervals around their own answers and find that they are systematically\noverconfident. To evaluate this behavior, we introduce FermiEval, a benchmark\nof Fermi-style estimation questions with a rigorous scoring rule for confidence\ninterval coverage and sharpness. Across several modern models, nominal 99\\%\nintervals cover the true answer only 65\\% of the time on average. With a\nconformal prediction based approach that adjusts the intervals, we obtain\naccurate 99\\% observed coverage, and the Winkler interval score decreases by\n54\\%. We also propose direct log-probability elicitation and quantile\nadjustment methods, which further reduce overconfidence at high confidence\nlevels. Finally, we develop a perception-tunnel theory explaining why LLMs\nexhibit overconfidence: when reasoning under uncertainty, they act as if\nsampling from a truncated region of their inferred distribution, neglecting its\ntails."}
{"id": "2510.26814", "categories": ["stat.AP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26814", "abs": "https://arxiv.org/abs/2510.26814", "authors": ["Angela Davey", "Arthur Leroy", "Eliana Vasquez Osorio", "Kate Vaughan", "Peter Clayton", "Marcel van Herk", "Mauricio A Alvarez", "Martin McCabe", "Marianne Aznar"], "title": "Towards Gaussian processes modelling to study the late effects of radiotherapy in children and young adults with brain tumours", "comment": "Presented at the XXth International Conference on the Use of\n  Computers in Radiation Therapy", "summary": "Survivors of childhood cancer need lifelong monitoring for side effects from\nradiotherapy. However, longitudinal data from routine monitoring is often\ninfrequently and irregularly sampled, and subject to inaccuracies. Due to this,\nmeasurements are often studied in isolation, or simple relationships (e.g.,\nlinear) are used to impute missing timepoints. In this study, we investigated\nthe potential role of Gaussian Processes (GP) modelling to make\npopulation-based and individual predictions, using insulin-like growth factor 1\n(IGF-1) measurements as a test case. With training data of 23 patients with a\nmedian (range) of 4 (1-16) timepoints we identified a trend within the range of\nliterature reported values. In addition, with 8 test cases, individual\npredictions were made with an average root mean squared error of 31.9 (10.1 -\n62.3) ng/ml and 27.4 (0.02 - 66.1) ng/ml for two approaches. GP modelling may\novercome limitations of routine longitudinal data and facilitate analysis of\nlate effects of radiotherapy."}
{"id": "2510.27006", "categories": ["stat.ME", "physics.data-an", "q-bio.PE", "62F30 (Primary) 62B10, 94A17 (Secondary)", "G.3; I.2.6; G.2.2"], "pdf": "https://arxiv.org/pdf/2510.27006", "abs": "https://arxiv.org/abs/2510.27006", "authors": ["Giuseppe M. Ferro", "Edwin T. Pos", "Andrea Somazzi"], "title": "Generalized Maximum Entropy: When and Why you need it", "comment": "Equal contribution: G.M.F., E.T.P., and A.S. contributed equally.\n  Submitted to Royal Society Open Science", "summary": "The classical Maximum-Entropy Principle (MEP) based on Shannon entropy is\nwidely used to construct least-biased probability distributions from partial\ninformation. However, the Shore-Johnson axioms that single out the Shannon\nfunctional hinge on strong system independence, an assumption often violated in\nreal-world, strongly correlated systems. We provide a self-contained guide to\nwhen and why practitioners should abandon the Shannon form in favour of the\none-parameter Uffink-Jizba-Korbel (UJK) family of generalized entropies. After\nreviewing the Shore and Johnson axioms from an applied perspective, we recall\nthe most commonly used entropy functionals and locate them within the UJK\nfamily. The need for generalized entropies is made clear with two applications,\none rooted in economics and the other in ecology. A simple mathematical model\nworked out in detail shows the power of generalized maximum entropy approaches\nin dealing with cases where strong system independence does not hold. We\nconclude with practical guidelines for choosing an entropy measure and\nreporting results so that analyses remain transparent and reproducible."}
{"id": "2510.26815", "categories": ["stat.AP", "cs.LG", "62H30, 68T05, 86A60", "I.2.6; I.5.3; H.2.8"], "pdf": "https://arxiv.org/pdf/2510.26815", "abs": "https://arxiv.org/abs/2510.26815", "authors": ["Dipal Shah", "Jordon Wade", "Timothy Haithcoat", "Robert Myers", "Kelly Wilson"], "title": "Toward precision soil health: A regional framework for site-specific management across Missouri", "comment": "20 pages, 5 figures", "summary": "Effective soil health management is crucial for sustaining agriculture,\nadopting ecosystem resilience, and preserving water quality. However,\nMissouri's diverse landscapes limit the effectiveness of broad generalized\nmanagement recommendations. The lack of resolution in existing soil grouping\nsystems necessitates data driven, site specific insights to guide tailored\ninterventions. To address these critical challenges, a regional soil clustering\nframework designed to support precision soil health management strategies\nacross the state. The methodology leveraged high resolution SSURGO dataset,\nexplicitly processing soil properties aggregated across the 0 to 30 cm root\nzone. Multivariate analysis incorporating a variational autoencoder and KMeans\nclustering was used to group soils with similar properties. The derived\nclusters were validated using statistical metrics, including silhouette scores\nand checks against existing taxonomic units, to confirm their spatial\ncoherence. This approach enabled us to delineate soil groups that capture\ntextures, hydraulic properties, chemical fertility, and biological indicators\nunique to Missouri's diverse agroecological regions. The clustering map\nidentified ten distinct soil health management zones. This alignment of 10\nclusters was selected as optimal because it was sufficiently large to capture\ninherited soil patterns while remaining manageable for practical statewide\napplication. Rooting depth limitation and saturated hydraulic conductivity\nemerged as principal variables driving soil differentiation. Each management\nzone is defined by a unique combination of clay, organic matter, pH, and\navailable water capacity. This framework bridges sophisticated data analysis\nwith actionable, site targeted recommendations, enabling conservation planners,\nand agronomists to optimize management practices and enhance resource\nefficiency statewide."}
{"id": "2510.27011", "categories": ["stat.ME", "math.OC", "90B50, 91B08"], "pdf": "https://arxiv.org/pdf/2510.27011", "abs": "https://arxiv.org/abs/2510.27011", "authors": ["Kolos Csaba Ágoston", "László Csató"], "title": "Inconsistency thresholds revisited: The effect of the graph associated with incomplete pairwise comparisons", "comment": "20 pages, 6 figures, 4 tables", "summary": "The inconsistency of pairwise comparisons remains difficult to interpret in\nthe absence of acceptability thresholds. The popular 10% cut-off rule proposed\nby Saaty has recently been applied to incomplete pairwise comparison matrices,\nwhich contain some unknown comparisons. This paper revises these inconsistency\nthresholds: we uncover that they depend not only on the size of the matrix and\nthe number of missing entries, but also on the undirected graph whose edges\nrepresent the known pairwise comparisons. Therefore, using our exact thresholds\nis especially important if the filling in patterns coincide for a large number\nof matrices, as has been recommended in the literature. The strong association\nbetween the new threshold values and the spectral radius of the representing\ngraph is also demonstrated. Our results can be integrated into software to\ncontinuously monitor inconsistency during the collection of pairwise\ncomparisons and immediately detect potential errors."}
{"id": "2510.26816", "categories": ["stat.AP", "astro-ph.IM", "cs.AI", "62H30, 62P12, 86A32", "I.2.6; I.5.2; J.2"], "pdf": "https://arxiv.org/pdf/2510.26816", "abs": "https://arxiv.org/abs/2510.26816", "authors": ["Rohit Rajendra Dhage"], "title": "Systematic Absence of Low-Confidence Nighttime Fire Detections in VIIRS Active Fire Product: Evidence of Undocumented Algorithmic Filtering", "comment": "11 pages, 8 tables, 2 algorithms. Submitted to ArXiv for open access\n  dissemination prior to journal submission", "summary": "The Visible Infrared Imaging Radiometer Suite (VIIRS) active fire product is\nwidely used for global fire monitoring, yet its confidence classification\nscheme exhibits an undocumented systematic pattern. Through analysis of\n21,540,921 fire detections spanning one year (January 2023 - January 2024), I\ndemonstrate a complete absence of low-confidence classifications during\nnighttime observations. Of 6,007,831 nighttime fires, zero were classified as\nlow confidence, compared to an expected 696,908 under statistical independence\n(chi-squared = 1,474,795, p < 10^-15, Z = -833). This pattern persists globally\nacross all months, latitude bands, and both NOAA-20 and Suomi-NPP satellites.\nMachine learning reverse-engineering (88.9% accuracy), bootstrap simulation\n(1,000 iterations), and spatial-temporal analysis confirm this is an\nalgorithmic constraint rather than a geophysical phenomenon. Brightness\ntemperature analysis reveals nighttime fires below approximately 295K are\nlikely excluded entirely rather than flagged as low-confidence, while daytime\nfires show normal confidence distributions. This undocumented behavior affects\n27.9% of all VIIRS fire detections and has significant implications for fire\nrisk assessment, day-night detection comparisons, confidence-weighted analyses,\nand any research treating confidence levels as uncertainty metrics. I recommend\nexplicit documentation of this algorithmic constraint in VIIRS user guides and\nreprocessing strategies for affected analyses."}
{"id": "2510.27144", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.27144", "abs": "https://arxiv.org/abs/2510.27144", "authors": ["Yang Liu", "Youjin Sung", "Jonathan P. Williams", "Jan Hannig"], "title": "Calibrating Bayesian Inference", "comment": null, "summary": "While Bayesian statistics is popular in psychological research for its\nintuitive uncertainty quantification and flexible decision-making, its\nperformance in finite samples can be unreliable. In this paper, we demonstrate\na key vulnerability: When analysts' chosen prior distribution mismatches the\ntrue parameter-generating process, Bayesian inference can be misleading in the\nlong run. Given that this true process is rarely known in practice, we propose\na safer alternative: calibrating Bayesian credible regions to achieve\nfrequentist validity. This latter criterion is stronger and guarantees validity\nof Bayesian inference regardless of the underlying parameter-generating\nmechanism. To solve the calibration problem in practice, we propose a novel\nstochastic approximation algorithm. A Monte Carlo experiment is conducted and\nreported, in which we observe that uncalibrated Bayesian inference can be\nliberal under certain parameter-generating scenarios, whereas our calibrated\nsolution is always able to maintain validity."}
{"id": "2510.27204", "categories": ["stat.AP"], "pdf": "https://arxiv.org/pdf/2510.27204", "abs": "https://arxiv.org/abs/2510.27204", "authors": ["Arthur Charpentier", "Qiheng Guo", "Mike Ludkovski"], "title": "Functional Analysis of Loss-development Patterns in P&C Insurance", "comment": "34 pages. Keywords: loss development; loss reserving; incremental\n  loss ratios; unsupervised learning; functional data; functional depth;\n  outlier detection; IBNR", "summary": "We analyze loss development in NAIC Schedule P loss triangles using\nfunctional data analysis methods. Adopting the functional viewpoint, our\ndataset comprises 3300+ curves of incremental loss ratios (ILR) of workers'\ncompensation lines over 24 accident years. Relying on functional data depth, we\nfirst study similarities and differences in development patterns based on\ncompany-specific covariates, as well as identify anomalous ILR curves.\n  The exploratory findings motivate the probabilistic forecasting framework\ndeveloped in the second half of the paper. We propose a functional model to\ncomplete partially developed ILR curves based on partial least squares\nregression of PCA scores. Coupling the above with functional bootstrapping\nallows us to quantify future ILR uncertainty jointly across all future lags. We\ndemonstrate that our method has much better probabilistic scores relative to\nChain Ladder and in particular can provide accurate functional predictive\nintervals."}
{"id": "2510.27150", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.27150", "abs": "https://arxiv.org/abs/2510.27150", "authors": ["Linh Do", "Dat Do", "Keisha J. Cook", "Scott A. McKinley"], "title": "Change-in-velocity detection for multidimensional data", "comment": null, "summary": "In this work, we introduce CPLASS (Continuous Piecewise-Linear Approximation\nvia Stochastic Search), an algorithm for detecting changes in velocity within\nmultidimensional data. The one-dimensional version of this problem is known as\nthe change-in-slope problem (see Fearnhead & Grose (2022), Baranowski et al.\n(2019)). Unlike traditional changepoint detection methods that focus on changes\nin mean, detecting changes in velocity requires a specialized approach due to\ncontinuity constraints and parameter dependencies, which frustrate popular\nalgorithms like binary segmentation and dynamic programming. To overcome these\ndifficulties, we introduce a specialized penalty function to balance\nimprovements in likelihood due to model complexity, and a Markov Chain Monte\nCarlo (MCMC)-based approach with tailored proposal mechanisms for efficient\nparameter exploration. Our method is particularly suited for analyzing\nintracellular transport data, where the multidimensional trajectories of\nmicroscale cargo are driven by teams of molecular motors that undergo complex\nbiophysical transitions. To ensure biophysical realism in the results, we\nintroduce a speed penalty that discourages overfitted of short noisy segments\nwhile maintaining consistency in the large-sample limit. Additionally, we\nintroduce a summary statistic called the Cumulative Speed Allocation, which is\nrobust with respect to idiosyncracies of changepoint detection while\nmaintaining the ability to discriminate between biophysically distinct\npopulations."}
{"id": "2510.27456", "categories": ["stat.AP", "J.2; I.2"], "pdf": "https://arxiv.org/pdf/2510.27456", "abs": "https://arxiv.org/abs/2510.27456", "authors": ["John Bagiliko", "David Stern", "Denis Ndanguza", "Francis Feehi Torgbor", "Danny Parsons", "Samuel Owusu Ansah"], "title": "Bias correction of satellite and reanalysis products for daily rainfall occurrence and intensity", "comment": null, "summary": "Satellite and reanalysis rainfall products (SREs) can serve as valuable\ncomplements or alternatives in data-sparse regions, but their significant\nbiases necessitate correction. This study rigorously evaluates a suite of bias\ncorrection (BC) methods, including statistical approaches (LOCI, QM), machine\nlearning (SVR, GPR), and hybrid techniques (LOCI-GPR, QM-GPR), applied to seven\nSREs across 38 stations in Ghana and Zambia, aimed at assessing their\nperformance in rainfall detection and intensity estimation. Results indicate\nthat the ENACTS product, which uniquely integrates a large number of station\nrecords, was the most corrigible SRE; in Zambia, nearly all BC methods\nsuccessfully reduced the mean error on daily rainfall amounts at over 70% of\nstations. However, this performance requires further validation at independent\nstations not incorporated into the ENACTS product. Overall, the statistical\nmethods (QM and LOCI) generally outperformed other techniques, although QM\nexhibited a tendency to inflate rainfall values. All corrected SREs\ndemonstrated a high capability for detecting dry days (POD $\\ge$ 0.80),\nsuggesting their potential utility for drought applications. A critical\nlimitation persisted, however, as most SREs and BC methods consistently failed\nto improve the detection of heavy and violent rainfall events (POD $\\leq$ 0.2),\nhighlighting a crucial area for future research."}
{"id": "2510.27365", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.27365", "abs": "https://arxiv.org/abs/2510.27365", "authors": ["Touqeer Ahmad", "Abid Hussain"], "title": "Flexible model for varying levels of zeros and outliers in count data", "comment": "19 pages, 6 figures, and 1 table", "summary": "Count regression models are necessary for examining discrete dependent\nvariables alongside covariates. Nonetheless, when data display outliers,\noverdispersion, and an abundance of zeros, traditional methods like the\nzero-inflated negative binomial (ZINB) model sometimes do not yield a\nsatisfactory fit, especially in the tail regions. This research presents a\nversatile, heavy-tailed discrete model as a resilient substitute for the ZINB\nmodel. The suggested framework is built by extending the generalized Pareto\ndistribution and its zero-inflated version to the discrete domain. This\nformulation efficiently addresses both overdispersion and zero inflation,\nproviding increased flexibility for heavy-tailed count data. Through intensive\nsimulation studies and real-world implementations, the proposed models are\nthoroughly tested to see how well they work. The results show that our models\nalways do better than classic negative binomial and zero-inflated negative\nbinomial regressions when it comes to goodness-of-fit. This is especially true\nfor datasets with a lot of zeros and outliers. These results highlight the\nproposed framework's potential as a strong and flexible option for modeling\ncomplicated count data."}
{"id": "2510.27551", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.27551", "abs": "https://arxiv.org/abs/2510.27551", "authors": ["Michela Frigeri", "Veronica Berrocal", "Alessandra Guglielmi"], "title": "Bayesian Source Apportionment of Spatio-temporal air pollution data", "comment": null, "summary": "Understanding the sources that contribute to fine particulate matter\n(PM$_{2.5}$) is of crucial importance for designing and implementing targeted\nair pollution mitigation strategies. Determining what factors contribute to a\npollutant's concentration goes under the name of source apportionment and it is\na problem long studied by atmospheric scientists and statisticians alike. In\nthis paper, we propose a Bayesian model for source apportionment, that advances\nthe literature on source apportionment by allowing estimation of the number of\nsources and accounting for spatial and temporal dependence in the observed\npollutants' concentrations. Taking as example observations of six species of\nfine particulate matter observed over the course of a year, we present a latent\nfunctional factor model that expresses the space-time varying observations of\nlog concentrations of the six pollutant as a linear combination of space-time\nvarying emissions produced by an unknown number of sources each multiplied by\nthe corresponding source's relative contribution to the pollutant. Estimation\nof the number of sources is achieved by introducing source-specific shrinkage\nparameters. Application of the model to simulated data showcases its ability to\nretrieve the true number of sources and to reliably estimate the functional\nlatent factors, whereas application to PM$_{2.5}$ speciation data in California\nidentifies 3 major sources for the six PM$_{2.5}$ species."}
{"id": "2510.27519", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.27519", "abs": "https://arxiv.org/abs/2510.27519", "authors": ["Shinichi Nakagawa", "Ayumi Mizuno", "Coralie Williams", "Santiago Ortega", "Szymon M. Drobniak", "Malgorzata Lagisz", "Yefeng Yang", "Alistair M. Senior", "Daniel W. A. Noble", "Erick Lundgren"], "title": "Mastering an Accurate and Generalizable Simulation-Based Method to Obtain Bias-corrected Point Estimates and Sampling Variance for Any Effect Sizes", "comment": null, "summary": "Meta-analyses require an effect-size estimate and its corresponding sampling\nvariance from primary studies. In some cases, estimators for the sampling\nvariance of a given effect size statistic may not exist, necessitating the\nderivation of a new formula for sampling variance. Traditionally, sampling\nvariance formulas are obtained via hand-derived Taylor expansions (the delta\nmethod), though this procedure can be challenging for non-statisticians.\nBuilding on the idea of single-fit parametric resampling, we introduce SAFE\nbootstrap: a Single-fit, Accurate, Fast, and Easy simulation recipe that\nreplaces potentially complex algebra with four intuitive steps: fit, draw,\ntransform, and summarise. In a unified framework, the SAFE bootstrap yields\nbias-corrected point estimates and standard errors for any effect size\nstatistic, regardless of whether the outcome is continuous or discrete. SAFE\nbootstrapping works by drawing once from a simple sampling model (normal,\nbinomial, etc.), converting each replicate into any effect size of interest and\nthen calculating the bias and sampling variance from simulated data. We\ndemonstrate how to implement the SAFE bootstrap for a simple example first, and\nthen for common effect sizes, such as the standardised mean difference and log\nodds ratio, as well as for less common effect sizes. With some additional\ncoding, SAFE can also handle zero values and small sample sizes. Our tutorial,\nwith R code supplements, should not only enhance understanding of sampling\nvariance for effect sizes, but also serve as an introduction to the power of\nsimulation-based methods for deriving any effect size with bias correction and\nits associated sampling variance."}
{"id": "2510.26917", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.26917", "abs": "https://arxiv.org/abs/2510.26917", "authors": ["Pavel Hernández-Amaro", "Maria Durban", "M. Carmen Aguilera-Morillo"], "title": "A novel generalized additive scalar-on-function regression model for partially observed multidimensional functional data: An application to air quality classification", "comment": null, "summary": "In this work we propose a generalized additive functional regression model\nfor partially observed functional data. Our approach accommodates functional\npredictors of varying dimensions without requiring imputation of missing\nobservations. Both the functional coefficients and covariates are represented\nusing basis function expansions, with B-splines used in this study, though the\nmethod is not restricted to any specific basis choice. Model coefficients are\nestimated via penalized likelihood, leveraging the mixed model representation\nof penalized splines for efficient computation and smoothing parameter\nestimation.The performance of the proposed approach is assessed through two\nsimulation studies: one involving two one-dimensional functional covariates,\nand another using a two-dimensional functional covariate. Finally, we\ndemonstrate the practical utility of our method in an application to\nair-pollution classification in Dimapur, India, where images are treated as\nobservations of a two-dimensional functional variable. This case study\nhighlights the models ability to effectively handle incomplete functional data\nand to accurately discriminate between pollution levels."}
{"id": "2510.27580", "categories": ["stat.ME"], "pdf": "https://arxiv.org/pdf/2510.27580", "abs": "https://arxiv.org/abs/2510.27580", "authors": ["Michael Doerfler", "Wenhao Mao", "Lin Ge", "Yuzi Zhang", "Timothy L. Lash", "Kevin C. Ward", "Lance A. Waller", "Robert H. Lyles"], "title": "Refining capture-recapture methods to estimate case counts in a finite population setting", "comment": "24 pages, 12 tables", "summary": "In this paper, we expand upon and refine a monitoring strategy proposed for\nsurveillance of diseases in finite, closed populations. This monitoring\nstrategy consists of augmenting an arbitrarily non-representative data stream\n(such as a voluntary flu testing program) with a random sample (referred to as\nan \"anchor stream\"). This design allows for the use of traditional\ncapture-recapture (CRC) estimators, as well as recently proposed anchor stream\nestimators that more efficiently utilize the data. Here, we focus on a\nparticularly common situation in which the first data stream only records\npositive test results, while the anchor stream documents both positives and\nnegatives. Due to the non-representative nature of the first data stream, along\nwith the fact that inference is being performed on a finite, closed population,\nthere are standard and non-standard finite population effects at play. Here, we\npropose two methods of incorporating finite population corrections (FPCs) for\ninference, along with an FPC-adjusted Bayesian credible interval. We compare\nthese approaches with existing methods through simulation and demonstrate that\nthe FPC adjustments can lead to considerable gains in precision. Finally, we\nprovide a real data example by applying these methods to estimating the breast\ncancer recurrence count among Metro Atlanta-area patients in the Georgia Cancer\nRegistry-based Cancer Recurrence Information and Surveillance Program (CRISP)\ndatabase."}
{"id": "2510.27365", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.27365", "abs": "https://arxiv.org/abs/2510.27365", "authors": ["Touqeer Ahmad", "Abid Hussain"], "title": "Flexible model for varying levels of zeros and outliers in count data", "comment": "19 pages, 6 figures, and 1 table", "summary": "Count regression models are necessary for examining discrete dependent\nvariables alongside covariates. Nonetheless, when data display outliers,\noverdispersion, and an abundance of zeros, traditional methods like the\nzero-inflated negative binomial (ZINB) model sometimes do not yield a\nsatisfactory fit, especially in the tail regions. This research presents a\nversatile, heavy-tailed discrete model as a resilient substitute for the ZINB\nmodel. The suggested framework is built by extending the generalized Pareto\ndistribution and its zero-inflated version to the discrete domain. This\nformulation efficiently addresses both overdispersion and zero inflation,\nproviding increased flexibility for heavy-tailed count data. Through intensive\nsimulation studies and real-world implementations, the proposed models are\nthoroughly tested to see how well they work. The results show that our models\nalways do better than classic negative binomial and zero-inflated negative\nbinomial regressions when it comes to goodness-of-fit. This is especially true\nfor datasets with a lot of zeros and outliers. These results highlight the\nproposed framework's potential as a strong and flexible option for modeling\ncomplicated count data."}
{"id": "2510.27593", "categories": ["stat.ME", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.27593", "abs": "https://arxiv.org/abs/2510.27593", "authors": ["Derik T. Boonstra", "Rakheon Kim", "Dean M. Young"], "title": "Subspace Ordering for Maximum Response Preservation in Sufficient Dimension Reduction", "comment": null, "summary": "Sufficient dimension reduction (SDR) methods aim to identify a dimension\nreduction subspace (DRS) that preserves all the information about the\nconditional distribution of a response given its predictor. Traditional SDR\nmethods determine the DRS by solving a method-specific generalized eigenvalue\nproblem and selecting the eigenvectors corresponding to the largest\neigenvalues. In this article, we argue against the long-standing convention of\nusing eigenvalues as the measure of subspace importance and propose alternative\nordering criteria that directly assess the predictive relevance of each\nsubspace. For a binary response, we introduce a subspace ordering criterion\nbased on the absolute value of the independent Student's t-statistic.\nTheoretically, our criterion identifies subspaces that achieve the local\nminimum Bayes' error rate and yields consistent ordering of directions under\nmild regularity conditions. Additionally, we employ an F-statistic to provide a\nframework that unifies categorical and continuous responses under a single\nsubspace criterion. We evaluate our proposed criteria within multiple SDR\nmethods through extensive simulation studies and applications to real data. Our\nempirical results demonstrate the efficacy of reordering subspaces using our\nproposed criteria, which generally improves classification accuracy and\nsubspace estimation compared to ordering by eigenvalues."}
{"id": "2510.27633", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.27633", "abs": "https://arxiv.org/abs/2510.27633", "authors": ["Gregory Fletcher Cox", "Xiaoxia Shi", "Yuya Shimizu"], "title": "Testing Inequalities Linear in Nuisance Parameters", "comment": "None", "summary": "This paper proposes a new test for inequalities that are linear in possibly\npartially identified nuisance parameters. This type of hypothesis arises in a\nbroad set of problems, including subvector inference for linear unconditional\nmoment (in)equality models, specification testing of such models, and inference\nfor parameters bounded by linear programs. The new test uses a two-step test\nstatistic and a chi-squared critical value with data-dependent degrees of\nfreedom that can be calculated by an elementary formula. Its simple structure\nand tuning-parameter-free implementation make it attractive for practical use.\nWe establish uniform asymptotic validity of the test, demonstrate its\nfinite-sample size and power in simulations, and illustrate its use in an\nempirical application that analyzes women's labor supply in response to a\nwelfare policy reform."}
{"id": "2510.27633", "categories": ["stat.ME", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.27633", "abs": "https://arxiv.org/abs/2510.27633", "authors": ["Gregory Fletcher Cox", "Xiaoxia Shi", "Yuya Shimizu"], "title": "Testing Inequalities Linear in Nuisance Parameters", "comment": "None", "summary": "This paper proposes a new test for inequalities that are linear in possibly\npartially identified nuisance parameters. This type of hypothesis arises in a\nbroad set of problems, including subvector inference for linear unconditional\nmoment (in)equality models, specification testing of such models, and inference\nfor parameters bounded by linear programs. The new test uses a two-step test\nstatistic and a chi-squared critical value with data-dependent degrees of\nfreedom that can be calculated by an elementary formula. Its simple structure\nand tuning-parameter-free implementation make it attractive for practical use.\nWe establish uniform asymptotic validity of the test, demonstrate its\nfinite-sample size and power in simulations, and illustrate its use in an\nempirical application that analyzes women's labor supply in response to a\nwelfare policy reform."}
{"id": "2510.26810", "categories": ["stat.AP", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.26810", "abs": "https://arxiv.org/abs/2510.26810", "authors": ["Tatsuru Kikuchi"], "title": "Emergent Dynamical Spatial Boundaries in Emergency Medical Services: A Navier-Stokes Framework from First Principles", "comment": "110 pages, 11 figures", "summary": "Emergency medical services (EMS) response times are critical determinants of\npatient survival, yet existing approaches to spatial coverage analysis rely on\ndiscrete distance buffers or ad-hoc geographic information system (GIS)\nisochrones without theoretical foundation. This paper derives continuous\nspatial boundaries for emergency response from first principles using fluid\ndynamics (Navier-Stokes equations), demonstrating that response effectiveness\ndecays exponentially with time: $\\tau(t) = \\tau_0 \\exp(-\\kappa t)$, where\n$\\tau_0$ is baseline effectiveness and $\\kappa$ is the temporal decay rate.\nUsing 10,000 simulated emergency incidents from the National Emergency Medical\nServices Information System (NEMSIS), I estimate decay parameters and calculate\ncritical boundaries $d^*$ where response effectiveness falls below\npolicy-relevant thresholds. The framework reveals substantial demographic\nheterogeneity: elderly populations (85+) experience 8.40-minute average\nresponse times versus 7.83 minutes for younger adults (18-44), with 33.6\\% of\npoor-access incidents affecting elderly populations despite representing 5.2\\%\nof the sample. Non-parametric kernel regression validation confirms exponential\ndecay is appropriate (mean squared error 8-12 times smaller than parametric),\nwhile traditional difference-in-differences analysis validates treatment effect\nexistence (DiD coefficient = -1.35 minutes, $p < 0.001$). The analysis\nidentifies vulnerable populations--elderly, rural, and low-income\ncommunities--facing systematically longer response times, informing optimal EMS\nstation placement and resource allocation to reduce health disparities."}
{"id": "2510.26811", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.26811", "abs": "https://arxiv.org/abs/2510.26811", "authors": ["Iman Mohamed Attia"], "title": "Proxy Variable in OECD Database: Application of Parametric Quantile Regression and Median Based Unit Rayleigh Distribution", "comment": null, "summary": "This paper presents an in-depth exploration of the innovative Median-based\nunit Rayleigh (MBUR) distribution, previously introduced by the author. This\nnew approach is specifically designed for conducting quantile regression\nanalysis, enabling researchers to gain valuable insights into real-world data\napplications. The author effectively demonstrates the feasible advantage of the\nMBUR distribution, highlighting its potential to connect advanced statistical\ntheory with meaningful results in data analysis. The author utilized OECD data\nin employing the parametric MBUR quantile regression using the response\nvariables which are distributed as MBUR."}
{"id": "2510.27498", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.27498", "abs": "https://arxiv.org/abs/2510.27498", "authors": ["Binh Thuan Tran", "Nicolas Schreuder"], "title": "Minimax-Optimal Two-Sample Test with Sliced Wasserstein", "comment": null, "summary": "We study the problem of nonparametric two-sample testing using the sliced\nWasserstein (SW) distance. While prior theoretical and empirical work indicates\nthat the SW distance offers a promising balance between strong statistical\nguarantees and computational efficiency, its theoretical foundations for\nhypothesis testing remain limited. We address this gap by proposing a\npermutation-based SW test and analyzing its performance. The test inherits\nfinite-sample Type I error control from the permutation principle. Moreover, we\nestablish non-asymptotic power bounds and show that the procedure achieves the\nminimax separation rate $n^{-1/2}$ over multinomial and bounded-support\nalternatives, matching the optimal guarantees of kernel-based tests while\nbuilding on the geometric foundations of Wasserstein distances. Our analysis\nfurther quantifies the trade-off between the number of projections and\nstatistical power. Finally, numerical experiments demonstrate that the test\ncombines finite-sample validity with competitive power and scalability, and --\nunlike kernel-based tests, which require careful kernel tuning -- it performs\nconsistently well across all scenarios we consider."}
{"id": "2510.27551", "categories": ["stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.27551", "abs": "https://arxiv.org/abs/2510.27551", "authors": ["Michela Frigeri", "Veronica Berrocal", "Alessandra Guglielmi"], "title": "Bayesian Source Apportionment of Spatio-temporal air pollution data", "comment": null, "summary": "Understanding the sources that contribute to fine particulate matter\n(PM$_{2.5}$) is of crucial importance for designing and implementing targeted\nair pollution mitigation strategies. Determining what factors contribute to a\npollutant's concentration goes under the name of source apportionment and it is\na problem long studied by atmospheric scientists and statisticians alike. In\nthis paper, we propose a Bayesian model for source apportionment, that advances\nthe literature on source apportionment by allowing estimation of the number of\nsources and accounting for spatial and temporal dependence in the observed\npollutants' concentrations. Taking as example observations of six species of\nfine particulate matter observed over the course of a year, we present a latent\nfunctional factor model that expresses the space-time varying observations of\nlog concentrations of the six pollutant as a linear combination of space-time\nvarying emissions produced by an unknown number of sources each multiplied by\nthe corresponding source's relative contribution to the pollutant. Estimation\nof the number of sources is achieved by introducing source-specific shrinkage\nparameters. Application of the model to simulated data showcases its ability to\nretrieve the true number of sources and to reliably estimate the functional\nlatent factors, whereas application to PM$_{2.5}$ speciation data in California\nidentifies 3 major sources for the six PM$_{2.5}$ species."}
