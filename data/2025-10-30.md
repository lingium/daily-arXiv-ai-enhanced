<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 16]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.AP](#stat.AP) [Total: 5]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Differential Density Analysis in Single-Cell Genomics Using Specially Designed Exponential Families](https://arxiv.org/abs/2510.24948)
*Hanxuan Ye,Zachary Qian,Hongzhe Li*

Main category: stat.ME

TL;DR: 提出了一个基于特殊设计指数族(SEF)的模型无关框架，用于单细胞RNA测序数据的密度估计和推断，能够进行个体特异性和群体水平的基因表达密度可视化及假设检验。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据的丰富性使得可以将个体基因表达建模为跨细胞的概率密度，这比均值或方差等汇总统计提供了更丰富的表示，允许进行更细致的组间比较。

Method: 基于特殊设计指数族(SEF)的模型无关框架，不依赖先验设定，能够适应各种底层模型，具有宽松的假设和已建立的渐近性质。

Result: 在各种场景下的模拟中，SEF方法显示出良好的误差控制和比竞争方法(包括伪批量检验和矩估计器)更高的统计功效。在系统性红斑狼疮患者的大规模scRNA-seq数据应用中，识别出了伪批量检验遗漏的基因和基因集。

Conclusion: SEF框架为单细胞RNA测序数据提供了有效的密度估计和推断方法，能够发现传统方法可能遗漏的重要生物学发现。

Abstract: Recent advances in high-resolution sequencing have paved the way for
population-scale analysis in single-cell RNA-sequencing (scRNA-seq) data.
scRNA-seq data, in particular, have proven to be extremely powerful in
profiling a variety of outcomes such as disease and aging. The abundance of
scRNA-seq data makes it possible to model each individual's gene expression as
a probability density across cells, offering a richer representation than
summary statistics such as means or variances, and allowing for more nuanced
group comparisons. To this end, we propose a model-agnostic framework for
density estimation and inference based on specially designed exponential
families~(SEF), which accommodates diverse underlying models without requiring
prior specifications. The proposed method enables estimation and visualization
for both individual-specific and group-level gene expression densities, as well
as conducting formal hypothesis testing for expression density difference
across groups of interest. It relies on relaxed assumptions with established
asymptotic properties and a consistent covariance estimator for valid
inference. Through simulation under various scenarios, the SEF-based approach
demonstrates good error control and improved statistical power over competing
methods,including pseudo-bulk tests and moment estimators. Application to a
population-scale scRNA-seq dataset from patients with systemic lupus
erythematosus identified genes and gene sets that are missed from pseudo-bulk
based tests.

</details>


### [2] [TabMGP: Martingale Posterior with TabPFN](https://arxiv.org/abs/2510.25154)
*Kenyon Ng,Edwin Fong,David T. Frazier,Jeremias Knoblauch,Susan Wei*

Main category: stat.ME

TL;DR: TabMGP是一种基于TabPFN变换器基础模型的马尔可夫后验方法，用于表格数据的贝叶斯推断，提供接近标称覆盖率的可信集，性能优于现有MGP构造和标准贝叶斯方法。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断虽然提供原则性的不确定性量化，但在先验设定、似然误设和计算负担方面存在挑战。马尔可夫后验(MGP)通过预测规则替代先验-似然设定，但现有MGP方法缺乏有说服力的实例。

Method: 利用基础变换器构建TabMGP，基于TabPFN变换器模型，其自回归生成与MGP的前向模拟过程相匹配，适用于表格数据的丰富预测建模。

Result: TabMGP生成的可信集具有接近标称的覆盖率，在性能上通常优于现有的MGP构造和标准贝叶斯方法。

Conclusion: 基于变换器基础模型的MGP方法为贝叶斯推断提供了有前景的替代方案，特别是在表格数据建模方面表现出色。

Abstract: Bayesian inference provides principled uncertainty quantification but is
often limited by challenges of prior elicitation, likelihood misspecification,
and computational burden. The martingale posterior (MGP, Fong et al., 2023)
offers an alternative, replacing prior-likelihood elicitation with a predictive
rule - namely, a sequence of one-step-ahead predictive distributions - for
forward data generation. The utility of MGPs depends on the choice of
predictive rule, yet the literature has offered few compelling examples.
Foundation transformers are well-suited here, as their autoregressive
generation mirrors this forward simulation and their general-purpose design
enables rich predictive modeling. We introduce TabMGP, an MGP built on TabPFN,
a transformer foundation model that is currently state-of-the-art for tabular
data. TabMGP produces credible sets with near-nominal coverage and often
outperforms both existing MGP constructions and standard Bayes.

</details>


### [3] [Bayesian Spatial Point Process Modeling for Cluster Randomized Trials](https://arxiv.org/abs/2510.24969)
*Jooyeon Lee,M. S.,Evan Kwiatkowski,Ph. D*

Main category: stat.ME

TL;DR: 本文提出了一个贝叶斯空间点过程框架，用于改进群组随机试验(CRT)的设计和分析，通过显式建模空间依赖性来解决传统非空间模型低估不确定性和导致误导性推断的问题。


<details>
  <summary>Details</summary>
Motivation: 群组随机试验在实际应用中面临精度有限和建模复杂的问题，而鲁棒贝叶斯方法和空间相关性在CRT设计和分析中的应用仍相对不成熟。

Method: 采用贝叶斯空间点过程框架，对地理群组内嵌套的个体进行建模，同时显式考虑空间依赖性。

Result: 传统非空间模型持续低估不确定性并导致误导性推断，而空间方法提高了估计稳定性、控制了I类错误并增强了统计功效。

Conclusion: 研究结果强调了在CRT中更广泛采用空间方法的价值和必要性。

Abstract: Cluster randomized trials (CRTs) offer a practical alternative for addressing
logistical challenges and ensuring feasibility in community health, education,
and prevention studies, even though randomized controlled trials are considered
the gold standard in evaluating therapeutic interventions. Despite their
utility, CRTs are often criticized for limited precision and complex modeling
requirements. Advances in robust Bayesian methods and the incorporation of
spatial correlation into CRT design and analysis remain relatively
underdeveloped. This paper introduces a Bayesian spatial point process
framework that models individuals nested within geographic clusters while
explicitly accounting for spatial dependence. We demonstrate that conventional
non-spatial models consistently underestimate uncertainty and lead to
misleading inferences, whereas our spatial approach improves estimation
stability, controls type I error, and enhances statistical power. Our results
underscore the value and need for wider adoption of spatial methods in CRT.

</details>


### [4] [Bayesian Adaptive Polynomial Chaos Expansions](https://arxiv.org/abs/2510.25036)
*Kellin N. Rumsey,Devin Francom,Graham C. Gibson,J. Derek Tucker,Gabriel Huerta*

Main category: stat.ME

TL;DR: 提出了一种新的完全贝叶斯自适应多项式混沌展开方法khaos，包含创新的提议分布和修改的g-先验，在R中实现，在代理建模、全局敏感性分析和序数回归任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 多项式混沌展开在不确定性量化中广泛应用，但在统计学文献中关注较少，完全贝叶斯公式尤其罕见，特别是在R中的实现

Method: 开发了完全贝叶斯自适应PCE方法khaos，包含数据驱动的交互选择提议分布和针对PCE结构优化的修改g-先验

Result: 通过模拟研究和实际UQ应用证明，贝叶斯自适应PCE在代理建模、全局敏感性分析和序数回归任务中具有竞争力

Conclusion: khaos为PCE提供了有效的贝叶斯自适应方法，填补了统计学文献中的空白，并在多个UQ任务中表现出色

Abstract: Polynomial chaos expansions (PCE) are widely used for uncertainty
quantification (UQ) tasks, particularly in the applied mathematics community.
However, PCE has received comparatively less attention in the statistics
literature, and fully Bayesian formulations remain rare, especially with
implementations in R. Motivated by the success of adaptive Bayesian machine
learning models such as BART, BASS, and BPPR, we develop a new fully Bayesian
adaptive PCE method with an efficient and accessible R implementation: khaos.
Our approach includes a novel proposal distribution that enables data-driven
interaction selection, and supports a modified g-prior tailored to PCE
structure. Through simulation studies and real-world UQ applications, we
demonstrate that Bayesian adaptive PCE provides competitive performance for
surrogate modeling, global sensitivity analysis, and ordinal regression tasks.

</details>


### [5] [Robust variable selection for spatial point processes observed with noise](https://arxiv.org/abs/2510.25550)
*Dominik Sturm,Ivo F. Sbalzarini*

Main category: stat.ME

TL;DR: 提出了一种结合稀疏性估计和噪声鲁棒模型选择的空间点过程强度函数变量选择方法，能够可靠地在各种噪声场景下恢复真实协变量


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率空间数据通过遥感和自动图像分析变得日益可用，识别影响事件定位的空间协变量对于理解潜在机制至关重要。但自动化获取技术的结果往往存在噪声，导致虚假位移和遗漏事件。

Method: 使用基于点过程子采样的稳定性选择，并结合非凸最佳子集惩罚来增强模型选择性能，研究噪声对稀疏点过程估计的影响，包括泊松过程和托马斯过程。

Result: 在广泛的模拟中，该方法在各种噪声场景下可靠地恢复了真实协变量，提高了选择准确性和稳定性。在林业数据集上的应用显示了该方法的实用性。

Conclusion: 该方法为噪声环境下空间点过程模型中的稳健变量选择提供了一个系统框架，无需额外的过程知识。

Abstract: We propose a method for variable selection in the intensity function of
spatial point processes that combines sparsity-promoting estimation with
noise-robust model selection. As high-resolution spatial data becomes
increasingly available through remote sensing and automated image analysis,
identifying spatial covariates that influence the localization of events is
crucial to understand the underlying mechanism. However, results from automated
acquisition techniques are often noisy, for example due to measurement
uncertainties or detection errors, which leads to spurious displacements and
missed events. We study the impact of such noise on sparse point-process
estimation across different models, including Poisson and Thomas processes. To
improve noise robustness, we propose to use stability selection based on
point-process subsampling and to incorporate a non-convex best-subset penalty
to enhance model-selection performance. In extensive simulations, we
demonstrate that such an approach reliably recovers true covariates under
diverse noise scenarios and improves both selection accuracy and stability. We
then apply the proposed method to a forestry data set, analyzing the
distribution of trees in relation to elevation and soil nutrients in a tropical
rain forest. This shows the practical utility of the method, which provides a
systematic framework for robust variable selection in spatial point-process
models under noise, without requiring additional knowledge of the process.

</details>


### [6] [Designing a quasi-experiment to study the clinical impact of adaptive risk prediction models](https://arxiv.org/abs/2510.25052)
*Valerie Odeh-Couvertier,Gabriel Zayas-Caban,Brian Patterson,Amy Cochran*

Main category: stat.ME

TL;DR: 提出了一种使用回归断点设计来评估风险预测模型与特定干预措施配对的方法，允许模型和阈值适应变化，解决了传统准实验设计的局限性。


<details>
  <summary>Details</summary>
Motivation: 临床风险预测模型与干预措施的评估通常需要随机对照试验，但实施困难且成本高昂。现有的准实验设计无法适应模型和阈值的调整，难以服务新人群和应对新趋势。

Method: 引入回归断点设计，当预测风险超过定义阈值时分配治疗，明确适应模型和阈值的调整。通过表征干扰并提供充分识别条件，以有效方式估计局部平均处理效应。

Result: 在模拟心血管风险计算器指导初级保健干预的情境中，评估了所提出估计器的性能，证明了方法的有效性。

Conclusion: 该方法为现代医疗系统中风险预测模型与干预措施的配对评估提供了可行的替代方案，能够适应模型和阈值的调整，具有实际应用价值。

Abstract: Clinical risk prediction is a valuable tool for guiding healthcare
interventions toward those most likely to benefit. Yet, evaluating the pairing
of a risk prediction model with an intervention using randomized controlled
trials (RCTs) presents substantial challenges to today's healthcare systems.
This makes quasi-experimental designs, which can offer nearly the same level of
evidence as an RCT, an attractive alternative. However, existing
quasi-experimental designs do not allow models and thresholds to adapt. As a
result, they struggle to serve new populations, meet emerging trends, and
address practical issues. To address this gap, we introduce regression
discontinuity designs for evaluating risk prediction models paired with
specific interventions in a modern healthcare system. In our designs, treatment
is assigned when predicted risk crosses a defined threshold, with the design
explicitly accommodating adaptations in both the model and threshold. We
account for the interference that arises from these adaptations to estimate the
local average treatment effect in a valid and efficient way. To that end, we
characterize interference and provide sufficient conditions for identification.
Estimators are introduced, and their performance is evaluated in a simulation
that emulates how cardiovascular risk calculators could guide interventions in
primary care settings.

</details>


### [7] [Existence and optimisation of the partial correlation graphical lasso](https://arxiv.org/abs/2510.25712)
*Jack Storror Carter,Cesare Molinari*

Main category: stat.ME

TL;DR: PCGLASSO是一种改进的图LASSO方法，用于高斯图模型的稀疏精度矩阵估计。本文解决了PCGLASSO的计算挑战，证明了在小样本情况下的存在性，并提出了新的交替算法实现。


<details>
  <summary>Details</summary>
Motivation: PCGLASSO虽然改进了图形LASSO方法，但由于其优化问题的非凸性面临计算挑战，且缺乏公开可用的实现。

Method: 证明了PCGLASSO估计在样本量小于维度时的存在性，提出并实现了一种新的交替算法，开发了R包PCGLASSO。

Result: PCGLASSO可以用于任何高斯数据，包括小样本情况；新算法在中等维度下具有竞争力的计算时间。

Conclusion: 本文为PCGLASSO提供了重要的计算突破，使其成为更实用的高斯图模型估计工具。

Abstract: The partial correlation graphical LASSO (PCGLASSO) is a penalised likelihood
method for Gaussian graphical models which provides scale invariant sparse
estimation of the precision matrix and improves upon the popular graphical
LASSO method. However, the PCGLASSO suffers from computational challenges due
to the non-convexity of its associated optimisation problem. This paper
provides some important breakthroughs in the computation of the PCGLASSO.
First, the existence of the PCGLASSO estimate is proven when the sample size is
smaller than the dimension - a case in which the maximum likelihood estimate
does not exist. This means that the PCGLASSO can be used with any Gaussian
data. Second, a new alternating algorithm for computing the PCGLASSO is
proposed and implemented in the R package PCGLASSO available at
https://github.com/JackStorrorCarter/PCGLASSO. This was the first publicly
available implementation of the PCGLASSO and provides competitive computation
time for moderate dimension size.

</details>


### [8] [Bayesian probabilistic projections of proportions with limited data: An application to subnational contraceptive method supply shares](https://arxiv.org/abs/2510.25153)
*Hannah Comiskey,Niamh Cahill,Leontine Alkema,David Fraizer,Worapree Maneesoonthorn*

Main category: stat.ME

TL;DR: 提出了一种利用DHS数据中的潜在属性来生成避孕方法供应份额的贝叶斯概率预测方法，以解决子国家级数据缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 随着各国采用分散化的计划生育服务模式，理解子国家层面的避孕方法供应来源对于创建公平、可持续和可及的医疗系统至关重要，但目前缺乏可靠的子国家级数据。

Method: 基于贝叶斯层次模型，使用惩罚样条跟踪公共和私营部门供应份额随时间的变化，利用数据的空间性质并纳入国家和子国家级供应份额观测值之间的相关性结构。

Result: 该建模框架在数据稀疏环境下对比例的估计优于可比和先前的方法，为子国家级估计领域做出了贡献。

Conclusion: 随着分散化继续重塑计划生育服务，为研究人员和政策制定者生成可靠的关键指标子国家级估计变得越来越重要。

Abstract: Engaging the private sector in contraceptive method supply is critical for
creating equitable, sustainable, and accessible healthcare systems. To achieve
this, it is essential to understand where women obtain their modern
contraceptives. While national-level estimates provide valuable insights into
overall trends in contraceptive supply, they often obscure variation within and
across subnational regions. Addressing localized needs has become increasingly
important as countries adopt decentralized models for family planning services.
Decentralization has also underscored the need for reliable subnational
estimates of key family planning indicators. The absence of regularly collected
subnational data has hindered effective monitoring and decision-making. To
bridge this gap, we propose a novel approach that leverages latent attributes
in Demographic and Health Survey (DHS) data to produce Bayesian probabilistic
projections of contraceptive method supply shares (the proportions of modern
contraceptive methods supplied by public and private sectors) with limited
data. Our modeling framework is built on Bayesian hierarchical models. Using
penalized splines to track public and private supply shares over time, we
leverage the spatial nature of the data and incorporate a correlation structure
between recent supply share observations at national and subnational levels.
This framework contributes to the domain of subnational estimation of
proportions in data-sparse settings, outperforming comparable and previous
approaches. As decentralization continues to reshape family planning services,
producing reliable subnational estimates of key indicators is increasingly
vital for researchers and policymakers.

</details>


### [9] [Statistical Process Monitoring based on Functional Data Analysis](https://arxiv.org/abs/2510.25742)
*Fabio Centofanti*

Main category: stat.ME

TL;DR: 这篇综述论文介绍了基于功能数据分析(FDA)的统计过程监控(SPM)方法，用于监控工业环境中的功能型数据配置文件，并提出了一个多变量功能数据监控的参考框架。


<details>
  <summary>Details</summary>
Motivation: 现代工业数据采集系统能够收集功能关系形式的数据（配置文件），传统监控技术存在局限性，需要应用FDA工具来改进统计过程监控。

Method: 提出了多变量功能数据监控的参考框架，并调查了几种FDA-based配置文件监控方法，包括集成额外功能协变量、鲁棒方法、实时监控技术和自适应策略。

Result: 开发了R包funcharts（可在CRAN获取），实现了所讨论的各种FDA-based配置文件监控方法。

Conclusion: 论文综述了现有的FDA-based配置文件监控方法，并提出了未来研究方向，为工业统计过程监控提供了功能数据分析的新视角。

Abstract: In modern industrial settings, advanced acquisition systems allow for the
collection of data in the form of profiles, that is, as functional
relationships linking responses to explanatory variables. In this context,
statistical process monitoring (SPM) aims to assess the stability of profiles
over time in order to detect unexpected behavior. This review focuses on SPM
methods that model profiles as functional data, i.e., smooth functions defined
over a continuous domain, and apply functional data analysis (FDA) tools to
address limitations of traditional monitoring techniques. A reference framework
for monitoring multivariate functional data is first presented. This review
then offers a focused survey of several recent FDA-based profile monitoring
methods that extend this framework to address common challenges encountered in
real-world applications. These include approaches that integrate additional
functional covariates to enhance detection power, a robust method designed to
accommodate outlying observations, a real-time monitoring technique for
partially observed profiles, and two adaptive strategies that target the
characteristics of the out-of-control distribution. These methods are all
implemented in the R package funcharts, available on CRAN. Finally, a review of
additional existing FDA-based profile monitoring methods is also presented,
along with suggestions for future research.

</details>


### [10] [Improving time series estimation and prediction via transfer learning](https://arxiv.org/abs/2510.25236)
*Yuchang Lin,Qianqian Zhu,Guodong Li*

Main category: stat.ME

TL;DR: 提出了一种基于表示学习的迁移学习框架，用于高维小样本时间序列的向量自回归模型估计，通过利用相关源数据集的信息来提高估计效率。


<details>
  <summary>Details</summary>
Motivation: 许多时间序列数据（如宏观经济变量）具有高维度但样本量有限的问题，仅使用目标数据集难以获得有效的估计和准确的预测。

Method: 提出了两阶段正则化估计程序，通过表示学习从具有丰富观测值的相关源数据集中提取信息，并采用交替更新算法进行估计。

Result: 该框架能够处理不同样本大小和异步时间点的时间序列，在模拟实验中表现出良好的有限样本性能，并在20个日本和9个其他国家宏观经济变量的实证分析中证明了其有效性。

Conclusion: 所提出的迁移学习框架为高维小样本时间序列分析提供了一种灵活有效的方法，能够显著提升估计效率和预测准确性。

Abstract: There are many time series in the literature with high dimension yet limited
sample sizes, such as macroeconomic variables, and it is almost impossible to
obtain efficient estimation and accurate prediction by using the corresponding
datasets themselves. This paper fills the gap by introducing a novel
representation-based transfer learning framework for vector autoregressive
models, and information from related source datasets with rich observations can
be leveraged to enhance estimation efficiency through representation learning.
A two-stage regularized estimation procedure is proposed with well established
non-asymptotic properties, and algorithms with alternating updates are
suggested to search for the estimates. Our transfer learning framework can
handle time series with varying sample sizes and asynchronous starting and/or
ending time points, thereby offering remarkable flexibility in integrating
information from diverse datasets. Simulation experiments are conducted to
evaluate the finite-sample performance of the proposed methodology, and its
usefulness is demonstrated by an empirical analysis on 20 macroeconomic
variables from Japan and another nine countries.

</details>


### [11] [Nonparametric bounds for vaccine effects in randomized trials](https://arxiv.org/abs/2510.25296)
*Rachel Axelrod,Uri Obolski,Daniel Nevo*

Main category: stat.ME

TL;DR: 本文提出了在疫苗随机试验中当盲法被破坏时，估计疫苗有效性的非参数因果界限方法，解决了传统方法需要强假设的问题。


<details>
  <summary>Details</summary>
Motivation: 疫苗随机试验通常采用盲法设计，但当盲法被破坏时，估计的疫苗有效性不仅反映免疫学效应，还包含行为效应。现有方法需要强假设（无未测量的感染风险和参与者信念的共同原因），而人格特质等因素可能违反这一假设。

Method: 采用两种方法构建非参数因果界限：基于线性规划的方法和基于单调性的方法。考虑了几种可能的疫苗试验因果结构，并展示了不同场景下非参数界限的差异。

Result: 使用完全合成数据和基于COVID-19疫苗试验的半合成数据示例说明了所提出界限的性能。

Conclusion: 提出的非参数因果界限方法能够在不依赖强假设的情况下，更准确地估计疫苗有效性，特别是在盲法被破坏的情况下。

Abstract: Vaccine randomized trials are typically designed to be blinded, ensuring that
the estimated vaccine efficacy (VE) reflects the immunological effect of the
vaccine. When blinding is broken, however, the estimated VE reflects not only
the immunological effect but also behavioral effects stemming from
participants' awareness of their treatment status. Recent work has proposed
alternative causal estimands to the standard VE to address this issue, but
their point identification results require a strong assumption: the absence of
unmeasured common causes of infection risk and participants' belief about
whether they received the vaccine. Personality traits, for example, may
plausibly violate this assumption. We relax this assumption and derive
nonparametric causal bounds for different types of VE. We construct these
bounds using two approaches: linear programming-based and monotonicity-based
methods. We further consider several possible causal structures for vaccine
trials and show how the nonparametric bounds differ across these scenarios.
Finally, we illustrate the performance of the proposed bounds using fully
synthetic data and a semi-synthetic data example based on a COVID-19 vaccine
trial.

</details>


### [12] [Tuning-Free Sampling via Optimization on the Space of Probability Measures](https://arxiv.org/abs/2510.25315)
*Louis Sharrock,Christopher Nemeth*

Main category: stat.ME

TL;DR: 提出了自适应、无需调参的步长调度方法，用于基于梯度的采样算法，包括ULA、SGLD、MFLD、SVGD和VGD等算法的免调参变体。


<details>
  <summary>Details</summary>
Motivation: 解决传统梯度采样算法需要手动调整步长参数的问题，提供自动适应且无需调参的算法变体。

Method: 通过Wasserstein梯度流的时间离散化获得自适应步长调度，适用于一类概率测度空间上的随机优化问题。

Result: 在温和假设下（如测地凸性和局部有界随机梯度），获得了与最优调参版本相当的收敛率（最多相差对数因子）。

Conclusion: 该方法在各种任务中实现了与现有算法最优性能相当的成果，且无需调整步长参数。

Abstract: We introduce adaptive, tuning-free step size schedules for gradient-based
sampling algorithms obtained as time-discretizations of Wasserstein gradient
flows. The result is a suite of tuning-free sampling algorithms, including
tuning-free variants of the unadjusted Langevin algorithm (ULA), stochastic
gradient Langevin dynamics (SGLD), mean-field Langevin dynamics (MFLD), Stein
variational gradient descent (SVGD), and variational gradient descent (VGD).
More widely, our approach yields tuning-free algorithms for solving a broad
class of stochastic optimization problems over the space of probability
measures. Under mild assumptions (e.g., geodesic convexity and locally bounded
stochastic gradients), we establish strong theoretical guarantees for our
approach. In particular, we recover the convergence rate of optimally tuned
versions of these algorithms up to logarithmic factors, in both nonsmooth and
smooth settings. We then benchmark the performance of our methods against
comparable existing approaches. Across a variety of tasks, our algorithms
achieve similar performance to the optimal performance of existing algorithms,
with no need to tune a step size parameter.

</details>


### [13] [Distributional Evaluation of Generative Models via Relative Density Ratio](https://arxiv.org/abs/2510.25507)
*Yuliang Xu,Yun Wei,Li Ma*

Main category: stat.ME

TL;DR: 提出基于相对密度比(RDR)的生成模型功能评估指标，用于表征真实样本与生成样本之间的分布差异，具有理论保证和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够有效评估生成模型性能的指标，不仅要比较模型的整体表现，还要能揭示分布差异的具体特征和原因。

Method: 通过φ-散度的变分形式进行凸优化，实现RDR的函数估计，提供基于M-估计器理论和神经网络估计器的收敛速率保证。

Result: 在MNIST、CelebA64和美国肠道项目微生物数据上的实验表明，RDR能有效比较生成模型性能，揭示拟合优度的本质特征。

Conclusion: RDR作为一种功能评估指标，不仅能比较生成模型的整体性能，还能揭示样本空间中生成器集中的区域和驱动分布差异的特征。

Abstract: We propose a functional evaluation metric for generative models based on the
relative density ratio (RDR) designed to characterize distributional
differences between real and generated samples. We show that the RDR as a
functional summary of the goodness-of-fit for the generative model, possesses
several desirable theoretical properties. It preserves $\phi$-divergence
between two distributions, enables sample-level evaluation that facilitates
downstream investigations of feature-specific distributional differences, and
has a bounded range that affords clear interpretability and numerical
stability. Functional estimation of the RDR is achieved efficiently through
convex optimization on the variational form of $\phi$-divergence. We provide
theoretical convergence rate guarantees for general estimators based on
M-estimator theory, as well as the convergence rates of neural network-based
estimators when the true ratio is in the anisotropic Besov space. We
demonstrate the power of the proposed RDR-based evaluation through numerical
experiments on MNIST, CelebA64, and the American Gut project microbiome data.
We show that the estimated RDR not only allows for an effective comparison of
the overall performance of competing generative models, but it can also offer a
convenient means of revealing the nature of the underlying goodness-of-fit.
This enables one to assess support overlap, coverage, and fidelity while
pinpointing regions of the sample space where generators concentrate and
revealing the features that drive the most salient distributional differences.

</details>


### [14] [Asymmetric Huber Periodogram](https://arxiv.org/abs/2510.25316)
*Tianbo Chen*

Main category: stat.ME

TL;DR: 提出了一种新的谱M估计器——非对称Huber周期图(AHP)，用于时间序列的周期性检测，相比现有方法具有更好的统计效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有周期图方法如普通周期图(PG)、分位数周期图(QP)和Huber周期图(HP)各有局限性，需要一种更全面、更高效的周期性检测方法。

Method: 通过三角非对称Huber回归构建AHP，使用特殊设计的检查函数替代普通周期图中的平方L2范数，通过调节非对称参数来全面分析数据。

Result: 理论证明了AHP的性质及其与非对称Huber谱(AHS)的关系，仿真和三个真实数据案例验证了AHP在周期性检测和抗异常值方面的优越性能。

Conclusion: AHP在统计效率上优于QP，在全面性上优于HP，是一种有效的周期性检测工具，特别适用于存在异常值的时间序列分析。

Abstract: This paper introduces a novel spectral M-estimator, called the asymmetric
Huber periodogram (AHP), for periodicity detection in time series. The AHP is
constructed from trigonometric asymmetric Huber regression, where a specially
designed check function is used to substitute the squared L2 norm that defines
the ordinary periodogram (PG). The AHP is statistically more efficient than the
quantile periodogram (QP), while offering a more comprehensive picture than the
Huber periodogram (HP) by examining the data across the entire range of the
asymmetric parameter. We prove the theoretical properties of the AHP and
investigate the relationship between the AHP and the so-called asymmetric Huber
spectrum (AHS). Finally, simulations and three real-world data examples
demonstrate that the AHP's capability in detecting periodicity and its
robustness against outliers.

</details>


### [15] [Latent variable estimation with composite Hilbert space Gaussian processes](https://arxiv.org/abs/2510.25371)
*Soham Mukherjee,Javier Enrique Aguilar,Marcello Zago,Manfred Claassen,Paul-Christian Bürkner*

Main category: stat.ME

TL;DR: 本文开发了一种基于复合高斯过程的可扩展隐变量估计模型，特别关注导数高斯过程，通过联合建模多个数据源来提高隐变量推断的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统精确高斯过程在大数据集上扩展性差，需要开发可扩展的隐变量估计方法，特别是在单细胞生物学等需要处理数千样本的领域。

Method: 将希尔伯特空间近似方法扩展到复合高斯过程，通过谱分解获得复合协方差函数的降秩表示，特别推导和分析了导数协方差函数的谱分解。

Result: 该方法可以轻松扩展到涉及数千样本的数据场景，在隐变量估计准确性、不确定性校准和推理速度方面都表现良好。

Conclusion: 通过单细胞生物学案例研究，证明了该方法在给定基因表达水平下估计潜在细胞排序的潜力，有助于更好地理解基础生物学过程。

Abstract: We develop a scalable class of models for latent variable estimation using
composite Gaussian processes, with a focus on derivative Gaussian processes. We
jointly model multiple data sources as outputs to improve the accuracy of
latent variable inference under a single probabilistic framework. Similarly
specified exact Gaussian processes scale poorly with large datasets. To
overcome this, we extend the recently developed Hilbert space approximation
methods for Gaussian processes to obtain a reduced-rank representation of the
composite covariance function through its spectral decomposition. Specifically,
we derive and analyze the spectral decomposition of derivative covariance
functions and further study their properties theoretically. Using these
spectral decompositions, our methods easily scale up to data scenarios
involving thousands of samples. We validate our methods in terms of latent
variable estimation accuracy, uncertainty calibration, and inference speed
across diverse simulation scenarios. Finally, using a real world case study
from single-cell biology, we demonstrate the potential of our models in
estimating latent cellular ordering given gene expression levels, thus
enhancing our understanding of the underlying biological process.

</details>


### [16] [Automatic selection of hyper-parameters via the use of softened profile likelihood](https://arxiv.org/abs/2510.25632)
*Gengyang Chen,Mu Zhu*

Main category: stat.ME

TL;DR: 提出一种扩展的启发式方法，通过最大化软化轮廓似然来自动选择多个超参数，应用于弹性网络、支持向量机和神经网络等模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能自动选择单个超参数，需要扩展到能够同时选择多个超参数，提高模型选择的自动化程度。

Method: 扩展了基于轮廓似然的启发式方法，提出了"软化"轮廓似然的概念，并提供了两种不同的参数化解决方案。

Result: 方法在弹性网络、支持向量机和神经网络上得到验证，能够有效自动选择多个超参数。

Conclusion: 该方法不仅适用于超参数选择，还可扩展到其他数据分析任务，为自动化模型选择提供了有效工具。

Abstract: We extend a heuristic method for automatic dimensionality selection, which
maximizes a profile likelihood to identify "elbows" in scree plots. Our
extension enables researchers to make automatic choices of multiple
hyper-parameters simultaneously. To facilitate our extension to
multi-dimensions, we propose a "softened" profile likelihood. We present two
distinct parameterizations of our solution and demonstrate our approach on
elastic nets, support vector machines, and neural networks. We also briefly
discuss applications of our method to other data-analytic tasks than
hyper-parameter selection.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [17] [Certainty in Uncertainty: Reasoning over Uncertain Knowledge Graphs with Statistical Guarantees](https://arxiv.org/abs/2510.24754)
*Yuqicheng Zhu,Jingcheng Wu,Yizhen Wang,Hongkuan Zhou,Jiaoyan Chen,Evgeny Kharlamov,Steffen Staab*

Main category: stat.ML

TL;DR: 提出UnKGCP框架，为不确定性知识图谱嵌入方法生成具有理论保证的预测区间，量化预测不确定性


<details>
  <summary>Details</summary>
Motivation: 现有不确定性知识图谱嵌入方法仅提供点估计，缺乏预测不确定性的量化，限制了在高风险应用中的可靠性

Method: 基于共形预测框架，提出专门针对UnKGE方法的非共形度量，并设计高效的区间构建程序

Result: 实验验证了区间的理论保证，证明区间紧凑且能有效捕捉预测不确定性

Conclusion: UnKGCP框架为UnKGE方法提供了可靠的预测区间，增强了模型在关键应用中的可信度

Abstract: Uncertain knowledge graph embedding (UnKGE) methods learn vector
representations that capture both structural and uncertainty information to
predict scores of unseen triples. However, existing methods produce only point
estimates, without quantifying predictive uncertainty-limiting their
reliability in high-stakes applications where understanding confidence in
predictions is crucial. To address this limitation, we propose \textsc{UnKGCP},
a framework that generates prediction intervals guaranteed to contain the true
score with a user-specified level of confidence. The length of the intervals
reflects the model's predictive uncertainty. \textsc{UnKGCP} builds on the
conformal prediction framework but introduces a novel nonconformity measure
tailored to UnKGE methods and an efficient procedure for interval construction.
We provide theoretical guarantees for the intervals and empirically verify
these guarantees. Extensive experiments on standard benchmarks across diverse
UnKGE methods further demonstrate that the intervals are sharp and effectively
capture predictive uncertainty.

</details>


### [18] [Tree Ensemble Explainability through the Hoeffding Functional Decomposition and TreeHFD Algorithm](https://arxiv.org/abs/2510.24815)
*Clément Bénard*

Main category: stat.ML

TL;DR: 提出了TreeHFD算法，用于从数据样本中估计树集合的Hoeffding分解，解决了树集合模型可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 树集合模型在表格数据预测中表现出色，但其黑盒特性限制了在关键决策应用中的使用。Hoeffding分解虽然能分解黑盒模型，但在输入变量相关时的实际估计仍是一个开放问题。

Method: 开发了TreeHFD算法，通过分层正交约束来估计树集合的Hoeffding分解，确保分解的唯一性和稀疏性。

Result: TreeHFD算法具有收敛性、正交性、稀疏性和因果变量选择等特性，在模拟和真实数据实验中表现出高性能。

Conclusion: TreeHFD为树集合模型提供了有效的可解释性方法，并且揭示了广泛使用的TreeSHAP方法与Hoeffding分解之间的紧密联系。

Abstract: Tree ensembles have demonstrated state-of-the-art predictive performance
across a wide range of problems involving tabular data. Nevertheless, the
black-box nature of tree ensembles is a strong limitation, especially for
applications with critical decisions at stake. The Hoeffding or ANOVA
functional decomposition is a powerful explainability method, as it breaks down
black-box models into a unique sum of lower-dimensional functions, provided
that input variables are independent. In standard learning settings, input
variables are often dependent, and the Hoeffding decomposition is generalized
through hierarchical orthogonality constraints. Such generalization leads to
unique and sparse decompositions with well-defined main effects and
interactions. However, the practical estimation of this decomposition from a
data sample is still an open problem. Therefore, we introduce the TreeHFD
algorithm to estimate the Hoeffding decomposition of a tree ensemble from a
data sample. We show the convergence of TreeHFD, along with the main properties
of orthogonality, sparsity, and causal variable selection. The high performance
of TreeHFD is demonstrated through experiments on both simulated and real data,
using our treehfd Python package (https://github.com/ThalesGroup/treehfd).
Besides, we empirically show that the widely used TreeSHAP method, based on
Shapley values, is strongly connected to the Hoeffding decomposition.

</details>


### [19] [Generative Bayesian Optimization: Generative Models as Acquisition Functions](https://arxiv.org/abs/2510.25240)
*Rafael Oliveira,Daniel M. Steinberg,Edwin V. Bonilla*

Main category: stat.ML

TL;DR: 提出一种将生成模型转化为批量贝叶斯优化候选解采样器的通用策略，利用直接偏好优化思想直接训练生成模型，无需构建代理模型。


<details>
  <summary>Details</summary>
Motivation: 解决批量贝叶斯优化中的大规模批处理、非连续设计空间、高维和组合设计等挑战，避免传统方法中构建代理模型的复杂性。

Method: 基于直接偏好优化思想，使用观测值直接计算的噪声效用值训练生成模型，形成与期望效用成正比的提案分布。

Result: 理论证明生成模型在BO过程中近似遵循一系列分布，在特定条件下渐近收敛于全局最优解，并在高维大规模批处理优化问题上验证了效果。

Conclusion: 该方法为生成模型在贝叶斯优化中的应用提供了通用框架，能够处理复杂优化问题并实现理论保证的收敛性。

Abstract: We present a general strategy for turning generative models into candidate
solution samplers for batch Bayesian optimization (BO). The use of generative
models for BO enables large batch scaling as generative sampling, optimization
of non-continuous design spaces, and high-dimensional and combinatorial design.
Inspired by the success of direct preference optimization (DPO), we show that
one can train a generative model with noisy, simple utility values directly
computed from observations to then form proposal distributions whose densities
are proportional to the expected utility, i.e., BO's acquisition function
values. Furthermore, this approach is generalizable beyond preference-based
feedback to general types of reward signals and loss functions. This
perspective avoids the construction of surrogate (regression or classification)
models, common in previous methods that have used generative models for
black-box optimization. Theoretically, we show that the generative models
within the BO process approximately follow a sequence of distributions which
asymptotically concentrate at the global optima under certain conditions. We
also demonstrate this effect through experiments on challenging optimization
problems involving large batches in high dimensions.

</details>


### [20] [Convergence of off-policy TD(0) with linear function approximation for reversible Markov chains](https://arxiv.org/abs/2510.25514)
*Maik Overmars,Jasper Goseling,Richard Boucherie*

Main category: stat.ML

TL;DR: 本文研究了在可逆马尔可夫链中，使用线性函数近似的离策略TD(0)算法的收敛性，证明了在折扣因子满足特定边界条件下，该标准算法能够以概率1收敛到零投影贝尔曼误差。


<details>
  <summary>Details</summary>
Motivation: 现有的离策略学习与函数近似结合会导致算法发散，现有方法通过重要性采样等修改算法来确保收敛，但增加了复杂度。本文旨在分析标准算法在可逆马尔可夫链中的收敛性，这种条件在许多应用中可以通过领域知识满足。

Method: 将Tsitsiklis和Van Roy[1997]用于同策略情况的随机近似框架适应到离策略情况，分析标准离策略TD(0)算法在可逆马尔可夫链中的收敛性，并建立折扣因子的显式边界条件。

Result: 证明了在可逆马尔可夫链中，当折扣因子满足基于同策略和离策略过程差异的上界时，标准离策略TD(0)算法能够以概率1收敛，且达到零投影贝尔曼误差。

Conclusion: 在可逆马尔可夫链的温和条件下，标准离策略TD(0)算法能够收敛，无需修改算法，这为实际应用提供了更简单的解决方案。

Abstract: We study the convergence of off-policy TD(0) with linear function
approximation when used to approximate the expected discounted reward in a
Markov chain. It is well known that the combination of off-policy learning and
function approximation can lead to divergence of the algorithm. Existing
results for this setting modify the algorithm, for instance by reweighing the
updates using importance sampling. This establishes convergence at the expense
of additional complexity. In contrast, our approach is to analyse the standard
algorithm, but to restrict our attention to the class of reversible Markov
chains. We demonstrate convergence under this mild reversibility condition on
the structure of the chain, which in many applications can be assumed using
domain knowledge. In particular, we establish a convergence guarantee under an
upper bound on the discount factor in terms of the difference between the
on-policy and off-policy process. This improves upon known results in the
literature that state that convergence holds for a sufficiently small discount
factor by establishing an explicit bound. Convergence is with probability one
and achieves projected Bellman error equal to zero. To obtain these results, we
adapt the stochastic approximation framework that was used by Tsitsiklis and
Van Roy [1997 for the on-policy case, to the off-policy case. We illustrate our
results using different types of reversible Markov chains, such as
one-dimensional random walks and random walks on a weighted graph.

</details>


### [21] [Using latent representations to link disjoint longitudinal data for mixed-effects regression](https://arxiv.org/abs/2510.25531)
*Clemens Schächter,Maren Hackenberg,Michelle Pfaffenlehner,Félix B. Tambe-Ndonfack,Thorsten Schmidt,Astrid Pechmann,Janbernd Kirschner,Jan Hasenauser,Harald Binder*

Main category: stat.ML

TL;DR: 提出一种结合变分自编码器和混合效应回归的方法，用于分析罕见疾病治疗转换的影响，解决了测量工具变化导致的数据不连续问题。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病治疗选择有限，患者经常转换治疗，但小样本量和测量工具变化使传统纵向建模方法难以应用。

Method: 使用变分自编码器将不同测量工具的观测值映射到共享的潜在空间，然后应用混合效应回归模型分析时间动态和治疗转换效应。

Result: 该方法成功应用于脊髓性肌萎缩症患者，能够量化治疗转换对运动表现的影响，并在小样本情况下实现统计推断。

Conclusion: 在联合潜在表示中进行建模为解决小数据挑战提供了有效途径，能够处理测量工具变化带来的数据不连续问题。

Abstract: Many rare diseases offer limited established treatment options, leading
patients to switch therapies when new medications emerge. To analyze the impact
of such treatment switches within the low sample size limitations of rare
disease trials, it is important to use all available data sources. This,
however, is complicated when usage of measurement instruments change during the
observation period, for example when instruments are adapted to specific age
ranges. The resulting disjoint longitudinal data trajectories, complicate the
application of traditional modeling approaches like mixed-effects regression.
We tackle this by mapping observations of each instrument to a aligned
low-dimensional temporal trajectory, enabling longitudinal modeling across
instruments. Specifically, we employ a set of variational autoencoder
architectures to embed item values into a shared latent space for each time
point. Temporal disease dynamics and treatment switch effects are then captured
through a mixed-effects regression model applied to latent representations. To
enable statistical inference, we present a novel statistical testing approach
that accounts for the joint parameter estimation of mixed-effects regression
and variational autoencoders. The methodology is applied to quantify the impact
of treatment switches for patients with spinal muscular atrophy. Here, our
approach aligns motor performance items from different measurement instruments
for mixed-effects regression and maps estimated effects back to the observed
item level to quantify the treatment switch effect. Our approach allows for
model selection as well as for assessing effects of treatment switching. The
results highlight the potential of modeling in joint latent representations for
addressing small data challenges.

</details>


### [22] [Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations](https://arxiv.org/abs/2510.25544)
*Hugo Lavenant,Giacomo Zanella*

Main category: stat.ML

TL;DR: 本文研究了掩码扩散模型的计算与精度权衡，提出了与数据维度无关的误差界，并优化了非恒定调度大小以获得更好的生成性能。


<details>
  <summary>Details</summary>
Motivation: 研究离散数据生成模型中计算成本与采样分布偏差之间的权衡，特别是掩码扩散模型相对于自回归模型的优势与局限。

Method: 使用采样算法直接定义方法，而非传统的反向扩散过程推导，提出了基于信息分布的最优调度大小优化方法。

Result: 建立了仅依赖于每轮生成令牌平均数的相对熵误差界，该界与数据维度无关，验证了MDMs的经验成功。

Conclusion: 通过优化调度大小可以显著提升生成性能，提出的方法具有简单透明的证明过程，为离散数据生成模型提供了理论指导。

Abstract: Recently proposed generative models for discrete data, such as Masked
Diffusion Models (MDMs), exploit conditional independence approximations to
reduce the computational cost of popular Auto-Regressive Models (ARMs), at the
price of some bias in the sampling distribution. We study the resulting
computation-vs-accuracy trade-off, providing general error bounds (in relative
entropy) that depend only on the average number of tokens generated per
iteration and are independent of the data dimensionality (i.e. sequence
length), thus supporting the empirical success of MDMs. We then investigate the
gain obtained by using non-constant schedule sizes (i.e. varying the number of
unmasked tokens during the generation process) and identify the optimal
schedule as a function of a so-called information profile of the data
distribution, thus allowing for a principled optimization of schedule sizes. We
define methods directly as sampling algorithms and do not use classical
derivations as time-reversed diffusion processes, leading us to simple and
transparent proofs.

</details>


### [23] [Monitoring the calibration of probability forecasts with an application to concept drift detection involving image classification](https://arxiv.org/abs/2510.25573)
*Christopher T. Franck,Anne R. Driscoll,Zoe Szajnfarber,William H. Woodall*

Main category: stat.ML

TL;DR: 提出了一种基于累积和动态控制限的方法，用于持续监测机器学习模型的校准度变化，特别适用于图像分类模型在部署过程中的校准度监控。


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习模型在图像分类中取得了显著精度，但如何评估和维持预测校准度是一个重要问题。现有方法主要关注校准评估和重新校准，缺乏对模型随时间推移可能出现的校准度下降的持续监测方法。

Method: 使用累积和控制图结合动态控制限的方法，能够在传统过程监控和概念漂移应用中检测校准度问题。该方法仅基于概率预测和事件结果，无需访问模型内部结构。

Result: 该方法能够早期检测到影响图像分类性能的操作环境变化，适用于任何需要随时间监控概率预测校准度的情况。

Conclusion: 提出的监控方法为机器学习模型在部署过程中的校准度持续监测提供了有效工具，有助于及时发现和应对模型性能退化问题。

Abstract: Machine learning approaches for image classification have led to impressive
advances in that field. For example, convolutional neural networks are able to
achieve remarkable image classification accuracy across a wide range of
applications in industry, defense, and other areas. While these machine
learning models boast impressive accuracy, a related concern is how to assess
and maintain calibration in the predictions these models make. A classification
model is said to be well calibrated if its predicted probabilities correspond
with the rates events actually occur. While there are many available methods to
assess machine learning calibration and recalibrate faulty predictions, less
effort has been spent on developing approaches that continually monitor
predictive models for potential loss of calibration as time passes. We propose
a cumulative sum-based approach with dynamic limits that enable detection of
miscalibration in both traditional process monitoring and concept drift
applications. This enables early detection of operational context changes that
impact image classification performance in the field. The proposed chart can be
used broadly in any situation where the user needs to monitor probability
predictions over time for potential lapses in calibration. Importantly, our
method operates on probability predictions and event outcomes and does not
require under-the-hood access to the machine learning model.

</details>


### [24] [How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs](https://arxiv.org/abs/2510.25753)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 本文研究了预训练Transformer中的上下文学习能力，分析了非线性MLP头在异构数据源下的表现，证明了非线性MLP能显著提升上下文学习性能，特别是在非线性任务上。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究通常基于简化架构和数据模型，与实际情况存在差距。本文旨在研究真实场景下预训练Transformer的上下文学习能力，特别是在非线性任务和异构数据源环境中的表现。

Method: 分析包含两层MLP的模型，第一层通过单步梯度训练，第二层完全优化。在高维渐近条件下，证明此类模型在上下文学习误差上等价于结构化多项式预测器，利用高斯普适性和正交多项式理论。

Result: 非线性MLP显著提升了上下文学习性能，特别是在非线性任务上。识别出高质量数据源的关键特性（低噪声、结构化协方差），并证明只有当任务协方差具有足够结构时才会出现特征学习。

Conclusion: 本文推进了Transformer上下文学习的理论基础，为架构和数据在上下文学习中的作用提供了可操作的见解，并通过多语言情感分析实验验证了理论发现的实际应用价值。

Abstract: Pretrained Transformers demonstrate remarkable in-context learning (ICL)
capabilities, enabling them to adapt to new tasks from demonstrations without
parameter updates. However, theoretical studies often rely on simplified
architectures (e.g., omitting MLPs), data models (e.g., linear regression with
isotropic inputs), and single-source training, limiting their relevance to
realistic settings. In this work, we study ICL in pretrained Transformers with
nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with
heterogeneous input, task, and noise distributions. We analyze a model where
the MLP comprises two layers, with the first layer trained via a single
gradient step and the second layer fully optimized. Under high-dimensional
asymptotics, we prove that such models are equivalent in ICL error to
structured polynomial predictors, leveraging results from the theory of
Gaussian universality and orthogonal polynomials. This equivalence reveals that
nonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear
tasks, compared to linear baselines. It also enables a precise analysis of data
mixing effects: we identify key properties of high-quality data sources (low
noise, structured covariances) and show that feature learning emerges only when
the task covariance exhibits sufficient structure. These results are validated
empirically across various activation functions, model sizes, and data
distributions. Finally, we experiment with a real-world scenario involving
multilingual sentiment analysis where each language is treated as a different
source. Our experimental results for this case exemplify how our findings
extend to real-world cases. Overall, our work advances the theoretical
foundations of ICL in Transformers and provides actionable insight into the
role of architecture and data in ICL.

</details>


### [25] [E-Scores for (In)Correctness Assessment of Generative Model Outputs](https://arxiv.org/abs/2510.25770)
*Guneet S. Dhillon,Javier González,Teodora Pandeva,Alicia Curth*

Main category: stat.ML

TL;DR: 本文提出使用e值来评估生成模型（特别是大语言模型）输出的错误性，解决了传统基于p值方法容易受到p-hacking影响的问题。


<details>
  <summary>Details</summary>
Motivation: 当前评估生成模型正确性的机制有限，基于p值的置信预测方法容易受到p-hacking的影响，即事后选择容错水平会破坏统计保证。

Method: 利用e值来补充生成模型输出，提供e分数作为错误性度量，允许用户在观察e分数后自适应选择容错水平。

Result: 实验证明该方法在评估数学事实性和属性约束满足等不同类型的正确性方面有效，同时保持了与之前方法相同的统计保证。

Conclusion: e分数方法不仅提供了与传统方法相同的统计保证，还赋予用户在事后自适应选择容错水平的灵活性，通过上界控制尺寸扭曲来确保有效性。

Abstract: While generative models, especially large language models (LLMs), are
ubiquitous in today's world, principled mechanisms to assess their
(in)correctness are limited. Using the conformal prediction framework, previous
works construct sets of LLM responses where the probability of including an
incorrect response, or error, is capped at a desired user-defined tolerance
level. However, since these methods are based on p-values, they are susceptible
to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the
guarantees. We therefore leverage e-values to complement generative model
outputs with e-scores as a measure of incorrectness. In addition to achieving
the same statistical guarantees as before, e-scores provide users flexibility
in adaptively choosing tolerance levels after observing the e-scores
themselves, by upper bounding a post-hoc notion of error called size
distortion. We experimentally demonstrate their efficacy in assessing LLM
outputs for different correctness types: mathematical factuality and property
constraints satisfaction.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [26] [Bayesian Neural Networks vs. Mixture Density Networks: Theoretical and Empirical Insights for Uncertainty-Aware Nonlinear Modeling](https://arxiv.org/abs/2510.25001)
*Riddhi Pratim Ghosh,Ian Barnett*

Main category: stat.CO

TL;DR: 比较贝叶斯神经网络(BNNs)和混合密度网络(MDNs)在不确定性感知非线性回归中的表现，发现MDNs在捕捉多模态响应和自适应不确定性方面更有效，而BNNs在有限数据下提供更可解释的认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 研究两种主要概率神经建模范式在不确定性感知非线性回归中的表现差异，为非线性系统中的不确定性建模提供指导。

Method: 建立统一的理论和实证框架，在理论方面推导收敛率和误差边界，在实证方面使用合成非线性数据集和放射学基准数据集进行评估。

Result: MDNs由于基于似然的性质实现更快的KL散度收敛，能更有效地捕捉多模态响应和自适应不确定性；BNNs存在变分推断引起的额外近似偏差，但在有限数据下提供更可解释的认知不确定性。

Conclusion: 两种方法具有互补优势：基于后验的BNNs和基于似然的MDNs在不确定性建模中各有特长，为非线性系统中的不确定性感知建模提供了明确指导。

Abstract: This paper investigates two prominent probabilistic neural modeling
paradigms: Bayesian Neural Networks (BNNs) and Mixture Density Networks (MDNs)
for uncertainty-aware nonlinear regression. While BNNs incorporate epistemic
uncertainty by placing prior distributions over network parameters, MDNs
directly model the conditional output distribution, thereby capturing
multimodal and heteroscedastic data-generating mechanisms. We present a unified
theoretical and empirical framework comparing these approaches. On the
theoretical side, we derive convergence rates and error bounds under H\"older
smoothness conditions, showing that MDNs achieve faster Kullback-Leibler (KL)
divergence convergence due to their likelihood-based nature, whereas BNNs
exhibit additional approximation bias induced by variational inference.
Empirically, we evaluate both architectures on synthetic nonlinear datasets and
a radiographic benchmark (RSNA Pediatric Bone Age Challenge). Quantitative and
qualitative results demonstrate that MDNs more effectively capture multimodal
responses and adaptive uncertainty, whereas BNNs provide more interpretable
epistemic uncertainty under limited data. Our findings clarify the
complementary strengths of posterior-based and likelihood-based probabilistic
learning, offering guidance for uncertainty-aware modeling in nonlinear
systems.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [27] [How can methods for classifying and clustering trajectories be used for prevention trials? An example in Alzheimer's disease area](https://arxiv.org/abs/2510.24751)
*Céline Bougel,Sébastien Déjean,Caroline Giulioli,Philippe Saint-Pierre,Nicolas Savy,Sandrine Andrieu*

Main category: stat.AP

TL;DR: 该研究探索了在阿尔茨海默病预防试验中应用数据驱动方法进行轨迹分类，以识别可能受益于干预的亚组。


<details>
  <summary>Details</summary>
Motivation: 传统临床试验使用基于模型的参数假设检验方法，当研究人群未观察到效应时，可能无法检测到亚组中的效应。研究者希望通过数据驱动方法识别同质亚组。

Method: 使用无监督方法：纵向数据k-means聚类、层次聚类分析、图形符号学，以及基于响应者状态的监督分析。在MAPT试验的复合Z评分数据上进行应用。

Result: 纵向k-means聚类识别出三个组，其中一个组显示认知下降。其他方法需要选择轨迹变化指标。层次聚类使用Ward聚合，图形符号学使用欧几里得距离和变化率。

Conclusion: 数据驱动方法能够识别认知下降的轨迹模式，突显了人群异质性可能掩盖治疗效果的问题。无监督方法为识别可能受益的亚组提供了替代途径。

Abstract: Background: Clinical trials are designed to prove the efficacy of an
intervention by means of model-based approaches involving parametric hypothesis
testing. Issues arise when no effect is observed in the study population.
Indeed, an effect may be present in a subgroup and the statistical test cannot
detect it. To investigate this possibility, we proposed to change the paradigm
to a data-driven approach. We selected exploratory methods to provide another
perspective on the data and to identify particular homogeneous subgroups of
subjects within which an effect might be detected. In the setting of prevention
trials, the endpoint is a trajectory of repeated measures. In the settings of
prevention trials, the endpoint is a trajectory of repeated measures, which
requires the use of methods that can take data autocorrelation into account.
The primary aim of this work was to explore the applicability of different
methods for clustering and classifying trajectories. Methods: The Multidomain
Alzheimer Preventive Trial (MAPT) was a three-year randomized controlled trial
with four parallel arms (NCT00672685). The primary outcome was a composite
Z-score combining four cognitive tests. The data were analyzed by quadratic
mixed effects model. This study was inconclusive. Exploratory analysis is
therefore relevant to investigate the use of data-driven methods for trajectory
classification. The methods used were unsupervised: k-means for longitudinal
data, Hierarchical Cluster Analysis (HCA), graphic semiology, and supervised
analysis with dichotomous classification according to responder status.
Results: Using k-means for longitudinal data, three groups were obtained and
one of these groups showed cognitive decline over the three years of follow-up.
This method could be applied directly to the primary outcome, the composite
Z-score with repeated observations over time. With the two others unsupervised
methods, we were unable to process longitudinal data directly. It was therefore
necessary to choose an indicator of change in trajectories and to consider the
rate of change between two measurements. For the HCA method, Ward's aggregation
was performed. The Euclidean distance and rates of change were applied for the
graphic semiology method. Lastly, as there were no objective criteria to define
responder status, we defined our responders based on clinical criteria.
Discussion: In the princeps study, the prevention trial was found to be
inconclusive, likely due to the heterogeneity of the population, which may have
masked a treatment effect later identified in a refined subgroup of high Beta
Amyloid subjects. So, we have adopted an alternative unsupervised approach to
subject stratification based on their trajectories. We could then identify
patterns of similar trajectories of cognitive decline and also highlight the
potential problem of a large heterogeneity of the profiles, maybe due to the
final endpoint considered.

</details>


### [28] [Forecasting Australian Electricity Generation by Fuel Mix](https://arxiv.org/abs/2510.25185)
*Han Lin Shang,Lin Han,Stefan Trück*

Main category: stat.AP

TL;DR: 提出了两种基于预测协调和成分数据分析的统计方法，用于预测短期电力供应中不同燃料组合的构成。在澳大利亚五个电力市场的实证研究中，自下而上的分层预测方法表现最佳，且在化石燃料占比较高的电力系统中预测精度更高。


<details>
  <summary>Details</summary>
Motivation: 随着可变可再生能源在电力系统中占比增加，电力需求和发电变得越来越不可预测。预测燃料组合的电力供应对于市场运营、确保电网稳定、优化成本、整合可再生能源以及支持可持续能源规划至关重要。

Method: 引入了两种统计方法：基于预测协调和成分数据分析的方法，用于预测短期电力供应中不同燃料组合的构成。

Result: 在澳大利亚五个电力市场的实证研究中，自下而上的分层预测方法（bottom-up hierarchical forecasting）始终优于其他方法。此外，在化石燃料占比较高的电力系统中，燃料组合预测的准确性更高。

Conclusion: 自下而上的分层预测方法是预测电力供应燃料组合的有效方法，特别是在传统化石燃料占主导地位的电力系统中表现更佳。

Abstract: Electricity demand and generation have become increasingly unpredictable with
the growing share of variable renewable energy sources in the power system.
Forecasting electricity supply by fuel mix is crucial for market operation,
ensuring grid stability, optimizing costs, integrating renewable energy
sources, and supporting sustainable energy planning. We introduce two
statistical methods, centering on forecast reconciliation and compositional
data analysis, to forecast short-term electricity supply by different types of
fuel mix. Using data for five electricity markets in Australia, we study the
forecast accuracy of these techniques. The bottom-up hierarchical forecasting
method consistently outperforms the other approaches. Moreover, fuel mix
forecasting is most accurate in power systems with a higher share of stable
fossil fuel generation.

</details>


### [29] [Inferring Mobility Reductions from COVID-19 Disease Spread along the Urban-Rural Gradient](https://arxiv.org/abs/2510.25424)
*Sydney Paltra,Jonas Dehning,Viola Priesemann,Kai Nagel*

Main category: stat.AP

TL;DR: 该研究使用贝叶斯分层模型分析德国400个地区在COVID-19疫情期间的移动电话数据，量化了不同环境、社会和人口因素对流动性减少的影响，发现城市地区对疾病传播反应最强烈，并识别了影响流动性行为的关键驱动因素。


<details>
  <summary>Details</summary>
Motivation: 了解哪些环境、社会和人口因素有助于流动性减少和疫情缓解，因为目前这方面的知识还很零散。

Method: 引入贝叶斯分层模型，使用匿名移动电话数据将流动性分解为疾病响应成分和疾病无关因素（温度、学校假期、公共假日），量化各因素的影响。

Result: 发现城市-农村梯度上对疾病传播的反应存在显著差异，大城市流动性减少最强烈；就业部门解释了第一波疫情期间反应强度的差异，而政治变量在第二波疫情期间变得显著；但流动性减少仅部分转化为较低的峰值发病率。

Conclusion: 研究结果识别了流动性减少的关键驱动因素，并证明流动性行为可以作为人群响应的操作性代理指标，但其他隐藏因素也影响疫情传播。

Abstract: The COVID-19 pandemic reshaped human mobility through policy interventions
and voluntary behavioral changes. Mobility adaptions helped mitigate pandemic
spread, however our knowledge which environmental, social, and demographic
factors helped mobility reduction and pandemic mitigation is patchy. We
introduce a Bayesian hierarchical model to quantify heterogeneity in mobility
responses across time and space in Germany's 400 districts using anonymized
mobile phone data. Decomposing mobility into a disease-responsive component and
disease-independent factors (temperature, school vacations, public holidays)
allows us to quantify the impact of each factor. We find significant
differences in reaction to disease spread along the urban-rural gradient, with
large cities reducing mobility most strongly. Employment sectors further help
explain variance in reaction strength during the first wave, while political
variables gain significance during the second wave. However, reduced mobility
only partially translates to lower peak incidence, indicating the influence of
other hidden factors. Our results identify key drivers of mobility reductions
and demonstrate that mobility behavior can serve as an operational proxy for
population response.

</details>


### [30] [General model for estimating range variances of terrestrial laser scanners based on (un-)scaled intensity values](https://arxiv.org/abs/2510.25587)
*Omar AbdelGafar,Selin Palaz,Yihui Yang,Christoph Holst*

Main category: stat.AP

TL;DR: 该研究提出了一种有效的方法来测量和估计地面激光扫描仪(TLS)的距离方差，使用原始和缩放强度值构建完整的随机模型，用于大地变形分析。


<details>
  <summary>Details</summary>
Motivation: 随着TLS在变形分析中的广泛应用，需要开发能够准确捕捉测量不确定性的综合随机模型，特别是构建完整的方差-协方差矩阵(VCM)，其中距离方差随强度测量值变化而变化。

Method: 采用二维扫描方法，对受控目标和任意物体进行扫描，使用提供原始强度值的TLS仪器(如Z+F Imager 5016A)和输出缩放强度的仪器(如Leica ScanStation P50)，并在水坝表面进行实地观测评估。

Result: 研究提出了建模高端TLS系统距离不确定性的完整工作流程，能够有效估计距离方差及其与强度测量的关系。

Conclusion: 该工作为TLS变形分析提供了全面的距离不确定性建模方法，有助于提高测量精度和可靠性。

Abstract: Recent advancements in technology have established terrestrial laser scanners
(TLS) as a powerful instrument in geodetic deformation analysis. As TLS becomes
increasingly integrated into this field, it is essential to develop a
comprehensive stochastic model that accurately captures the measurement
uncertainties. A key component of this model is the construction of a complete
and valid variance-covariance matrix (VCM) for TLS polar measurements, which
requires the estimation of variances for range, vertical, and horizontal
angles, as well as their correlations. While angular variances can be obtained
from manufacturer specifications, the range variance varies with different
intensity measurements. As a primary contribution, this study presents an
effective methodology for measuring and estimating TLS range variances using
both raw and scaled intensity values. A two-dimensional scanning approach is
applied to both controlled targets and arbitrary objects using TLS instruments
that provide raw intensity values (e.g., Z+F~Imager~5016A) and those that
output scaled intensities (e.g., Leica~ScanStation~P50). The methodology is
further evaluated using field observations on a water dam surface. Overall,
this work introduces a comprehensive workflow for modeling range uncertainties
in high-end TLS systems.

</details>


### [31] [COBASE: A new copula-based shuffling method for ensemble weather forecast postprocessing](https://arxiv.org/abs/2510.25610)
*Maurits Flos,Bastien François,Irene Schicker,Kirien Whan,Elisa Perrone*

Main category: stat.AP

TL;DR: COBASE是一种基于copula的后处理框架，通过秩重排机制结合参数化建模的灵活性和非参数技术的优势，为多变量集合预报提供校准的边缘分布和真实的依赖结构重建。


<details>
  <summary>Details</summary>
Motivation: 传统两步法后处理中，参数化方法（如GCA）由于随机采样修正后的单变量边缘而产生校准不良的多变量预报，而非参数方法在历史数据有限时表现不佳。需要一种既能保持参数化建模灵活性又能模拟非参数技术的方法。

Method: 提出COBASE框架，使用copula方法结合秩重排机制，在保持参数化建模灵活性的同时模拟非参数技术，确保校准的边缘分布和真实的依赖结构重建。

Result: 在奥地利ALADIN-LAEF集合的多站点2米温度预报和荷兰ECMWF系统的温度与露点温度联合预报上，COBASE变体始终优于传统copula方法（如GCA），与最先进的非参数方法（如SimSchaake和ECC）性能相当，在不同设置下差异极小。

Conclusion: COBASE作为多变量集合后处理的一个竞争性和稳健的替代方案，为参数化和非参数依赖重建提供了一个原则性的桥梁。

Abstract: Weather predictions are often provided as ensembles generated by repeated
runs of numerical weather prediction models. These forecasts typically exhibit
bias and inaccurate dependence structures due to numerical and dispersion
errors, requiring statistical postprocessing for improved precision. A common
correction strategy is the two-step approach: first adjusting the univariate
forecasts, then reconstructing the multivariate dependence. The second step is
usually handled with nonparametric methods, which can underperform when
historical data are limited. Parametric alternatives, such as the Gaussian
Copula Approach (GCA), offer theoretical advantages but often produce poorly
calibrated multivariate forecasts due to random sampling of the corrected
univariate margins. In this work, we introduce COBASE, a novel copula-based
postprocessing framework that preserves the flexibility of parametric modeling
while mimicking the nonparametric techniques through a rank-shuffling
mechanism. This design ensures calibrated margins and realistic dependence
reconstruction. We evaluate COBASE on multi-site 2-meter temperature forecasts
from the ALADIN-LAEF ensemble over Austria and on joint forecasts of
temperature and dew point temperature from the ECMWF system in the Netherlands.
Across all regions, COBASE variants consistently outperform traditional
copula-based approaches, such as GCA, and achieve performance on par with
state-of-the-art nonparametric methods like SimSchaake and ECC, with only
minimal differences across settings. These results position COBASE as a
competitive and robust alternative for multivariate ensemble postprocessing,
offering a principled bridge between parametric and nonparametric dependence
reconstruction.

</details>
