<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 10]
- [stat.AP](#stat.AP) [Total: 4]
- [stat.ME](#stat.ME) [Total: 10]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Deep networks learn to parse uniform-depth context-free languages from local statistics](https://arxiv.org/abs/2602.06065)
*Jack T. Parley,Francesco Cagnetta,Matthieu Wyart*

Main category: stat.ML

TL;DR: 该研究提出了一个可调的概率上下文无关文法（PCFG）框架，通过控制歧义度和跨尺度相关性结构，研究语言结构如何从句子中学习，并建立了学习机制与样本复杂度之间的关系。


<details>
  <summary>Details</summary>
Motivation: 理解语言结构如何仅从句子中学习是认知科学和机器学习的核心问题。虽然大型语言模型（LLMs）在预测下一个词时表现出解析文本的能力，但哪些数据统计特征使这些成就成为可能，以及需要多少数据，仍然未知。

Method: 1. 引入可调类别的PCFG，可以控制歧义度和跨尺度相关性结构；2. 提出受深度卷积网络结构启发的推理算法，将可学习性和样本复杂度与特定语言统计特征联系起来；3. 在深度卷积和基于Transformer的架构上进行实证验证。

Result: 建立了一个统一框架，其中不同尺度的相关性解决了局部歧义，使数据的层次表示得以涌现。验证了该框架在深度卷积和Transformer架构中的有效性。

Conclusion: 该研究提供了一个系统框架，将语言统计特征与神经网络学习层次表示的能力联系起来，揭示了跨尺度相关性在解决局部歧义和促进语言结构学习中的关键作用。

Abstract: Understanding how the structure of language can be learned from sentences alone is a central question in both cognitive science and machine learning. Studies of the internal representations of Large Language Models (LLMs) support their ability to parse text when predicting the next word, while representing semantic notions independently of surface form. Yet, which data statistics make these feats possible, and how much data is required, remain largely unknown. Probabilistic context-free grammars (PCFGs) provide a tractable testbed for studying these questions. However, prior work has focused either on the post-hoc characterization of the parsing-like algorithms used by trained networks; or on the learnability of PCFGs with fixed syntax, where parsing is unnecessary. Here, we (i) introduce a tunable class of PCFGs in which both the degree of ambiguity and the correlation structure across scales can be controlled; (ii) provide a learning mechanism -- an inference algorithm inspired by the structure of deep convolutional networks -- that links learnability and sample complexity to specific language statistics; and (iii) validate our predictions empirically across deep convolutional and transformer-based architectures. Overall, we propose a unifying framework where correlations at different scales lift local ambiguities, enabling the emergence of hierarchical representations of the data.

</details>


### [2] [Algebraic Robustness Verification of Neural Networks](https://arxiv.org/abs/2602.06105)
*Yulia Alexandr,Hao Duan,Guido Montúfar*

Main category: stat.ML

TL;DR: 论文将神经网络鲁棒性验证表述为代数优化问题，利用欧几里得距离度（ED degree）作为架构相关的内在复杂度度量，并开发了基于数值同伦延拓的精确验证算法。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络鲁棒性验证方法缺乏对验证问题内在复杂度的理论理解。作者希望从代数几何角度分析验证问题的本质复杂度，为神经网络验证提供理论基础。

Method: 1. 将鲁棒性验证表述为距离最小化代数优化问题；2. 引入ED degree作为架构复杂度度量；3. 定义ED判别式区分验证难易实例；4. 提出参数判别式识别复杂度降低的参数区域；5. 推导多种架构的ED degree闭式表达式；6. 开发基于数值同伦延拓的精确验证算法。

Result: 1. 建立了神经网络验证与度量代数几何的理论联系；2. 获得了多种神经网络架构的ED degree闭式表达式；3. 在无限宽度极限下推导了实临界点期望数的公式；4. 实现了基于同伦延拓的精确鲁棒性认证算法。

Conclusion: 该工作为神经网络鲁棒性验证提供了代数几何理论基础，将ED degree作为验证复杂度的自然度量，并开发了相应的计算工具，为理解神经网络验证问题的本质复杂度提供了新视角。

Abstract: We formulate formal robustness verification of neural networks as an algebraic optimization problem. We leverage the Euclidean Distance (ED) degree, which is the generic number of complex critical points of the distance minimization problem to a classifier's decision boundary, as an architecture-dependent measure of the intrinsic complexity of robustness verification. To make this notion operational, we define the associated ED discriminant, which characterizes input points at which the number of real critical points changes, distinguishing test instances that are easier or harder to verify. We provide an explicit algorithm for computing this discriminant. We further introduce the parameter discriminant of a neural network, identifying parameters where the ED degree drops and the decision boundary exhibits reduced algebraic complexity. We derive closed-form expressions for the ED degree for several classes of neural architectures, as well as formulas for the expected number of real critical points in the infinite-width limit. Finally, we present an exact robustness certification algorithm based on numerical homotopy continuation, establishing a concrete link between metric algebraic geometry and neural network verification.

</details>


### [3] [Inheritance Between Feedforward and Convolutional Networks via Model Projection](https://arxiv.org/abs/2602.06245)
*Nicolas Ewen,Jairo Diaz-Rodriguez,Kelly Ramsay*

Main category: stat.ML

TL;DR: 论文提出模型投影方法，将卷积网络转换为类前馈网络形式，实现参数高效迁移学习


<details>
  <summary>Details</summary>
Motivation: 前馈网络和卷积网络技术常被跨家族复用，但两者模型类别关系不明确；两者在每输入参数化方面存在不匹配

Method: 提出模型投影方法：冻结预训练的每输入通道滤波器，为每个（输出通道，输入通道）贡献学习单个标量门控；证明投影节点采用广义前馈网络形式

Result: 模型投影在多个ImageNet预训练骨干网络和下游图像分类数据集上表现出色，成为简单训练方案下的强迁移学习基线

Conclusion: 广义前馈网络是广义卷积网络的严格子集；模型投影使卷积网络能够继承前馈网络技术，同时保持参数高效性

Abstract: Techniques for feedforward networks (FFNs) and convolutional networks (CNNs) are frequently reused across families, but the relationship between the underlying model classes is rarely made explicit. We introduce a unified node-level formalization with tensor-valued activations and show that generalized feedforward networks form a strict subset of generalized convolutional networks. Motivated by the mismatch in per-input parameterization between the two families, we propose model projection, a parameter-efficient transfer learning method for CNNs that freezes pretrained per-input-channel filters and learns a single scalar gate for each (output channel, input channel) contribution. Projection keeps all convolutional layers adaptable to downstream tasks while substantially reducing the number of trained parameters in convolutional layers. We prove that projected nodes take the generalized FFN form, enabling projected CNNs to inherit feedforward techniques that do not rely on homogeneous layer inputs. Experiments across multiple ImageNet-pretrained backbones and several downstream image classification datasets show that model projection is a strong transfer learning baseline under simple training recipes.

</details>


### [4] [Time-uniform conformal and PAC prediction](https://arxiv.org/abs/2602.06297)
*Kayla E. Scharfstein,Arun Kumar Kuchibhotla*

Main category: stat.ML

TL;DR: 本文提出了一种适用于序列数据环境的扩展型置信预测方法，能够在数据流式生成且样本量不固定的情况下提供随时有效的预测区间保证。


<details>
  <summary>Details</summary>
Motivation: 传统置信预测方法在序列设置中存在两个主要问题：1）需要预先固定样本量才能提供保证；2）无法处理序列更新的预测。随着机器学习模型越来越多地应用于高风险决策，需要在流式数据环境中提供可靠的置信度量化方法。

Method: 开发了置信预测和PAC预测框架的序列扩展版本，适用于样本量不预先固定的序列设置。该方法生成随时有效的预测集，即使在分析师基于数据选择时间点的情况下，也能保证期望覆盖率维持在所需水平。

Result: 提出的方法在理论和实证上都得到了验证：1）提供了理论保证；2）在模拟和真实数据集上证明了方法的有效性和实用性。

Conclusion: 该研究成功地将置信预测框架扩展到序列设置，解决了传统方法在流式数据环境中的局限性，为机器学习模型在高风险决策中的不确定性量化提供了更灵活的解决方案。

Abstract: Given that machine learning algorithms are increasingly being deployed to aid in high stakes decision-making, uncertainty quantification methods that wrap around these black box models such as conformal prediction have received much attention in recent years. In sequential settings, where data are observed/generated in a streaming fashion, traditional conformal methods do not provide any guarantee without fixing the sample size. More importantly, traditional conformal methods cannot cope with sequentially updated predictions. As such, we develop an extension of the conformal prediction and related probably approximately correct (PAC) prediction frameworks to sequential settings where the number of data points is not fixed in advance. The resulting prediction sets are anytime-valid in that their expected coverage is at the required level at any time chosen by the analyst even if this choice depends on the data. We present theoretical guarantees for our proposed methods and demonstrate their validity and utility on simulated and real datasets.

</details>


### [5] [High-Dimensional Limit of Stochastic Gradient Flow via Dynamical Mean-Field Theory](https://arxiv.org/abs/2602.06320)
*Sota Nishiyama,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 该研究提出了一个分析多轮次小批量SGD高维动态的理论框架，基于随机梯度流(SGF)近似，使用动态平均场理论(DMFT)推导出低维连续时间方程，统一了现有SGD动态描述。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏分析非线性模型多轮次小批量SGD高维渐进行为的解析框架，需要填补这一空白以理解现代机器学习模型在高维空间中的训练动态。

Method: 使用随机梯度流(SGF)近似多轮次小批量SGD，在数据样本数n和维度d成比例增长的极限下，应用动态平均场理论(DMFT)推导出低维连续时间方程，并扩展DMFT技术处理SGF的随机性。

Result: 推导出描述SGF参数渐近分布的低维连续时间方程系统，该理论适用于广义线性模型和两层神经网络等广泛模型，并能统一现有SGD动态描述框架。

Conclusion: 该研究填补了多轮次小批量SGD高维动态分析的空白，提供了一个统一的理论框架，为理解现代机器学习模型的训练动态提供了新的分析工具。

Abstract: Modern machine learning models are typically trained via multi-pass stochastic gradient descent (SGD) with small batch sizes, and understanding their dynamics in high dimensions is of great interest. However, an analytical framework for describing the high-dimensional asymptotic behavior of multi-pass SGD with small batch sizes for nonlinear models is currently missing. In this study, we address this gap by analyzing the high-dimensional dynamics of a stochastic differential equation called a \emph{stochastic gradient flow} (SGF), which approximates multi-pass SGD in this regime. In the limit where the number of data samples $n$ and the dimension $d$ grow proportionally, we derive a closed system of low-dimensional and continuous-time equations and prove that it characterizes the asymptotic distribution of the SGF parameters. Our theory is based on the dynamical mean-field theory (DMFT) and is applicable to a wide range of models encompassing generalized linear models and two-layer neural networks. We further show that the resulting DMFT equations recover several existing high-dimensional descriptions of SGD dynamics as special cases, thereby providing a unifying perspective on prior frameworks such as online SGD and high-dimensional linear regression. Our proof builds on the existing DMFT technique for gradient flow and extends it to handle the stochasticity in SGF using tools from stochastic calculus.

</details>


### [6] [Revisiting the Sliced Wasserstein Kernel for persistence diagrams: a Figalli-Gigli approach](https://arxiv.org/abs/2602.06539)
*Marc Janthial,Théo Lacombe*

Main category: stat.ML

TL;DR: 本文提出了一种新的切片Figalli-Gigli核(SFGK)，用于持久图分析，相比现有的切片Wasserstein核(SWK)更忠实于持久图的自然几何结构。


<details>
  <summary>Details</summary>
Motivation: 现有的切片Wasserstein核(SWK)虽然有效，但依赖于对Wasserstein距离的临时调整来适应持久图的特殊几何结构。作者希望直接使用Figalli-Gigli距离（持久图比较中常用的部分匹配距离）作为核的基础构建块。

Method: 提出切片Figalli-Gigli核(SFGK)，直接使用Figalli-Gigli距离代替Wasserstein距离作为核的构建基础。该方法保留了SWK的重要特性，同时更忠实于持久图的自然几何。

Result: SFGK在理论上具有与SWK相似的重要性质，包括嵌入的失真结果和计算便利性，同时能更自然地处理无限持久图和持久测度。在数值实验中，SFGK在基准应用上与SWK表现相当。

Conclusion: 切片Figalli-Gigli核提供了一种更忠实于持久图几何结构的核方法，在保持SWK优点的同时，能够更自然地处理持久图分析中的几何特性。

Abstract: The Sliced Wasserstein Kernel (SWK) for persistence diagrams was introduced in (Carri{è}re et al. 2017) as a powerful tool to implicitly embed persistence diagrams in a Hilbert space with reasonable distortion. This kernel is built on the intuition that the Figalli-Gigli distance-that is the partial matching distance routinely used to compare persistence diagrams-resembles the Wasserstein distance used in the optimal transport literature, and that the later could be sliced to define a positive definite kernel on the space of persistence diagrams. This efficient construction nonetheless relies on ad-hoc tweaks on the Wasserstein distance to account for the peculiar geometry of the space of persistence diagrams. In this work, we propose to revisit this idea by directly using the Figalli-Gigli distance instead of the Wasserstein one as the building block of our kernel. On the theoretical side, our sliced Figalli-Gigli kernel (SFGK) shares most of the important properties of the SWK of Carri{è}re et al., including distortion results on the induced embedding and its ease of computation, while being more faithful to the natural geometry of persistence diagrams. In particular, it can be directly used to handle infinite persistence diagrams and persistence measures. On the numerical side, we show that the SFGK performs as well as the SWK on benchmark applications.

</details>


### [7] [Operationalizing Stein's Method for Online Linear Optimization: CLT-Based Optimal Tradeoffs](https://arxiv.org/abs/2602.06545)
*Zhiyu Zhang,Aaditya Ramdas*

Main category: stat.ML

TL;DR: 将Stein方法转化为计算高效的在线线性优化算法，实现加性锐利的上界，超越传统最优性


<details>
  <summary>Details</summary>
Motivation: 传统基于动态规划的在线线性优化算法计算效率低，需要将描述性结果转化为计算高效的算法

Method: 将Stein方法（概率极限定理证明框架）操作化为计算高效的OLO算法，受Röllin（2018）Wasserstein鞅中心极限定理证明启发

Result: 1. 相同计算复杂度下超越OGD和MWU的总损失上界
2. 实现总损失与比较器最大遗憾之间连续最优两点权衡
3. 无界支持随机化对手下实现噪声反馈OLO的锐利期望性能保证

Conclusion: Stein方法可转化为计算高效的OLO算法，实现加性锐利上界，超越传统最优性，在多个具体应用中展现优势

Abstract: Adversarial online linear optimization (OLO) is essentially about making performance tradeoffs with respect to the unknown difficulty of the adversary. In the setting of one-dimensional fixed-time OLO on a bounded domain, it has been observed since Cover (1966) that achievable tradeoffs are governed by probabilistic inequalities, and these descriptive results can be converted into algorithms via dynamic programming, which, however, is not computationally efficient. We address this limitation by showing that Stein's method, a classical framework underlying the proofs of probabilistic limit theorems, can be operationalized as computationally efficient OLO algorithms. The associated regret and total loss upper bounds are "additively sharp", meaning that they surpass the conventional big-O optimality and match normal-approximation-based lower bounds by additive lower order terms. Our construction is inspired by the remarkably clean proof of a Wasserstein martingale central limit theorem (CLT) due to Röllin (2018).
  Several concrete benefits can be obtained from this general technique. First, with the same computational complexity, the proposed algorithm improves upon the total loss upper bounds of online gradient descent (OGD) and multiplicative weight update (MWU). Second, our algorithm can realize a continuum of optimal two-point tradeoffs between the total loss and the maximum regret over comparators, improving upon prior works in parameter-free online learning. Third, by allowing the adversary to randomize on an unbounded support, we achieve sharp in-expectation performance guarantees for OLO with noisy feedback.

</details>


### [8] [Infinite-dimensional generative diffusions via Doob's h-transform](https://arxiv.org/abs/2602.06621)
*Thorben Pieper-Sethmacher,Daniel Paulin*

Main category: stat.ML

TL;DR: 提出基于Doob's h变换的无限维生成扩散模型框架，通过指数测度变换将参考扩散推向目标分布，而非依赖去噪过程的时间反转。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型方法难以直接推广到无限维设置，限制了模型的灵活性。需要一种能够在无限维空间中严格定义生成扩散模型的框架。

Method: 使用Doob's h变换和指数测度变换方法，将参考扩散过程强制推向目标分布。通过最小化分数匹配目标来近似测度变换后的强制过程。

Result: 在可验证条件下严格推导了构造，建立了相对于目标测度的界限。在合成和真实数据上验证了方法的有效性。

Conclusion: 该框架为无限维生成扩散模型提供了严格基础，相比现有方法具有更好的泛化能力和灵活性。

Abstract: This paper introduces a rigorous framework for defining generative diffusion models in infinite dimensions via Doob's h-transform. Rather than relying on time reversal of a noising process, a reference diffusion is forced towards the target distribution by an exponential change of measure. Compared to existing methodology, this approach readily generalises to the infinite-dimensional setting, hence offering greater flexibility in the diffusion model. The construction is derived rigorously under verifiable conditions, and bounds with respect to the target measure are established. We show that the forced process under the changed measure can be approximated by minimising a score-matching objective and validate our method on both synthetic and real data.

</details>


### [9] [Missing At Random as Covariate Shift: Correcting Bias in Iterative Imputation](https://arxiv.org/abs/2602.06713)
*Luke Shannon,Song Liu,Katarzyna Reluga*

Main category: stat.ML

TL;DR: 论文提出了一种基于重要性加权的新型缺失数据插补方法，通过纠正观测数据与未观测数据之间的协变量偏移偏差，提升插补准确性。


<details>
  <summary>Details</summary>
Motivation: 现有缺失数据插补方法未考虑观测数据与未观测数据之间的分布偏移（协变量偏移），这种偏差会导致插补结果不理想，影响下游机器学习性能。

Method: 将缺失数据插补建模为风险最小化问题，推导出理论上有效的重要性权重来纠正分布偏差，并提出联合估计重要性权重和插补模型的新算法。

Result: 在基准数据集上，相比未加权的相同方法，均方根误差降低达7%，Wasserstein距离减少达20%。

Conclusion: 通过重要性权重纠正协变量偏移偏差能显著提升缺失数据插补的准确性，提出的联合估计算法在多个指标上优于传统方法。

Abstract: Accurate imputation of missing data is critical to downstream machine learning performance. We formulate missing data imputation as a risk minimisation problem, which highlights a covariate shift between the observed and unobserved data distributions. This covariate shift induced bias is not accounted for by popular imputation methods and leads to suboptimal performance. In this paper, we derive theoretically valid importance weights that correct for the induced distributional bias. Furthermore, we propose a novel imputation algorithm that jointly estimates both the importance weights and imputation models, enabling bias correction throughout the imputation process. Empirical results across benchmark datasets show reductions in root mean squared error and Wasserstein distance of up to 7% and 20%, respectively, compared to otherwise identical unweighted methods.

</details>


### [10] [Optimal Learning-Rate Schedules under Functional Scaling Laws: Power Decay and Warmup-Stable-Decay](https://arxiv.org/abs/2602.06797)
*Binghui Li,Zilin Wang,Fengling Chen,Shiyang Zhao,Ruiheng Zheng,Lei Wu*

Main category: stat.ML

TL;DR: 在函数缩放定律框架下，研究最优学习率调度，发现任务难度存在相变：简单任务遵循幂衰减，困难任务呈现热身-稳定-衰减结构。


<details>
  <summary>Details</summary>
Motivation: 现有学习率调度策略缺乏理论指导，本文旨在基于函数缩放定律框架，为线性回归和LLM预训练等任务提供最优学习率调度的理论分析。

Method: 在固定训练时长N下，基于函数缩放定律框架，推导最优学习率调度公式，分析不同任务难度下的相变行为，并研究形状固定调度的性能。

Result: 发现任务难度存在相变：当s≥1-1/β时，最优调度为幂衰减；当s<1-1/β时，呈现热身-稳定-衰减结构。将幂衰减调度应用于核回归SGD，消除了先前分析中的对数次优性。

Conclusion: 基于函数缩放定律的理论分析为学习率调度提供了原则性指导，揭示了任务难度对最优调度结构的决定性影响，并为实际调度策略提供了理论评估框架。

Abstract: We study optimal learning-rate schedules (LRSs) under the functional scaling law (FSL) framework introduced in Li et al. (2025), which accurately models the loss dynamics of both linear regression and large language model (LLM) pre-training. Within FSL, loss dynamics are governed by two exponents: a source exponent $s>0$ controlling the rate of signal learning, and a capacity exponent $β>1$ determining the rate of noise forgetting. Focusing on a fixed training horizon $N$, we derive the optimal LRSs and reveal a sharp phase transition. In the easy-task regime $s \ge 1 - 1/β$, the optimal schedule follows a power decay to zero, $η^*(z) = η_{\mathrm{peak}}(1 - z/N)^{2β- 1}$, where the peak learning rate scales as $η_{\mathrm{peak}} \eqsim N^{-ν}$ for an explicit exponent $ν= ν(s,β)$. In contrast, in the hard-task regime $s < 1 - 1/β$, the optimal LRS exhibits a warmup-stable-decay (WSD) (Hu et al. (2024)) structure: it maintains the largest admissible learning rate for most of training and decays only near the end, with the decay phase occupying a vanishing fraction of the horizon.
  We further analyze optimal shape-fixed schedules, where only the peak learning rate is tuned -- a strategy widely adopted in practiceand characterize their strengths and intrinsic limitations. This yields a principled evaluation of commonly used schedules such as cosine and linear decay. Finally, we apply the power-decay LRS to one-pass stochastic gradient descent (SGD) for kernel regression and show the last iterate attains the exact minimax-optimal rate, eliminating the logarithmic suboptimality present in prior analyses. Numerical experiments corroborate our theoretical predictions.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [11] [Early warning of Mpox outbreaks in U.S. jurisdictions using Lasso Vector Autoregression models with cross-jurisdictional lags](https://arxiv.org/abs/2602.06135)
*Hannah Craddock,Joel O. Wertheim,Eliah Aronoff-Spencer,Mark Beatty,David Valentine,Rishi Graham,Jade C. Wang,Lior Rennert,Seema Shah,Ravi Goyal,Natasha K. Martin*

Main category: stat.AP

TL;DR: 开发VAR-Lasso模型预测美国猴痘病例，利用跨辖区数据提升预测准确性，特别关注病例上升期的预警能力


<details>
  <summary>Details</summary>
Motivation: 猴痘传播具有间歇性和空间异质性，需要及时、针对特定区域的预测来支持精准公共卫生响应

Method: 使用带Lasso正则化的向量自回归模型(VAR-Lasso)，基于CDC监测数据对8个高发病率美国辖区进行滚动两周预测

Result: VAR-Lasso模型相比单变量AR模型和朴素移动平均基准，在斜率加权RMSE、MAE和偏差上分别减少12%/7%/66%和16%/13%/76%，能识别跨辖区的长期滞后预测因子

Conclusion: 稀疏多变量时间序列模型利用跨辖区病例数据对猴痘暴发进行早期预测具有重要价值，可帮助卫生部门提前部署资源和信息以减轻未来暴发风险

Abstract: Mpox is an orthopoxvirus that infects humans and animals and is transmitted primarily through close physical contact. The episodic and spatially heterogeneous dynamics of Mpox transmission underscores the need for timely, area-specific forecasts to support targeted public health responses in the U.S. We develop a Vector Autoregression model with Lasso regularization (VAR-Lasso) to generate rolling two-week-ahead forecasts of weekly Mpox cases for eight high-incidence U.S. jurisdictions using national surveillance data from the Centers for Disease Control and Prevention (CDC). The VAR-Lasso model identifies significant long-lag, cross-jurisdictional predictors. For a case study in San Diego County (SDC), these statistical predictors align with phylogenetic analysis that traces a 2023 cluster in SDC to an outbreak in Illinois six months earlier. As the need for public health action is often greatest when incidence is increasing, our performance evaluation focuses on positive-slope weighted error metrics. Forecast performance of the VAR-Lasso model is compared to a uni-variate Auto-Regressive (AR) Lasso model and a naive moving-average estimate. The models are compared using slope-weighted Root Mean Squared Error (RMSE), slope-weighted Mean Absolute Error (MAE), and slope-weighted bias. Across all observations, the VAR-Lasso model reduces slope-weighted RMSE, MAE, and bias by 12%, 7%, and 66% relative to the AR model, and by 16%, 13%, and 76% relative to the naive benchmark. Our findings highlight the value of sparse multivariate time-series models that leverage cross-jurisdictional case data for early forecasting of Mpox outbreaks. Such forecasting can aid health departments in proactively providing timely resources and messaging to mitigate the risks of a future outbreak.

</details>


### [12] [Non-Linear Drivers of Population Dynamics: a Nonparametric Coalescent Approach](https://arxiv.org/abs/2602.06148)
*Filippo Monti,Nuno R. Faria,Xiang Ji,Philippe Lemey,Moritz U. G. Kraemer,Marc A. Suchard*

Main category: stat.AP

TL;DR: 提出一个灵活的贝叶斯框架，通过高斯过程先验将协变量整合到具有分段恒定有效种群大小的溯祖模型中，以捕捉非线性协变量效应。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设协变量与有效种群大小之间存在对数线性关系，这种假设可能无法捕捉复杂的生物过程，并在真实关系为非线性时引入偏差。需要更灵活的方法来理解生态和环境因素如何驱动种群动态。

Method: 开发了一个贝叶斯框架，将协变量整合到具有分段恒定有效种群大小的溯祖模型中，使用高斯过程先验来容纳非线性协变量效应，同时结合高斯马尔可夫随机场来确保有效种群大小轨迹的平滑性。

Result: 通过模拟研究和三个实证应用（巴西黄热病毒动态、晚第四纪麝牛种群统计、喀麦隆HIV-1 CRF02-AG进化）表明，该方法既能确认适当的线性关系，又能揭示原本会被遗漏或错误表征的非线性协变量效应。

Conclusion: 该框架通过实现更准确和生物学上更真实的建模，推进了系统动力学推断，能够更好地理解环境和流行病学因素如何随时间塑造种群大小。

Abstract: Effective population size (Ne(t)) is a fundamental parameter in population genetics and phylodynamics that quantifies genetic diversity and reveals demographic history. Coalescent-based methods enable the inference of Ne(t) trajectories through time from phylogenies reconstructed from molecular sequence data. Understanding the ecological and environmental drivers of population dynamics requires linking Ne(t) to external covariates. Existing approaches typically impose log-linear relationships between covariates and Ne(t), which may fail to capture complex biological processes and can introduce bias when the true relationship is nonlinear. We present a flexible Bayesian framework that integrates covariates into coalescent models with piecewise-constant Ne(t) through a Gaussian process (GP) prior. The GP, a distribution over functions, naturally accommodates nonlinear covariate effects without restrictive parametric assumptions. This formulation improves estimation of covariate-Ne(t) relationships, mitigates bias under nonlinear associations, and yields interpretable uncertainty quantification that varies across the covariate space. To balance global covariate-driven patterns with local temporal dynamics, we couple the GP prior with a Gaussian Markov random field that enforces smoothness in Ne(t) trajectories. Through simulation studies and three empirical applications - yellow fever virus dynamics in Brazil (2016-2018), late-Quaternary musk ox demography, and HIV-1 CRF02-AG evolution in Cameroon - we demonstrate that our method both confirms linear relationships where appropriate and reveals nonlinear covariate effects that would otherwise be missed or mischaracterized. This framework advances phylodynamic inference by enabling more accurate and biologically realistic modeling of how environmental and epidemiological factors shape population size through time.

</details>


### [13] [Evaluating Predictive Modeling Strategies for Predicting Individual Treatment Effects in Precision Medicine](https://arxiv.org/abs/2602.06210)
*Pamela M. Chiroque-Solano,M Lee Van Horn,Thomas Jaki*

Main category: stat.AP

TL;DR: 比较了30多种预测个体治疗效果(PITE)的建模方法，发现惩罚回归和投影方法在大多数情况下表现最佳，而灵活学习器仅在强信号和大样本下表现好。


<details>
  <summary>Details</summary>
Motivation: 精准医疗需要准确预测个体治疗效果(PITE)，但PITE估计面临反事实不可观测、高维度和复杂交互作用的挑战，需要系统比较不同建模方法的性能。

Method: 使用结构化模拟框架比较30多种建模策略，包括惩罚回归、投影方法、灵活学习器和树集成方法。模拟变化样本量、维度、多重共线性和交互复杂度，通过RMSE和方向准确率评估性能。

Result: 惩罚回归和投影方法(岭回归、LASSO、弹性网络、PLS、PCR)在RMSE和方向准确率上表现最稳健。灵活学习器仅在强信号和大样本下表现优异。内部验证过于乐观，外部验证更能揭示模型弱点。

Conclusion: 推荐使用稳健的线性/投影方法作为默认选择，并强调严格外部验证的必要性，特别是在存在分布偏移和复杂交互作用的情况下。

Abstract: Precision medicine seeks to match patients with treatments that produce the greatest benefit. The Predicted Individual Treatment Effect (PITE)-the difference between predicted outcomes under treatment and control-quantifies this benefit but is difficult to estimate due to unobserved counterfactuals, high dimensionality, and complex interactions. We compared 30+ modeling strategies, including penalized and projection-based methods, flexible learners, and tree-ensembles, using a structured simulation framework varying sample size, dimensionality, multicollinearity, and interaction complexity. Performance was measured using root mean squared error (RMSE) for prediction accuracy and directional accuracy (DIR) for correctly classifying benefit versus harm. Internal validation produced optimistic estimates, whereas external validation with distributional shifts and higher-order interactions more clearly revealed model weaknesses. Penalized and projection-based approaches-ridge, lasso, elastic net, partial least squares (PLS), and principal components regression (PCR)-consistently achieved strong RMSE and DIR performance. Flexible learners excelled only under strong signals and sufficient sample sizes. Results highlight robust linear/projection defaults and the necessity of rigorous external validation.

</details>


### [14] [Modeling the Hazard Function with Non-linear Systems in Dynamical Survival Analysis](https://arxiv.org/abs/2602.06322)
*Dananjani Liyanage,Mahmudul Bari Hridoy,Fahad Mostafa*

Main category: stat.AP

TL;DR: 提出基于高阶ODE的危险函数建模框架，能捕捉振荡、非线性阻尼等复杂风险动态，超越传统单调模型


<details>
  <summary>Details</summary>
Motivation: 传统一阶ODE危险函数模型只能产生单调风险模式，无法表示振荡行为、非线性阻尼或耦合增长衰减动态，限制了在生存分析中的应用

Method: 提出高阶ODE统计框架，将危险函数建模为依赖当前水平、变化率和时间的系统；通过将高阶ODE转化为非线性一阶方程组进行数值求解，通过累积危险函数反演生成失效时间；开发右删失下的似然推断方法

Result: 该框架能捕捉复杂的时间依赖风险行为，通过矩生成函数分析表征尾部行为；模拟研究和真实世界生存数据验证了振荡危险动态能捕捉超出标准单调模型的时间风险模式

Conclusion: 高阶ODE危险函数模型为生存分析和可靠性研究提供了更灵活的工具，能够表示传统单调模型无法捕捉的复杂风险动态

Abstract: Hazard functions play a central role in survival analysis, offering insight into the underlying risk dynamics of time to event data, with broad applications in medicine, epidemiology, and related fields. First order ordinary differential equation (ODE) formulations of the hazard function have been explored as extensions beyond classical parametric models. However, such approaches typically produce monotonic hazard patterns, limiting their ability to represent oscillatory behavior, nonlinear damping, or coupled growth decay dynamics. We propose a new statistical framework for modeling and simulating hazard functions governed by higher-order ODEs, allowing risk to depend on both its current level, its rate of change, and time. This class of models captures complex time dependent risk behaviors relevant to survival analysis and reliability studies. We develop a simulation procedure by reformulating the higher order ODE as a system of nonlinear first order equations solved numerically, with failure times generated via cumulative hazard inversion. Likelihood based inference under right censoring is also developed, and moment generating function analysis is used to characterize tail behavior. The proposed framework is evaluated through simulation studies and illustrated using real world survival data, where oscillatory hazard dynamics capture temporal risk patterns beyond standard monotone models.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [15] [A Compound Logistic Regression Model for Binary Responses](https://arxiv.org/abs/2602.06153)
*Anthony Almudevar,Jacob Almudevar*

Main category: stat.ME

TL;DR: 提出复合逻辑回归模型，扩展传统逻辑回归以处理相关响应和协变量，克服固定渐近线限制


<details>
  <summary>Details</summary>
Motivation: 传统逻辑回归的渐近线固定在0和1，这在许多应用中不适用。现有更灵活的模型通常通过为逻辑响应函数添加额外参数来实现，但需要扩展到处理相关响应和协变量。

Method: 提出复合逻辑回归模型，其中平均响应是多个逻辑回归函数的函数。这允许更大的模型多样性，同时保留逻辑回归的优点。

Result: 该模型能够处理相关响应和协变量，提供比传统逻辑回归更灵活的建模框架，适用于渐近线约束不合适的应用场景。

Conclusion: 复合逻辑回归模型扩展了传统逻辑回归，克服了固定渐近线的限制，同时保持了逻辑回归的优势，为更广泛的应用提供了灵活的建模工具。

Abstract: Logistic regression is the most commonly used method for constructing predictive models for binary responses. One significant drawback to this approach, however, is that the asymptotes of the logistic response function are fixed at 0 and 1, and there are many applications for which this constraint is inappropriate. More flexible models have been proposed for this application, most proceeding by supplementing the logistic response function with additional parameters. In this article we extend these models to allow correlated responses and the inclusion of covariates. This is achieved through the \emph{compound logistic regression model}, for which the mean response is a function of several logistic regression functions. This permits a greater variety of models, while retaining the advantages of logistic regression.

</details>


### [16] [Latent variation in pathogen strain-specific effects under multiple-versions-of-treatment theory](https://arxiv.org/abs/2602.06262)
*Bronner P. Gonçalves*

Main category: stat.ME

TL;DR: 该论文探讨了在存在病原体菌株特异性效应异质性的情况下，如何解释感染对健康影响的流行病学研究，强调了菌株组成信息的重要性。


<details>
  <summary>Details</summary>
Motivation: 基于证据的感染政策需要准确估计感染对健康的影响，但病原体变异（不同菌株导致不同不良结果）可能使许多传染源的研究复杂化。当存在菌株特异性效应异质性且缺乏菌株组成信息时，需要澄清流行病学研究中报告量的因果解释。

Method: 使用潜在结果和因果推断理论，分析存在多种治疗版本（不同菌株）的情况。论证这些研究中常报告的量具有依赖于感染菌株群体频率的因果解释，并讨论治疗变异无关性假设可能被违反时的可迁移性问题。

Result: 研究表明，在存在菌株特异性效应异质性的情况下，流行病学研究中报告的量具有因果解释，但这种解释依赖于感染菌株的群体频率。可迁移性需要超出非复合暴露的额外考虑。

Conclusion: 考虑菌株特异性效应的潜在异质性有助于解释这些研究，并强调了病原体亚型数据的价值。对于存在治疗变异的情况，因果解释和可迁移性都需要特别考虑菌株组成信息。

Abstract: Evidence-informed policy on infections requires estimates of their effects on health. However, pathogenic variation, whereby occurrence of adverse outcomes depends on the infecting strain, might complicate the study of many infectious agents. Here, we consider the interpretation of epidemiologic studies on effects of infections on health when there is heterogeneity in strain-specific effects and information on strain composition is unavailable. We use potential outcomes and causal inference theory for analyses in the presence of multiple versions of treatment to argue that oft-reported quantities in these studies have a causal interpretation that depends on population frequencies of infecting strains. Moreover, as in other contexts where the treatment-variation-irrelevance assumption might be violated, transportability requires additional considerations, beyond those needed for non-compound exposures. This discussion, that considers potential heterogeneity in strain-specific effects, will facilitate interpretation of these studies, and for the reasons mentioned above, also highlights the value of pathogen subtype data.

</details>


### [17] [Conformal changepoint localization](https://arxiv.org/abs/2602.06267)
*Rohan Hore,Aaditya Ramdas*

Main category: stat.ME

TL;DR: CONCH是一种无需分布假设的离线变点定位方法，仅基于可交换性构建有限样本置信集，具有理论保证和实际有效性。


<details>
  <summary>Details</summary>
Motivation: 现有变点定位方法通常依赖参数假设、尾部条件或渐近近似，或仅提供点估计，缺乏无需分布假设的有限样本置信集构建方法。

Method: 提出CONCH（CONformal CHangepoint localization）方法，基于可交换性论证构建置信集，通过证明conformal Neyman-Pearson引理推导出能产生信息丰富（小）置信集的评分函数。

Result: CONCH能在有限样本下提供精确的置信集覆盖，在弱假设下置信集的归一化长度收敛到零，且证明任何分布无关的变点定位方法都必须是CONCH的实例。

Conclusion: CONCH为无需分布假设的变点定位提供了理论严谨且实用的解决方案，在图像和文本等挑战性场景中表现良好。

Abstract: We study the problem of offline changepoint localization in a distribution-free setting. One observes a vector of data with a single changepoint, assuming that the data before and after the changepoint are iid (or more generally exchangeable) from arbitrary and unknown distributions. The goal is to produce a finite-sample confidence set for the index at which the change occurs without making any other assumptions. Existing methods often rely on parametric assumptions, tail conditions, or asymptotic approximations, or only produce point estimates. In contrast, our distribution-free algorithm, CONformal CHangepoint localization (CONCH), only leverages exchangeability arguments to construct confidence sets with finite sample coverage. By proving a conformal Neyman-Pearson lemma, we derive principled score functions that yield informative (small) sets. Moreover, with such score functions, the normalized length of the confidence set shrinks to zero under weak assumptions. We also establish a universality result showing that any distribution-free changepoint localization method must be an instance of CONCH. Experiments suggest that CONCH delivers precise confidence sets even in challenging settings involving images or text.

</details>


### [18] [Design-Conditional Prior Elicitation for Dirichlet Process Mixtures: A Unified Framework for Cluster Counts and Weight Control](https://arxiv.org/abs/2602.06301)
*JoonHo Lee*

Main category: stat.ME

TL;DR: 提出Design-Conditional Elicitation (DCE)框架，通过Gamma超先验将研究者对聚类结构的先验信念转化为可计算的参数，解决Dirichlet过程混合模型中浓度参数设定的难题。


<details>
  <summary>Details</summary>
Motivation: Dirichlet过程混合模型在教育和行为研究中广泛应用，但浓度参数的设定一直是个关键障碍。默认的超先验往往对聚类结构施加了强烈且非预期的假设，而现有的基于聚类数量的校准方法计算效率低下，且无法控制混合权重的分布。

Method: 提出Design-Conditional Elicitation (DCE)统一框架，包含三个核心贡献：1) 使用Two-Stage Moment Matching (TSMM)方法，结合闭式近似和精确牛顿优化来校准超参数，避免网格搜索；2) 引入Dual-Anchor协议来诊断和约束权重主导风险；3) 在开源R包DPprior中实现完整工作流程，包含可复现的诊断和报告清单。

Result: 模拟研究表明，常见的默认设置如Gamma(1, 1)会导致超过60%的后验崩溃率，而DCE校准的先验显著减少了偏差，并在不同数据信息水平下改善了聚类恢复效果。

Conclusion: DCE框架为Dirichlet过程混合模型提供了一种系统化的先验设定方法，能够将研究者的先验信念转化为可计算的超参数，避免了默认设置的缺陷，提高了模型的可靠性和解释性。

Abstract: Dirichlet process mixture (DPM) models are widely used for semiparametric Bayesian analysis in educational and behavioral research, yet specifying the concentration parameter remains a critical barrier. Default hyperpriors often impose strong, unintended assumptions about clustering, while existing calibration methods based on cluster counts suffer from computational inefficiency and fail to control the distribution of mixture weights. This article introduces Design-Conditional Elicitation (DCE), a unified framework that translates practitioner beliefs about cluster structure into coherent Gamma hyperpriors for a fixed design size J. DCE makes three contributions. First, it solves the computational bottleneck using Two-Stage Moment Matching (TSMM), which couples a closed-form approximation with an exact Newton refinement to calibrate hyperparameters without grid search. Second, addressing the "unintended prior" phenomenon, DCE incorporates a Dual-Anchor protocol to diagnose and optionally constrain the risk of weight dominance while transparently reporting the resulting trade-off against cluster-count fidelity. Third, the complete workflow is implemented in the open-source DPprior R package with reproducible diagnostics and a reporting checklist. Simulation studies demonstrate that common defaults such as Gamma(1, 1) induce posterior collapse rates exceeding 60% regardless of the true cluster structure, while DCE-calibrated priors substantially reduce bias and improve recovery across varying levels of data informativeness.

</details>


### [19] [E-values for Adaptive Clinical Trials: Anytime-Valid Monitoring in Practice](https://arxiv.org/abs/2602.06379)
*Alexandra Sokolova,Vadim Sokolov*

Main category: stat.ME

TL;DR: 本文介绍了e值和e过程在适应性临床试验中的应用，提供了方法论指南，比较了不同监测方法的性能，并开发了相应的R包实现。


<details>
  <summary>Details</summary>
Motivation: 适应性临床试验中的期中分析和灵活停止策略使得传统固定时间范围的统计方法难以保证统计有效性，需要开发能够处理可选停止和可选继续的统计方法。

Method: 采用e值和e过程构建赌注鞅方法，处理复合零假设，支持无效性监测，并与传统的组序贯和贝叶斯方法进行比较。

Result: 数值研究表明：校准的组序贯方法具有最高功效，e值方法提供稳健的随时有效性控制但功效中等，校准的贝叶斯方法最保守。在连续监测下，e值方法的功效可能超过组序贯方法。

Conclusion: e值和e过程为适应性临床试验提供了有效的随时有效性控制工具，可与传统方法结合使用，相关方法已在R包中实现并符合监管框架要求。

Abstract: Adaptive clinical trials rely on interim analyses, flexible stopping, and data-dependent design modifications that complicate statistical guarantees when fixed-horizon test statistics are repeatedly inspected or reused after adaptations. E-values and e-processes provide anytime-valid tests and confidence sequences that remain valid under optional stopping and optional continuation without requiring a prespecified monitoring schedule.
  This paper is a methodology guide for practitioners. We develop the betting-martingale construction of e-processes for two-arm randomized controlled trials, show how e-values naturally handle composite null hypotheses and support futility monitoring, and provide guidance on when e-values are appropriate, when established alternatives are preferable, and how to integrate e-value monitoring with group sequential and Bayesian adaptive workflows.
  A numerical study compares five monitoring rules -- naive and calibrated versions of frequentist, Bayesian, and e-value approaches -- in a two-arm binary-endpoint trial. Naive repeated testing and naive posterior thresholds inflate Type I error substantially under frequent interim looks. Among the valid methods, the calibrated group sequential rule achieves the highest power, the e-value rule provides robust anytime-valid control with moderate power, and the calibrated Bayesian rule is the most conservative.
  Extended simulations show that the power gap between group sequential and e-value methods depends on the monitoring schedule and reverses under continuous monitoring. The methodology, including futility monitoring, platform trial multiplicity control, and hybrid strategies combining e-values with established methods, is implemented in the open-source R package `evalinger` and situated within the regulatory framework of the January 2026 FDA draft guidance on Bayesian methodology.

</details>


### [20] [Social Interactions Models with Latent Structures](https://arxiv.org/abs/2602.06435)
*Zhongjian Lin,Zhentao Shi,Yapeng Zheng*

Main category: stat.ME

TL;DR: 论文提出了一种估计具有潜在结构异质性同伴效应的方法，使用Classifier-Lasso算法发现潜在聚类，并通过参数自助法解决二元选择模型中的偶发参数问题。


<details>
  <summary>Details</summary>
Motivation: 研究具有组固定效应和斜率异质性的异质性同伴效应估计与推断问题，特别是在存在潜在结构的情况下。需要解决二元选择模型中社会交互作用带来的偶发参数问题。

Method: 采用Classifier-Lasso算法一致地发现潜在结构并确定聚类数量。针对二元选择模型中的社会交互作用，提出参数自助法进行去偏，并建立其渐近有效性。

Result: 蒙特卡洛模拟证实了方法在有限样本下的优异性能。在学生风险行为应用中，算法检测到两个潜在聚类，并发现其中一个聚类内的同伴效应显著，展示了在揭示异质性社会交互作用中的实际应用价值。

Conclusion: 该方法能够有效估计具有潜在结构的异质性同伴效应，Classifier-Lasso算法能可靠发现潜在聚类，参数自助法成功解决了二元选择模型中的偶发参数问题，具有理论和实践意义。

Abstract: This paper studies estimation and inference of heterogeneous peer effects featuring group fixed effects and slope heterogeneity under latent structure. We adapt the Classifier-Lasso algorithm to consistently discover latent structures and determine the number of clusters. To solve the incidental parameter problem in the binary choice model with social interactions, we propose a parametric bootstrap method to debias and establish its asymptotic validity. Monte Carlo simulations confirm strong finite sample performance of our methods. In an application to students' risky behaviors, the algorithm detects two latent clusters and finds that peer effects are significant within one of the clusters, demonstrating the practical applicability in uncovering heterogeneous social interactions.

</details>


### [21] [On Stein's Method of Moments and Generalized Score Matching](https://arxiv.org/abs/2602.06482)
*Alfred Kume,Stephen G. Walker*

Main category: stat.ME

TL;DR: 论文展示了Stein类矩估计器的一个特例与广义得分匹配估计器类重合，通过将广义得分匹配置于矩估计框架中，可以扩展Stein类到广义矩方法，从而解决权重函数选择问题并得到具有最优性质的估计器。


<details>
  <summary>Details</summary>
Motivation: 广义得分匹配中权重函数的选择并不直接，需要解决这一实际问题。

Method: 将广义得分匹配置于矩估计框架中，扩展Stein类到广义矩方法，从而能够使用多个函数并推导具有最优性质的广义得分匹配估计器。

Result: 证明了Stein类矩估计器的特例与广义得分匹配估计器类重合，通过框架转换解决了权重函数选择问题。

Conclusion: 通过将广义得分匹配纳入矩估计框架，可以更灵活地处理权重函数选择问题，并得到具有最优统计性质的估计器。

Abstract: We show that a special case of method of moment estimator derived from the Stein class coincides with the class of generalized score matching estimator. Choosing a suitable weight function for generalized score matching is not straightforward. However, by placing it within the method of moment framework we can alleviate this problem by extending the Stein class to generalized method of moments. As a consequence we can work with a number of functions and hence derive generalized score matching estimators with optimal properties.

</details>


### [22] [Efficient Online Variational Estimation via Monte Carlo Sampling](https://arxiv.org/abs/2602.06579)
*Mathis Chagneux,Mathias Müller,Pierre Gloaguen,Sylvain Le Corff,Jimmy Olsson*

Main category: stat.ME

TL;DR: 提出一种在线变分估计算法，用于参数状态空间模型，通过蒙特卡洛采样和深度架构高效计算证据下界及其梯度，支持流数据场景下的模型参数和潜状态分布联合训练。


<details>
  <summary>Details</summary>
Motivation: 在流数据场景下，需要高效在线估计状态空间模型的参数和潜状态分布。传统方法难以同时处理模型参数训练和潜状态分布估计，特别是在数据连续到达的情况下。

Method: 基于i.i.d.蒙特卡洛采样，结合精心设计的深度架构，在流数据设置中高效计算证据下界及其梯度。算法支持模型参数和给定观测的潜状态分布同时训练。

Result: 方法在合成数据和真实空气质量数据上表现良好，展示了计算效率和灵活性。理论上有渐近对比函数的存在性和底层马尔可夫链的遍历性作为支撑。

Conclusion: 提出的在线变分估计算法有效解决了参数状态空间模型的流数据估计问题，可推广到状态空间模型中后验分布下加性期望的计算。

Abstract: This article addresses online variational estimation in parametric state-space models. We propose a new procedure for efficiently computing the evidence lower bound and its gradient in a streaming-data setting, where observations arrive sequentially. The algorithm allows for the simultaneous training of the model parameters and the distribution of the latent states given the observations. It is based on i.i.d. Monte Carlo sampling, coupled with a well-chosen deep architecture, enabling both computational efficiency and flexibility. The performance of the method is illustrated on both synthetic data and real-world air-quality data. The proposed approach is theoretically motivated by the existence of an asymptotic contrast function and the ergodicity of the underlying Markov chain, and applies more generally to the computation of additive expectations under posterior distributions in state-space models.

</details>


### [23] [A prediction interval for the population-wise error rate](https://arxiv.org/abs/2602.06828)
*Remi Luschei,Werner Brannath*

Main category: stat.ME

TL;DR: 构建了针对临床试验中重叠患者群体的人口水平错误率(PWER)的渐近预测区间，该区间在模拟中显示出所需的覆盖概率


<details>
  <summary>Details</summary>
Motivation: PWER是临床试验中评估患者接受无效治疗概率的多重I类错误标准，但由于未知的人口分层规模，通常只能估计PWER，因此需要构建预测区间来评估真实PWER的不确定性

Method: 应用delta方法构建PWER的渐近预测区间，通过模拟验证区间覆盖概率，并使用真实数据示例进行说明

Result: 构建的预测区间在模拟中显示出所需的覆盖概率，能够有效评估真实PWER的不确定性

Conclusion: 提出的渐近预测区间方法为临床试验中重叠患者群体的PWER评估提供了有效的统计工具，能够帮助研究人员更好地理解和控制多重I类错误

Abstract: We construct an asymptotic prediction interval for the population-wise error rate (PWER), which is a multiple type I error criterion for clinical trials with overlapping patient populations. The PWER is the probability that a randomly selected patient will receive an ineffective treatment. It must usually be estimated due to unknown population strata sizes, such that only an estimate can be controlled at the given significance level. We apply the delta method to find a prediction interval for the resulting true PWER, we demonstrate by simulations that the interval has the required coverage probability, and illustrate the approach with real data examples.

</details>


### [24] [Assessment of evidence against homogeneity in exhaustive subgroup treatment effect plots](https://arxiv.org/abs/2602.06910)
*Björn Bornkamp,Jiarui Lu,Frank Bretz*

Main category: stat.ME

TL;DR: 提出一种计算高效的方法，在详尽亚组治疗效果图中生成同质性区域，使用双重稳健学习器估计亚组效应并推导参考分布，量化观察到的异质性在同质效应模型下的显著性。


<details>
  <summary>Details</summary>
Motivation: 在临床研究的详尽亚组分析中，观察到的治疗效果估计存在样本量小和多重性问题，需要更可解释的探索性评估方法来量化治疗效果的异质性。

Method: 使用双重稳健学习器计算伪结果来估计亚组效应，推导参考分布，建立同质性区域，比较不同临界值计算方法，通过心血管试验案例和模拟研究进行验证。

Result: 方法在模拟中显示出良好校准的推断性能，优于使用观察组均值简单差异的标准方法，能够有效量化治疗效果异质性的显著性。

Conclusion: 该方法为临床研究的探索性亚组分析提供了更可解释的工具，通过同质性区域帮助研究者评估观察到的治疗效果异质性是否具有统计学意义。

Abstract: Exhaustive subgroup treatment effect plots are constructed by displaying all subgroup treatment effects of interest against subgroup sample size, providing a useful overview of the observed treatment effect heterogeneity in a clinical trial. As in any exploratory subgroup analysis, however, the observed estimates suffer from small sample sizes and multiplicity issues. To facilitate more interpretable exploratory assessments, this paper introduces a computationally efficient method to generate homogeneity regions within exhaustive subgroup treatment effect plots. Using the Doubly Robust (DR) learner, pseudo-outcomes are used to estimate subgroup effects and derive reference distributions, quantifying how surprising observed heterogeneity is under a homogeneous effects model. Explicit formulas are derived for the homogeneity region and different methods for calculation of the critical values are compared. The method is illustrated with a cardiovascular trial and evaluated via simulation, showing well-calibrated inference and improved performance over standard approaches using simple differences of observed group means.

</details>
